{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/shivam/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shivam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shivam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vec_path = '../../../GoogleNews-vectors-negative300.bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = gensim.models.KeyedVectors.load_word2vec_format(eng_vec_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sim:\n",
    "    def __init__(self, words, vectors):\n",
    "        self.word_to_idx = {a: b for b, a in\n",
    "                            enumerate(w.strip() for w in open(words))}\n",
    "        self.mat = np.loadtxt(vectors)\n",
    "\n",
    "    def bow_vec(self, b):\n",
    "        vec = np.zeros(self.mat.shape[1])\n",
    "        for k, v in b.items():\n",
    "            idx = self.word_to_idx.get(k, -1)\n",
    "            if idx >= 0:\n",
    "                vec += self.mat[idx] / (norm(self.mat[idx]) + 1e-8) * v\n",
    "        return vec\n",
    "\n",
    "    def calc(self, b1, b2):\n",
    "        v1 = self.bow_vec(b1)\n",
    "        v2 = self.bow_vec(b2)\n",
    "        return abs(v1.dot(v2) / (norm(v1) + 1e-8) / (norm(v2) + 1e-8))\n",
    "\n",
    "stopwords = set([\n",
    "\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n",
    "\"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \"this\", \"to\",\n",
    "\"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\", \"the\", \"'s\", \"did\",\n",
    "\"have\", \"has\", \"had\", \"were\", \"'ll\"\n",
    "])\n",
    "\n",
    "nyt_sim = Sim('nyt_words.txt', 'nyt_word_vectors.txt')\n",
    "wiki_sim = Sim('wikipedia_words.txt', 'wikipedia_word_vectors.txt')\n",
    "\n",
    "def fix_compounds(a, b):\n",
    "    sb = set(x.lower() for x in b)\n",
    "\n",
    "    a_fix = []\n",
    "    la = len(a)\n",
    "    i = 0\n",
    "    while i < la:\n",
    "        if i + 1 < la:\n",
    "            comb = a[i] + a[i + 1]\n",
    "            if comb.lower() in sb:\n",
    "                a_fix.append(a[i] + a[i + 1])\n",
    "                i += 2\n",
    "                continue\n",
    "        a_fix.append(a[i])\n",
    "        i += 1\n",
    "    return a_fix\n",
    "\n",
    "def load_data(path):\n",
    "    sentences_pos = []\n",
    "    r1 = re.compile(r'\\<([^ ]+)\\>')\n",
    "    r2 = re.compile(r'\\$US(\\d)')\n",
    "    for l in open(path):\n",
    "        #l = l.decode('utf-8')\n",
    "        l = l.replace(u'’', \"'\")\n",
    "        l = l.replace(u'``', '\"')\n",
    "        l = l.replace(u\"''\", '\"')\n",
    "        l = l.replace(u\"—\", '--')\n",
    "        l = l.replace(u\"–\", '--')\n",
    "        l = l.replace(u\"´\", \"'\")\n",
    "        l = l.replace(u\"-\", \" \")\n",
    "        l = l.replace(u\"/\", \" \")\n",
    "        l = r1.sub(r'\\1', l)\n",
    "        l = r2.sub(r'$\\1', l)\n",
    "        s = l.strip().split('\\t')\n",
    "        sa, sb = tuple(nltk.word_tokenize(s)\n",
    "                          for s in l.strip().split('\\t'))\n",
    "        '''sa, sb = ([x.encode('utf-8') for x in sa],\n",
    "                  [x.encode('utf-8') for x in sb])'''\n",
    "\n",
    "        for s in (sa, sb):\n",
    "            for i in range(len(s)):\n",
    "                if s[i] == \"n't\":\n",
    "                    s[i] = \"not\"\n",
    "                elif s[i] == \"'m\":\n",
    "                    s[i] = \"am\"\n",
    "        sa, sb = fix_compounds(sa, sb), fix_compounds(sb, sa)\n",
    "        sentences_pos.append((nltk.pos_tag(sa), nltk.pos_tag(sb)))\n",
    "    return sentences_pos\n",
    "\n",
    "def load_wweight_table(path):\n",
    "    lines = open(path).readlines()\n",
    "    wweight = defaultdict(float)\n",
    "    if not len(lines):\n",
    "        return (wweight, 0.)\n",
    "    totfreq = int(lines[0])\n",
    "    for l in lines[1:]:\n",
    "        w, freq = l.split()\n",
    "        freq = float(freq)\n",
    "        if freq < 10:\n",
    "            continue\n",
    "        wweight[w] = math.log(totfreq / freq)\n",
    "\n",
    "    return wweight\n",
    "\n",
    "wweight = load_wweight_table('word-frequencies.txt')\n",
    "minwweight = min(wweight.values())\n",
    "\n",
    "def len_compress(l):\n",
    "    return math.log(1. + l)\n",
    "\n",
    "to_wordnet_tag = {\n",
    "        'NN':wordnet.NOUN,\n",
    "        'JJ':wordnet.ADJ,\n",
    "        'VB':wordnet.VERB,\n",
    "        'RB':wordnet.ADV\n",
    "    }\n",
    "\n",
    "word_matcher = re.compile('[^0-9,.(=)\\[\\]/_`]+$')\n",
    "def is_word(w):\n",
    "    return word_matcher.match(w) is not None\n",
    "\n",
    "def get_locase_words(spos):\n",
    "    return [x[0].lower() for x in spos\n",
    "            if is_word(x[0])]\n",
    "\n",
    "def make_ngrams(l, n):\n",
    "    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n",
    "    rez.append(l[n - 1:])\n",
    "    return list(zip(*rez))\n",
    "\n",
    "def wvec_sim(la,lb):\n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    for i in la:\n",
    "        try:\n",
    "            veca+= word_vec[i]/norm(word_vec[i]+ 1e-8)\n",
    "        except KeyError:\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "    for i in lb:\n",
    "        try:\n",
    "            vecb+= word_vec[i]/norm(word_vec[i] + 1e-8)\n",
    "        except:\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "        \n",
    "    return abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def weighted_wvec_sim(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    \n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    \n",
    "    for k,v in wa.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             veca+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "\n",
    "    for k,v in wb.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             vecb+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "        \n",
    "    return  abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def dist_sim(sim, la, lb):\n",
    "    wa = Counter(la)\n",
    "    wb = Counter(lb)\n",
    "    d1 = {x:1 for x in wa}\n",
    "    d2 = {x:1 for x in wb}\n",
    "    return sim.calc(d1, d2)\n",
    "\n",
    "def weighted_dist_sim(sim, lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    return sim.calc(wa, wb)\n",
    "\n",
    "def weighted_word_match(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wsuma = sum(wweight[w] * wa[w] for w in wa)\n",
    "    wsumb = sum(wweight[w] * wb[w] for w in wb)\n",
    "    wsum = 0.\n",
    "\n",
    "    for w in wa:\n",
    "        wd = min(wa[w], wb[w])\n",
    "        wsum += wweight[w] * wd\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if wsuma > 0 and wsum > 0:\n",
    "        p = wsum / wsuma\n",
    "    if wsumb > 0 and wsum > 0:\n",
    "        r = wsum / wsumb\n",
    "    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "wpathsimcache = {}\n",
    "def wpathsim(a, b):\n",
    "    \n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache:\n",
    "        return wpathsimcache[p]\n",
    "    if a == b:\n",
    "        wpathsimcache[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    mx = max([wa.path_similarity(wb)\n",
    "              for wa in sa\n",
    "              for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n",
    "    wpathsimcache[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_jc = {}\n",
    "def wpathsim_jc(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_jc:\n",
    "        return wpathsimcache_jc[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_jc[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.jcn_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_jc[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_lin = {}\n",
    "def wpathsim_lin(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_lin:\n",
    "        return wpathsimcache_lin[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_lin[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.lin_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_lin[p] = mx\n",
    "    return mx\n",
    "\n",
    "def calc_wn_prec(lema, lemb,val):\n",
    "    rez = 0.\n",
    "    for a in lema:\n",
    "        ms = 0.\n",
    "        for b in lemb:\n",
    "            if(val==0):\n",
    "                ms = max(ms, wpathsim(a, b))\n",
    "            elif(val==1):\n",
    "                ms = max(ms, wpathsim_jc(a, b))\n",
    "            else:\n",
    "                ms = max(ms, wpathsim_lin(a, b))\n",
    "                \n",
    "        rez += ms\n",
    "    return rez / len(lema)\n",
    "\n",
    "def wn_sim_match(lema, lemb,val):\n",
    "    f1 = 1.\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if len(lema) > 0 and len(lemb) > 0:\n",
    "        p = calc_wn_prec(lema, lemb,val)\n",
    "        r = calc_wn_prec(lemb, lema,val)\n",
    "        f1 = 2. * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def ngram_match(sa, sb, n):\n",
    "    nga = make_ngrams(sa, n)\n",
    "    ngb = make_ngrams(sb, n)\n",
    "    matches = 0\n",
    "    c1 = Counter(nga)\n",
    "    for ng in ngb:\n",
    "        if c1[ng] > 0:\n",
    "            c1[ng] -= 1\n",
    "            matches += 1\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    f1 = 1.\n",
    "    if len(nga) > 0 and len(ngb) > 0:\n",
    "        p = matches / float(len(nga))\n",
    "        r = matches / float(len(ngb))\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def get_lemmatized_words(sa):\n",
    "    rez = []\n",
    "    for w, wpos in sa:\n",
    "        w = w.lower()\n",
    "        if w in stopwords or not is_word(w):\n",
    "            continue\n",
    "        wtag = to_wordnet_tag.get(wpos[:2])\n",
    "        if wtag is None:\n",
    "            wlem = w\n",
    "        else:\n",
    "            wlem = wordnet.morphy(w, wtag) or w\n",
    "        rez.append(wlem)\n",
    "    return rez\n",
    "\n",
    "def is_stock_tick(w):\n",
    "    return w[0] == '.' and len(w) > 1 and w[1:].isupper()\n",
    "\n",
    "def stocks_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa if is_stock_tick(x[0]))\n",
    "    cb = set(x[0] for x in sb if is_stock_tick(x[0]))\n",
    "    isect = len(ca.intersection(cb))\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "def case_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    cb = set(x[0] for x in sb[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "    isect = len(ca.intersection(cb))\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "risnum = re.compile(r'^[0-9,./-]+$')\n",
    "rhasdigit = re.compile(r'[0-9]')\n",
    "\n",
    "def match_number(xa, xb):\n",
    "    if xa == xb:\n",
    "        return True\n",
    "    xa = xa.replace(',', '')\n",
    "    xb = xb.replace(',', '')\n",
    "\n",
    "    try:\n",
    "        va = int(float(xa))\n",
    "        vb = int(float(xb))\n",
    "        if (va == 0 or vb == 0) and va != vb:\n",
    "            return False\n",
    "        fxa = float(xa)\n",
    "        fxb = float(xb)\n",
    "        if abs(fxa - fxb) > 1:\n",
    "            return False\n",
    "        diga = xa.find('.')\n",
    "        digb = xb.find('.')\n",
    "        diga = 0 if diga == -1 else len(xa) - diga - 1\n",
    "        digb = 0 if digb == -1 else len(xb) - digb - 1\n",
    "        if diga > 0 and digb > 0 and va != vb:\n",
    "            return False\n",
    "        dmin = min(diga, digb)\n",
    "        if dmin == 0:\n",
    "            if abs(round(fxa, 0) - round(fxb, 0)) < 1e-5:\n",
    "                return True\n",
    "            return va == vb\n",
    "        return abs(round(fxa, dmin) - round(fxb, dmin)) < 1e-5\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "def number_features(sa, sb):\n",
    "    numa = set(x[0] for x in sa if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    numb = set(x[0] for x in sb if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    isect = 0\n",
    "    for na in numa:\n",
    "        if na in numb:\n",
    "            isect += 1\n",
    "            continue\n",
    "        for nb in numb:\n",
    "            if match_number(na, nb):\n",
    "                isect += 1\n",
    "                break\n",
    "\n",
    "    la, lb = len(numa), len(numb)\n",
    "\n",
    "    f = 1.\n",
    "    subset = 0.\n",
    "    if la + lb > 0:\n",
    "        if isect == la or isect == lb:\n",
    "            subset = 1.\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2. * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f, subset)\n",
    "\n",
    "def relative_len_difference(lca, lcb):\n",
    "    la, lb = len(lca), len(lcb)\n",
    "    return abs(la - lb) / float(max(la, lb) + 1e-5)\n",
    "\n",
    "def relative_ic_difference(lca, lcb):\n",
    "    #wa = sum(wweight[x] for x in lca)\n",
    "    #wb = sum(wweight[x] for x in lcb)\n",
    "    wa = sum(max(0., wweight[x] - minwweight) for x in lca)\n",
    "    wb = sum(max(0., wweight[x] - minwweight) for x in lcb)\n",
    "    return abs(wa - wb) / (max(wa, wb) + 1e-5)\n",
    "\n",
    "def my_custom_function(y_true, y_predict):\n",
    "    corr,_ = pearsonr(y_true, y_predict)\n",
    "    return corr\n",
    "\n",
    "    \n",
    " #   gs = GridSearchCV(clf, param_grid={'kernel': [ 'rbf'],'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='roc_auc')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(path):\n",
    "    import sys\n",
    "\n",
    "\n",
    "    orig = []\n",
    "    for l in open('../../test-gold/STS.input.surprise.OnWN.txt'):\n",
    "        orig.append([x.strip() for x in l.lower().split(\"\\t\")])\n",
    "\n",
    "    scores = list(map(float, open(path).readlines()))\n",
    "\n",
    "    if len(orig) != len(scores):\n",
    "        print (sys.stderr, \"Error: inputs should have the same number of lines\")\n",
    "        exit(1)\n",
    "    processed=[]\n",
    "    f = open('processed_'+path, 'w')\n",
    "    for i, s in enumerate(scores):\n",
    "        if orig[0] == orig[1]:\n",
    "            s = 5.\n",
    "        if s > 5:\n",
    "            s = 5.\n",
    "        if s < 0:\n",
    "            s = 0.\n",
    "        processed.append(s)\n",
    "        #print (s)\n",
    "        f.write(\"%s\\n\" % s)\n",
    "    f.close()\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## takelab_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d154bc860617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../train/STS.input.MSRvid_clean.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../train/STS.input.MSRpar.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d154bc860617>\u001b[0m in \u001b[0;36mcalc_features\u001b[0;34m(sa, sb)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mngram_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mngram_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mwn_sim_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mweighted_word_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0molca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0molcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mweighted_word_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-05963bd9c7c0>\u001b[0m in \u001b[0;36mwn_sim_match\u001b[0;34m(lema, lemb, val)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_wn_prec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_wn_prec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-05963bd9c7c0>\u001b[0m in \u001b[0;36mcalc_wn_prec\u001b[0;34m(lema, lemb, val)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwpathsim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwpathsim_jc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-05963bd9c7c0>\u001b[0m in \u001b[0;36mwpathsim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    216\u001b[0m     print(sb)'''\n\u001b[1;32m    217\u001b[0m     mx = max([wa.path_similarity(wb)\n\u001b[0;32m--> 218\u001b[0;31m               \u001b[0;32mfor\u001b[0m \u001b[0mwa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m               for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n\u001b[1;32m    220\u001b[0m     \u001b[0mwpathsimcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-05963bd9c7c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    217\u001b[0m     mx = max([wa.path_similarity(wb)\n\u001b[1;32m    218\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mwa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m               for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mwpathsimcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mpath_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         distance = self.shortest_path_distance(\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulate_root\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_needs_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m         )\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mshortest_path_distance\u001b[0;34m(self, other, simulate_root)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mdist_dict1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shortest_hypernym_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulate_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mdist_dict2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shortest_hypernym_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulate_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# For each ancestor synset common to both subject synsets, find the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_shortest_hypernym_paths\u001b[0;34m(self, simulate_root)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m             \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_hypernyms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minstance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('takelab_output.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_output.txt'))\n",
    "\n",
    "\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson: 0.88224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using JC Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('takelab_jc.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('takelab_jc.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT: (Pearson: 0.87477)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lin Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6904731912143911\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT:(Pearson: 0.87057)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec with weighted_LSA_vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7061338093720511\n"
     ]
    }
   ],
   "source": [
    "with open('wv_wlsa.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('wv_wlsa.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT: Pearson Correlation: ( 0.8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JC AND LIN Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb),\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6899007679205905\n"
     ]
    }
   ],
   "source": [
    "with open('jc_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('jc_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT:(Pearson: 0.87136)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec with weighted_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "oots\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "seadoo\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "descale\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "episcopalian\n",
      "episcopalian\n",
      "amgen\n",
      "allergan\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "webvpn\n",
      "smarnet\n",
      "and\n",
      "and\n",
      "dallager\n",
      "dallager\n",
      "''\n",
      "karanja\n",
      "karanja\n",
      "and\n",
      "and\n",
      "swartz\n",
      "swartz\n",
      "and\n",
      "and\n",
      "españa\n",
      "and\n",
      "espaa\n",
      "and\n",
      "plofsky\n",
      "plofsky\n",
      "frankel\n",
      "and\n",
      "hilsenrath\n",
      "and\n",
      "klarman\n",
      "klarman\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "episcopalian\n",
      "and\n",
      "episcopalian\n",
      "squyres\n",
      "athena\n",
      "squyres\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "bucklew\n",
      "bucklew\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "and\n",
      "and\n",
      "waksal\n",
      "and\n",
      "waksal\n",
      "and\n",
      "and\n",
      "zambrano\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "and\n",
      "'\n",
      "vba\n",
      "studdard\n",
      "aiken\n",
      "studdard\n",
      "aiken\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "imclone\n",
      "erbitux\n",
      "and\n",
      "imclone\n",
      "sensenbrenner\n",
      "sensenbrenner\n",
      "and\n",
      "and\n",
      "and\n",
      "heatley\n",
      "and\n",
      "heatley\n",
      "and\n",
      "strayhorn\n",
      "strayhorn\n",
      "sahel\n",
      "ntsb\n",
      "nhtsa\n",
      "and\n",
      "jazeera\n",
      "jazeera\n",
      "franklyn\n",
      "bergonzi\n",
      "and\n",
      "franklyn\n",
      "bergonzi\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "cavender\n",
      ":\n",
      "''\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "and\n",
      "morgenthau\n",
      "morgenthau\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "edmund\n",
      "sargus\n",
      "akron\n",
      "firstenergy\n",
      "edmund\n",
      "sargus\n",
      "galvin\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ";\n",
      "''\n",
      "''\n",
      "chante\n",
      "chante\n",
      "jawaon\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "phipps\n",
      "and\n",
      "mclamb\n",
      "monteiro\n",
      "monteiro\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "obetz\n",
      "obetz\n",
      "and\n",
      "and\n",
      "centre\n",
      "taegu\n",
      "centre\n",
      "and\n",
      "sunncomm\n",
      "mediamax\n",
      "'advertised\n",
      "'\n",
      "mediamax\n",
      "''\n",
      "kadyrov\n",
      "itar\n",
      "and\n",
      "netgear\n",
      "and\n",
      "and\n",
      "verisign\n",
      "verisign\n",
      "''\n",
      "dmca\n",
      "and\n",
      "and\n",
      "druce\n",
      "and\n",
      "druce\n",
      "filipina\n",
      "emile\n",
      "laroza\n",
      "epicentre\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "freitas\n",
      "'\n",
      "mertel\n",
      "mertel\n",
      "kleiner\n",
      "caufield\n",
      "byers\n",
      "and\n",
      "and\n",
      "kleiner\n",
      "''\n",
      "''\n",
      "mindanao\n",
      "neighbour\n",
      "mindanao\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "and\n",
      "''\n",
      "davidowitz\n",
      "''\n",
      "davidowitz\n",
      "and\n",
      "and\n",
      "reveller\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kurd\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "armoured\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "covello\n",
      "and\n",
      "''\n",
      "covello\n",
      "fujitsu\n",
      "spansion\n",
      "spansion\n",
      "and\n",
      "fujitsu\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "pataki\n",
      "and\n",
      "lipa\n",
      "kessel\n",
      "lipa\n",
      "kessel\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "chiron\n",
      "and\n",
      "chiron\n",
      "and\n",
      "and\n",
      "and\n",
      "ncri\n",
      "safavi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "ncri\n",
      "safavi\n",
      "and\n",
      "mcentee\n",
      "and\n",
      "prodi\n",
      "halabi\n",
      "halabi\n",
      "waksal\n",
      ":\n",
      "waksal\n",
      "and\n",
      "and\n",
      "jayroe\n",
      "organisation\n",
      ":\n",
      "and\n",
      "organisation\n",
      "and\n",
      "'\n",
      "''\n",
      "baystar\n",
      "baystar\n",
      "and\n",
      "and\n",
      "kreme\n",
      "lafferty\n",
      "ezzell\n",
      ":\n",
      "''\n",
      "ezzell\n",
      ";\n",
      "''\n",
      "''\n",
      "klarman\n",
      "klarman\n",
      "meagre\n",
      "and\n",
      "lleyton\n",
      "lleyton\n",
      ":\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "edna\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "rhode\n",
      "adrs\n",
      "and\n",
      "bremer\n",
      "and\n",
      "bremer\n",
      "and\n",
      "sayliyah\n",
      "sayliyah\n",
      "mcbride\n",
      "fontana\n",
      "fontana\n",
      "prodi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "mcbride\n",
      "and\n",
      "corixa\n",
      "corixa\n",
      "and\n",
      "shukrijumah\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      "programme\n",
      "''\n",
      ":\n",
      "programme\n",
      "and\n",
      "''\n",
      "gabriela\n",
      "lemus\n",
      "lulac\n",
      "''\n",
      "gabriela\n",
      "lemus\n",
      "and\n",
      "and\n",
      "''\n",
      "jillian\n",
      ":\n",
      "and\n",
      "''\n",
      "jillian\n",
      "dennehy\n",
      "baylor\n",
      "dennehy\n",
      "baylor\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "mahendradatta\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "lorna\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "novellus\n",
      "and\n",
      "novellus\n",
      "''\n",
      "rwanda\n",
      "and\n",
      "rwanda\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      "vliet\n",
      "'\n",
      "vliet\n",
      "and\n",
      "melanesian\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ";\n",
      "''\n",
      "''\n",
      "javaone\n",
      "'\n",
      "javaone\n",
      "and\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "hebron\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ellison\n",
      "peoplesoft\n",
      "ellison\n",
      "peoplesoft\n",
      "and\n",
      "almeida\n",
      "''\n",
      "''\n",
      "wyden\n",
      "and\n",
      "dorgan\n",
      "''\n",
      "milunovich\n",
      "milunovich\n",
      "''\n",
      "jennie\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "veneman\n",
      "and\n",
      "liberian\n",
      "liberian\n",
      "audubon\n",
      "audubon\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "shumaker\n",
      "''\n",
      "shumaker\n",
      "bulger\n",
      "''\n",
      "bulger\n",
      "''\n",
      ";\n",
      "!\n",
      "''\n",
      ":\n",
      "!\n",
      "''\n",
      "and\n",
      ":\n",
      "matsushita\n",
      "hitachi\n",
      "and\n",
      "celf\n",
      "hitachi\n",
      "matsushita\n",
      "and\n",
      "?\n",
      "''\n",
      "ropeik\n",
      "''\n",
      "ropeik\n",
      "bergin\n",
      "baywatch\n",
      "''\n",
      "bergin\n",
      "''\n",
      "''\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gemstar\n",
      "gemstar\n",
      "''\n",
      "barbini\n",
      "tuscany\n",
      "barbini\n",
      "tuscany\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertson\n",
      "favourable\n",
      "and\n",
      "''\n",
      "?\n",
      "?\n",
      "mediaq\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "strasbourg\n",
      "strasbourg\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "?\n",
      "axelrod\n",
      "and\n",
      "?\n",
      "mackey\n",
      "and\n",
      "''\n",
      "mackey\n",
      "and\n",
      "trenton\n",
      "somerville\n",
      "trenton\n",
      "and\n",
      "spaceshipone\n",
      "esco\n",
      "esco\n",
      ":\n",
      "''\n",
      "riyadh\n",
      "riyadh\n",
      "qaida\n",
      "and\n",
      "and\n",
      "'\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "griffith\n",
      "manteo\n",
      "griffith\n",
      "and\n",
      "manteo\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "bremer\n",
      "hrt\n",
      "''\n",
      "''\n",
      "mccartney\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "lagrou\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "barbour\n",
      "barbour\n",
      "and\n",
      "epicentre\n",
      "emile\n",
      "laroza\n",
      "epicentre\n",
      "rusch\n",
      "lastings\n",
      "milledge\n",
      "lakewood\n",
      "lastings\n",
      "milledge\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      ";\n",
      "niro\n",
      "niro\n",
      "albertsons\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "and\n",
      "centre\n",
      "centre\n",
      "brendsel\n",
      "and\n",
      "vaughn\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "plmo\n",
      "and\n",
      "psrc\n",
      "palmsource\n",
      ":\n",
      "psrc\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "yorktown\n",
      "and\n",
      "halliburton\n",
      "halliburton\n",
      "ishtar\n",
      "sheraton\n",
      "and\n",
      "ishtar\n",
      "sheraton\n",
      "zhaoxing\n",
      "zhaoxing\n",
      "peoplesoft\n",
      "peoplesoft\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "‘\n",
      "taikong\n",
      "''\n",
      "taikong\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "''\n",
      "'\n",
      "''\n",
      "and\n",
      "'\n",
      "''\n",
      "barbini\n",
      "barbini\n",
      "and\n",
      "and\n",
      "gann\n",
      "gann\n",
      "''\n",
      "and\n",
      "mohave\n",
      "''\n",
      "boxley\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "russin\n",
      ";\n",
      "and\n",
      "russin\n",
      "and\n",
      "armidale\n",
      "''\n",
      "armidale\n",
      "kollar\n",
      "kotelly\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "junya\n",
      "tanase\n",
      "junya\n",
      "tanase\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "reyna\n",
      "and\n",
      "altria\n",
      "altria\n",
      "''\n",
      "tambe\n",
      "''\n",
      "tambe\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "hickenlooper\n",
      "hickenlooper\n",
      "cwa\n",
      "and\n",
      "and\n",
      "annika\n",
      "sorenstam\n",
      "annika\n",
      "sorenstam\n",
      "and\n",
      "''\n",
      "qasim\n",
      "''\n",
      "and\n",
      "nasd\n",
      "nasd\n",
      "and\n",
      "'\n",
      "concordes\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "aftra\n",
      "aftra\n",
      "malawi\n",
      "qaida\n",
      "malawi\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "alibek\n",
      ":\n",
      "''\n",
      "alibek\n",
      "selenski\n",
      "and\n",
      "selenski\n",
      "fischi\n",
      "kernan\n",
      "o'bannon\n",
      "kernan\n",
      "o'bannon\n",
      "and\n",
      "'\n",
      "and\n",
      "'\n",
      "''\n",
      "and\n",
      "bogden\n",
      "and\n",
      "bogden\n",
      "fujitsu\n",
      "fujitsu\n",
      "'\n",
      "and\n",
      "and\n",
      "somers\n",
      ";\n",
      "and\n",
      "somers\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "colgate\n",
      "colgate\n",
      "corixa\n",
      "corixa\n",
      "grey\n",
      "''\n",
      "'new\n",
      "'\n",
      "''\n",
      "and\n",
      "citicorp\n",
      "and\n",
      "citicorp\n",
      "and\n",
      "and\n",
      "mahmoud\n",
      "jordanian\n",
      "aqaba\n",
      "and\n",
      "mahmoud\n",
      "aqaba\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "''\n",
      "mabry\n",
      "''\n",
      "mabry\n",
      "xscale\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      ":\n",
      "littleton\n",
      "echostar\n",
      "echostar\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "mclean\n",
      "guillermo\n",
      "and\n",
      "netherlander\n",
      "verkerk\n",
      "guillermo\n",
      "and\n",
      "verkerk\n",
      "and\n",
      "?\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "cancuen\n",
      "vanderbilt\n",
      "and\n",
      "cancun\n",
      "vanderbilt\n",
      "and\n",
      "allergan\n",
      "allergan\n",
      "''\n",
      "''\n",
      "and\n",
      "meagre\n",
      "imclone\n",
      "erbitux\n",
      "imclone\n",
      "levine\n",
      "levine\n",
      "airtran\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "pingeon\n",
      "'\n",
      "pingeon\n",
      "assan\n",
      "and\n",
      "pnc\n",
      "''\n",
      "and\n",
      "rohr\n",
      "rohr\n",
      "and\n",
      "pnc\n",
      "ccag\n",
      "rowland\n",
      "and\n",
      "congolese\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "navistar\n",
      "navistar\n",
      "and\n",
      "nimitz\n",
      "baldacci\n",
      "''\n",
      "baldacci\n",
      "and\n",
      "and\n",
      "and\n",
      "healthpark\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "davey\n",
      "and\n",
      "davey\n",
      "and\n",
      "cronyn\n",
      "cronyn\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "schindler\n",
      "schiavo\n",
      "johnnie\n",
      "''\n",
      "johnnie\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dewhurst\n",
      "''\n",
      "''\n",
      "and\n",
      "antetonitrus\n",
      "antetonitrus\n",
      "and\n",
      "strutt\n",
      "and\n",
      "strutt\n",
      "olvera\n",
      "olvera\n",
      "mohsen\n",
      "zubaidi\n",
      "mohsen\n",
      "zubaidi\n",
      "and\n",
      "''\n",
      "buell\n",
      "and\n",
      "''\n",
      "buell\n",
      "and\n",
      "''\n",
      "gammerman\n",
      "and\n",
      "''\n",
      "gammerman\n",
      "inuit\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "caracas\n",
      "phoberomys\n",
      "'\n",
      "caracas\n",
      "jemaah\n",
      "islamiyah\n",
      "jemaah\n",
      "islamiyah\n",
      "''\n",
      "nayef\n",
      "nayef\n",
      "and\n",
      "and\n",
      "millette\n",
      "mcdonnell\n",
      "mcdonnell\n",
      "doud\n",
      "doud\n",
      "and\n",
      "and\n",
      "pribbenow\n",
      "'\n",
      "pribbenow\n",
      "and\n",
      "clemons\n",
      "'\n",
      "clemons\n",
      "''\n",
      "and\n",
      "o'malley\n",
      "o'malley\n",
      "and\n",
      "''\n",
      "silvester\n",
      "silvester\n",
      "and\n",
      "capps\n",
      "and\n",
      "''\n",
      "capps\n",
      "and\n",
      "''\n",
      "hovan\n",
      "speranza\n",
      "''\n",
      "hovan\n",
      "''\n",
      "speranza\n",
      "''\n",
      "daschle\n",
      "''\n",
      "daschle\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "medimmune\n",
      "gaithersburg\n",
      "medimmune\n",
      "flumist\n",
      "and\n",
      "kaichen\n",
      "teya\n",
      "teya\n",
      "jemaah\n",
      "islamiyah\n",
      "qa'eda\n",
      "samudra\n",
      "qaida\n",
      "jemaah\n",
      "islamiyah\n",
      "ortega\n",
      "''\n",
      "ortega\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "ernst\n",
      "ernst\n",
      "kerrigan\n",
      "and\n",
      "supoyo\n",
      "and\n",
      "kosovan\n",
      "kosovan\n",
      "''\n",
      ":\n",
      "grassley\n",
      "and\n",
      "and\n",
      "geraldine\n",
      "geraldine\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "hammersmith\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "samudra\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "halliburton\n",
      "kbr\n",
      "and\n",
      "''\n",
      "kbr\n",
      "and\n",
      "''\n",
      "halliburton\n",
      "elecia\n",
      "elecia\n",
      "and\n",
      "dbtel\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "''\n",
      "''\n",
      "barrenas\n",
      "colpin\n",
      "centre\n",
      "olaf\n",
      "centre\n",
      "and\n",
      "and\n",
      "ballmer\n",
      "ballmer\n",
      "talabani\n",
      "and\n",
      "''\n",
      "talabani\n",
      "and\n",
      "''\n",
      "organise\n",
      "rosenblatt\n",
      "''\n",
      "greenberg\n",
      "''\n",
      "greenberg\n",
      "and\n",
      "and\n",
      "and\n",
      "rucker\n",
      "and\n",
      "''\n",
      "wolfcale\n",
      "wolfcale\n",
      "and\n",
      "''\n",
      "'\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "peoplesoft\n",
      "and\n",
      "and\n",
      "peoplesoft\n",
      "''\n",
      "and\n",
      "''\n",
      "nicklaus\n",
      "nicklaus\n",
      "''\n",
      "''\n",
      "comdex\n",
      "comdex\n",
      "and\n",
      "tatar\n",
      "tehuacan\n",
      "teotihuacan\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "marce\n",
      "''\n",
      "mirant\n",
      "marce\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "darvish\n",
      "bijan\n",
      "darvish\n",
      "''\n",
      "and\n",
      "and\n",
      "patton\n",
      "and\n",
      "patton\n",
      "and\n",
      "mcgill\n",
      "mcgill\n",
      "grasso\n",
      "grasso\n",
      "moriarty\n",
      "and\n",
      "carella\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "''\n",
      "''\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "and\n",
      "idj\n",
      "and\n",
      "siebel\n",
      "siebel\n",
      "and\n",
      "and\n",
      "''\n",
      "jimenez\n",
      "and\n",
      "''\n",
      "jiménez\n",
      "and\n",
      "and\n",
      "rebell\n",
      "''\n",
      "rebell\n",
      "dotson\n",
      "dennehy\n",
      "dotson\n",
      "dennehy\n",
      "and\n",
      "and\n",
      "jiuquan\n",
      "and\n",
      "and\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "metre\n",
      "''\n",
      "metre\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "aiken\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "schroeder\n",
      "stefani\n",
      "apologise\n",
      "berlusconi\n",
      "stefani\n",
      "silvio\n",
      "berlusconi\n",
      "openmanage\n",
      "and\n",
      "and\n",
      "''\n",
      "moriarty\n",
      "moriarty\n",
      "''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lurd\n",
      "ja'neh\n",
      "and\n",
      "kedo\n",
      ":\n",
      "''\n",
      "tiburtina\n",
      "and\n",
      "tiburtina\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "bloomfield\n",
      "taubman\n",
      "cgt\n",
      "and\n",
      "cgt\n",
      "and\n",
      "and\n",
      "and\n",
      "benning\n",
      "benning\n",
      "gambian\n",
      "gambian\n",
      "roush\n",
      "and\n",
      "diarrhoea\n",
      "and\n",
      "kiernan\n",
      "seifert\n",
      "seifert\n",
      "mccloskey\n",
      "mccloskey\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "meps\n",
      "and\n",
      "and\n",
      "honourable\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "favourable\n",
      "igc\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "plough\n",
      "kostunica\n",
      "and\n",
      "koitunica\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "recognise\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "plough\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "'no\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      ";\n",
      "unipersonnelle\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      ";\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "reliques\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumiãšre\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "gayssot\n",
      "gayssot\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "'model\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "flexibilisation\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "uvre\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "organise\n",
      "organise\n",
      "and\n",
      "and\n",
      "meps\n",
      "esb\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "vigour\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "disgracieuse\n",
      "omc\n",
      "organise\n",
      "organise\n",
      "and\n",
      "defence\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "'\n",
      "and\n",
      "fusse\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "organise\n",
      "organise\n",
      "language‒describes\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "réconfortants\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "programme\n",
      "and\n",
      "programme\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "'\n",
      "and\n",
      "plough\n",
      ";\n",
      "unipersonnelle\n",
      "'\n",
      "and\n",
      "fusse\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "overarch\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "disgracieuse\n",
      "and\n",
      "programme\n",
      "programme\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marã­n\n",
      "galicia\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "meps\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "defence\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "stabilisation\n",
      "and\n",
      "stabilisation\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "'\n",
      "and\n",
      "and\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "defence\n",
      "and\n",
      "meps\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "meps\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "fulfil\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "favour\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "organise\n",
      "and\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "margallo\n",
      "marfil\n",
      "meps\n",
      "récréationnelles\n",
      "“\n",
      "”\n",
      "and\n",
      "and\n",
      "uvre\n",
      "plough\n",
      ";\n",
      "unipersonnelle\n",
      ":\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "disgracieuse\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "plough\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "'\n",
      "meps\n",
      "and\n",
      "and\n",
      "plough\n",
      "and\n",
      ":\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "and\n",
      "karamanou\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "albania\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "organise\n",
      "organise\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "“\n",
      "”\n",
      "and\n",
      "?\n",
      "plough\n",
      "and\n",
      ":\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "galicia\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "''\n",
      "organise\n",
      "'no\n",
      "'\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "neighbourly\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "modernised\n",
      "!\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      ":\n",
      "nº\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      ";\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "obtiendraient\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "?\n",
      "'austrian\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      ";\n",
      "unipersonnelle\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "and\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "favourable\n",
      "igc\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      ";\n",
      "favour\n",
      "and\n",
      "favour\n",
      "programme\n",
      "programme\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "favourable\n",
      "igc\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "‘\n",
      "'model\n",
      "and\n",
      "?\n",
      "and\n",
      "rã©crã©ationnelles\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "defence\n",
      "and\n",
      "programme\n",
      "programme\n",
      ";\n",
      "unipersonal\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      ":\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "koitunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "inflectional\n",
      "offence\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "nonacceptance\n",
      ";\n",
      ";\n",
      "diacritic\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "judgement\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "univalent\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "judgement\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "telic\n",
      "and\n",
      ";\n",
      "fulfil\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "programme\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ":\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "enfranchisment\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "inquiringly\n",
      "inquiringly\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "gase\n",
      "rna\n",
      "rna\n",
      ";\n",
      ";\n",
      ";\n",
      "thames\n",
      "thames\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "           # wn_sim_match(lema, lemb,1),\n",
    "            #wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7067845445588152\n"
     ]
    }
   ],
   "source": [
    "with open('wv_wwv.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('wv_wwv.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: (Pearson: 0.88384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec_wwv_jc_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "oots\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "seadoo\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "descale\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "episcopalian\n",
      "episcopalian\n",
      "amgen\n",
      "allergan\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "webvpn\n",
      "smarnet\n",
      "and\n",
      "and\n",
      "dallager\n",
      "dallager\n",
      "''\n",
      "karanja\n",
      "karanja\n",
      "and\n",
      "and\n",
      "swartz\n",
      "swartz\n",
      "and\n",
      "and\n",
      "españa\n",
      "and\n",
      "espaa\n",
      "and\n",
      "plofsky\n",
      "plofsky\n",
      "frankel\n",
      "and\n",
      "hilsenrath\n",
      "and\n",
      "klarman\n",
      "klarman\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "episcopalian\n",
      "and\n",
      "episcopalian\n",
      "squyres\n",
      "athena\n",
      "squyres\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "bucklew\n",
      "bucklew\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "and\n",
      "and\n",
      "waksal\n",
      "and\n",
      "waksal\n",
      "and\n",
      "and\n",
      "zambrano\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "and\n",
      "'\n",
      "vba\n",
      "studdard\n",
      "aiken\n",
      "studdard\n",
      "aiken\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "imclone\n",
      "erbitux\n",
      "and\n",
      "imclone\n",
      "sensenbrenner\n",
      "sensenbrenner\n",
      "and\n",
      "and\n",
      "and\n",
      "heatley\n",
      "and\n",
      "heatley\n",
      "and\n",
      "strayhorn\n",
      "strayhorn\n",
      "sahel\n",
      "ntsb\n",
      "nhtsa\n",
      "and\n",
      "jazeera\n",
      "jazeera\n",
      "franklyn\n",
      "bergonzi\n",
      "and\n",
      "franklyn\n",
      "bergonzi\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "cavender\n",
      ":\n",
      "''\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "and\n",
      "morgenthau\n",
      "morgenthau\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "edmund\n",
      "sargus\n",
      "akron\n",
      "firstenergy\n",
      "edmund\n",
      "sargus\n",
      "galvin\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ";\n",
      "''\n",
      "''\n",
      "chante\n",
      "chante\n",
      "jawaon\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "phipps\n",
      "and\n",
      "mclamb\n",
      "monteiro\n",
      "monteiro\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "obetz\n",
      "obetz\n",
      "and\n",
      "and\n",
      "centre\n",
      "taegu\n",
      "centre\n",
      "and\n",
      "sunncomm\n",
      "mediamax\n",
      "'advertised\n",
      "'\n",
      "mediamax\n",
      "''\n",
      "kadyrov\n",
      "itar\n",
      "and\n",
      "netgear\n",
      "and\n",
      "and\n",
      "verisign\n",
      "verisign\n",
      "''\n",
      "dmca\n",
      "and\n",
      "and\n",
      "druce\n",
      "and\n",
      "druce\n",
      "filipina\n",
      "emile\n",
      "laroza\n",
      "epicentre\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "freitas\n",
      "'\n",
      "mertel\n",
      "mertel\n",
      "kleiner\n",
      "caufield\n",
      "byers\n",
      "and\n",
      "and\n",
      "kleiner\n",
      "''\n",
      "''\n",
      "mindanao\n",
      "neighbour\n",
      "mindanao\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "and\n",
      "''\n",
      "davidowitz\n",
      "''\n",
      "davidowitz\n",
      "and\n",
      "and\n",
      "reveller\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kurd\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "armoured\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "covello\n",
      "and\n",
      "''\n",
      "covello\n",
      "fujitsu\n",
      "spansion\n",
      "spansion\n",
      "and\n",
      "fujitsu\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "pataki\n",
      "and\n",
      "lipa\n",
      "kessel\n",
      "lipa\n",
      "kessel\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "chiron\n",
      "and\n",
      "chiron\n",
      "and\n",
      "and\n",
      "and\n",
      "ncri\n",
      "safavi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "ncri\n",
      "safavi\n",
      "and\n",
      "mcentee\n",
      "and\n",
      "prodi\n",
      "halabi\n",
      "halabi\n",
      "waksal\n",
      ":\n",
      "waksal\n",
      "and\n",
      "and\n",
      "jayroe\n",
      "organisation\n",
      ":\n",
      "and\n",
      "organisation\n",
      "and\n",
      "'\n",
      "''\n",
      "baystar\n",
      "baystar\n",
      "and\n",
      "and\n",
      "kreme\n",
      "lafferty\n",
      "ezzell\n",
      ":\n",
      "''\n",
      "ezzell\n",
      ";\n",
      "''\n",
      "''\n",
      "klarman\n",
      "klarman\n",
      "meagre\n",
      "and\n",
      "lleyton\n",
      "lleyton\n",
      ":\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "edna\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "rhode\n",
      "adrs\n",
      "and\n",
      "bremer\n",
      "and\n",
      "bremer\n",
      "and\n",
      "sayliyah\n",
      "sayliyah\n",
      "mcbride\n",
      "fontana\n",
      "fontana\n",
      "prodi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "mcbride\n",
      "and\n",
      "corixa\n",
      "corixa\n",
      "and\n",
      "shukrijumah\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      "programme\n",
      "''\n",
      ":\n",
      "programme\n",
      "and\n",
      "''\n",
      "gabriela\n",
      "lemus\n",
      "lulac\n",
      "''\n",
      "gabriela\n",
      "lemus\n",
      "and\n",
      "and\n",
      "''\n",
      "jillian\n",
      ":\n",
      "and\n",
      "''\n",
      "jillian\n",
      "dennehy\n",
      "baylor\n",
      "dennehy\n",
      "baylor\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "mahendradatta\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "lorna\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "novellus\n",
      "and\n",
      "novellus\n",
      "''\n",
      "rwanda\n",
      "and\n",
      "rwanda\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      "vliet\n",
      "'\n",
      "vliet\n",
      "and\n",
      "melanesian\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ";\n",
      "''\n",
      "''\n",
      "javaone\n",
      "'\n",
      "javaone\n",
      "and\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "hebron\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ellison\n",
      "peoplesoft\n",
      "ellison\n",
      "peoplesoft\n",
      "and\n",
      "almeida\n",
      "''\n",
      "''\n",
      "wyden\n",
      "and\n",
      "dorgan\n",
      "''\n",
      "milunovich\n",
      "milunovich\n",
      "''\n",
      "jennie\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "veneman\n",
      "and\n",
      "liberian\n",
      "liberian\n",
      "audubon\n",
      "audubon\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "shumaker\n",
      "''\n",
      "shumaker\n",
      "bulger\n",
      "''\n",
      "bulger\n",
      "''\n",
      ";\n",
      "!\n",
      "''\n",
      ":\n",
      "!\n",
      "''\n",
      "and\n",
      ":\n",
      "matsushita\n",
      "hitachi\n",
      "and\n",
      "celf\n",
      "hitachi\n",
      "matsushita\n",
      "and\n",
      "?\n",
      "''\n",
      "ropeik\n",
      "''\n",
      "ropeik\n",
      "bergin\n",
      "baywatch\n",
      "''\n",
      "bergin\n",
      "''\n",
      "''\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gemstar\n",
      "gemstar\n",
      "''\n",
      "barbini\n",
      "tuscany\n",
      "barbini\n",
      "tuscany\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertson\n",
      "favourable\n",
      "and\n",
      "''\n",
      "?\n",
      "?\n",
      "mediaq\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "strasbourg\n",
      "strasbourg\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "?\n",
      "axelrod\n",
      "and\n",
      "?\n",
      "mackey\n",
      "and\n",
      "''\n",
      "mackey\n",
      "and\n",
      "trenton\n",
      "somerville\n",
      "trenton\n",
      "and\n",
      "spaceshipone\n",
      "esco\n",
      "esco\n",
      ":\n",
      "''\n",
      "riyadh\n",
      "riyadh\n",
      "qaida\n",
      "and\n",
      "and\n",
      "'\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "griffith\n",
      "manteo\n",
      "griffith\n",
      "and\n",
      "manteo\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "bremer\n",
      "hrt\n",
      "''\n",
      "''\n",
      "mccartney\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "lagrou\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "barbour\n",
      "barbour\n",
      "and\n",
      "epicentre\n",
      "emile\n",
      "laroza\n",
      "epicentre\n",
      "rusch\n",
      "lastings\n",
      "milledge\n",
      "lakewood\n",
      "lastings\n",
      "milledge\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      ";\n",
      "niro\n",
      "niro\n",
      "albertsons\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "and\n",
      "centre\n",
      "centre\n",
      "brendsel\n",
      "and\n",
      "vaughn\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "plmo\n",
      "and\n",
      "psrc\n",
      "palmsource\n",
      ":\n",
      "psrc\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "yorktown\n",
      "and\n",
      "halliburton\n",
      "halliburton\n",
      "ishtar\n",
      "sheraton\n",
      "and\n",
      "ishtar\n",
      "sheraton\n",
      "zhaoxing\n",
      "zhaoxing\n",
      "peoplesoft\n",
      "peoplesoft\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "‘\n",
      "taikong\n",
      "''\n",
      "taikong\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "''\n",
      "'\n",
      "''\n",
      "and\n",
      "'\n",
      "''\n",
      "barbini\n",
      "barbini\n",
      "and\n",
      "and\n",
      "gann\n",
      "gann\n",
      "''\n",
      "and\n",
      "mohave\n",
      "''\n",
      "boxley\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "russin\n",
      ";\n",
      "and\n",
      "russin\n",
      "and\n",
      "armidale\n",
      "''\n",
      "armidale\n",
      "kollar\n",
      "kotelly\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "junya\n",
      "tanase\n",
      "junya\n",
      "tanase\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "reyna\n",
      "and\n",
      "altria\n",
      "altria\n",
      "''\n",
      "tambe\n",
      "''\n",
      "tambe\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "hickenlooper\n",
      "hickenlooper\n",
      "cwa\n",
      "and\n",
      "and\n",
      "annika\n",
      "sorenstam\n",
      "annika\n",
      "sorenstam\n",
      "and\n",
      "''\n",
      "qasim\n",
      "''\n",
      "and\n",
      "nasd\n",
      "nasd\n",
      "and\n",
      "'\n",
      "concordes\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "aftra\n",
      "aftra\n",
      "malawi\n",
      "qaida\n",
      "malawi\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "alibek\n",
      ":\n",
      "''\n",
      "alibek\n",
      "selenski\n",
      "and\n",
      "selenski\n",
      "fischi\n",
      "kernan\n",
      "o'bannon\n",
      "kernan\n",
      "o'bannon\n",
      "and\n",
      "'\n",
      "and\n",
      "'\n",
      "''\n",
      "and\n",
      "bogden\n",
      "and\n",
      "bogden\n",
      "fujitsu\n",
      "fujitsu\n",
      "'\n",
      "and\n",
      "and\n",
      "somers\n",
      ";\n",
      "and\n",
      "somers\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "colgate\n",
      "colgate\n",
      "corixa\n",
      "corixa\n",
      "grey\n",
      "''\n",
      "'new\n",
      "'\n",
      "''\n",
      "and\n",
      "citicorp\n",
      "and\n",
      "citicorp\n",
      "and\n",
      "and\n",
      "mahmoud\n",
      "jordanian\n",
      "aqaba\n",
      "and\n",
      "mahmoud\n",
      "aqaba\n",
      "''\n",
      "and\n",
      "''\n",
      "theoctober\n",
      "''\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "''\n",
      "mabry\n",
      "''\n",
      "mabry\n",
      "xscale\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      ":\n",
      "littleton\n",
      "echostar\n",
      "echostar\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "mclean\n",
      "guillermo\n",
      "and\n",
      "netherlander\n",
      "verkerk\n",
      "guillermo\n",
      "and\n",
      "verkerk\n",
      "and\n",
      "?\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "cancuen\n",
      "vanderbilt\n",
      "and\n",
      "cancun\n",
      "vanderbilt\n",
      "and\n",
      "allergan\n",
      "allergan\n",
      "''\n",
      "''\n",
      "and\n",
      "meagre\n",
      "imclone\n",
      "erbitux\n",
      "imclone\n",
      "levine\n",
      "levine\n",
      "airtran\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "pingeon\n",
      "'\n",
      "pingeon\n",
      "assan\n",
      "and\n",
      "pnc\n",
      "''\n",
      "and\n",
      "rohr\n",
      "rohr\n",
      "and\n",
      "pnc\n",
      "ccag\n",
      "rowland\n",
      "and\n",
      "congolese\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "navistar\n",
      "navistar\n",
      "and\n",
      "nimitz\n",
      "baldacci\n",
      "''\n",
      "baldacci\n",
      "and\n",
      "and\n",
      "and\n",
      "healthpark\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "davey\n",
      "and\n",
      "davey\n",
      "and\n",
      "cronyn\n",
      "cronyn\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "schindler\n",
      "schiavo\n",
      "johnnie\n",
      "''\n",
      "johnnie\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dewhurst\n",
      "''\n",
      "''\n",
      "and\n",
      "antetonitrus\n",
      "antetonitrus\n",
      "and\n",
      "strutt\n",
      "and\n",
      "strutt\n",
      "olvera\n",
      "olvera\n",
      "mohsen\n",
      "zubaidi\n",
      "mohsen\n",
      "zubaidi\n",
      "and\n",
      "''\n",
      "buell\n",
      "and\n",
      "''\n",
      "buell\n",
      "and\n",
      "''\n",
      "gammerman\n",
      "and\n",
      "''\n",
      "gammerman\n",
      "inuit\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "caracas\n",
      "phoberomys\n",
      "'\n",
      "caracas\n",
      "jemaah\n",
      "islamiyah\n",
      "jemaah\n",
      "islamiyah\n",
      "''\n",
      "nayef\n",
      "nayef\n",
      "and\n",
      "and\n",
      "millette\n",
      "mcdonnell\n",
      "mcdonnell\n",
      "doud\n",
      "doud\n",
      "and\n",
      "and\n",
      "pribbenow\n",
      "'\n",
      "pribbenow\n",
      "and\n",
      "clemons\n",
      "'\n",
      "clemons\n",
      "''\n",
      "and\n",
      "o'malley\n",
      "o'malley\n",
      "and\n",
      "''\n",
      "silvester\n",
      "silvester\n",
      "and\n",
      "capps\n",
      "and\n",
      "''\n",
      "capps\n",
      "and\n",
      "''\n",
      "hovan\n",
      "speranza\n",
      "''\n",
      "hovan\n",
      "''\n",
      "speranza\n",
      "''\n",
      "daschle\n",
      "''\n",
      "daschle\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "medimmune\n",
      "gaithersburg\n",
      "medimmune\n",
      "flumist\n",
      "and\n",
      "kaichen\n",
      "teya\n",
      "teya\n",
      "jemaah\n",
      "islamiyah\n",
      "qa'eda\n",
      "samudra\n",
      "qaida\n",
      "jemaah\n",
      "islamiyah\n",
      "ortega\n",
      "''\n",
      "ortega\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "ernst\n",
      "ernst\n",
      "kerrigan\n",
      "and\n",
      "supoyo\n",
      "and\n",
      "kosovan\n",
      "kosovan\n",
      "''\n",
      ":\n",
      "grassley\n",
      "and\n",
      "and\n",
      "geraldine\n",
      "geraldine\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "hammersmith\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "samudra\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "halliburton\n",
      "kbr\n",
      "and\n",
      "''\n",
      "kbr\n",
      "and\n",
      "''\n",
      "halliburton\n",
      "elecia\n",
      "elecia\n",
      "and\n",
      "dbtel\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "''\n",
      "''\n",
      "barrenas\n",
      "colpin\n",
      "centre\n",
      "olaf\n",
      "centre\n",
      "and\n",
      "and\n",
      "ballmer\n",
      "ballmer\n",
      "talabani\n",
      "and\n",
      "''\n",
      "talabani\n",
      "and\n",
      "''\n",
      "organise\n",
      "rosenblatt\n",
      "''\n",
      "greenberg\n",
      "''\n",
      "greenberg\n",
      "and\n",
      "and\n",
      "and\n",
      "rucker\n",
      "and\n",
      "''\n",
      "wolfcale\n",
      "wolfcale\n",
      "and\n",
      "''\n",
      "'\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "peoplesoft\n",
      "and\n",
      "and\n",
      "peoplesoft\n",
      "''\n",
      "and\n",
      "''\n",
      "nicklaus\n",
      "nicklaus\n",
      "''\n",
      "''\n",
      "comdex\n",
      "comdex\n",
      "and\n",
      "tatar\n",
      "tehuacan\n",
      "teotihuacan\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "marce\n",
      "''\n",
      "mirant\n",
      "marce\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "darvish\n",
      "bijan\n",
      "darvish\n",
      "''\n",
      "and\n",
      "and\n",
      "patton\n",
      "and\n",
      "patton\n",
      "and\n",
      "mcgill\n",
      "mcgill\n",
      "grasso\n",
      "grasso\n",
      "moriarty\n",
      "and\n",
      "carella\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "''\n",
      "''\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "and\n",
      "idj\n",
      "and\n",
      "siebel\n",
      "siebel\n",
      "and\n",
      "and\n",
      "''\n",
      "jimenez\n",
      "and\n",
      "''\n",
      "jiménez\n",
      "and\n",
      "and\n",
      "rebell\n",
      "''\n",
      "rebell\n",
      "dotson\n",
      "dennehy\n",
      "dotson\n",
      "dennehy\n",
      "and\n",
      "and\n",
      "jiuquan\n",
      "and\n",
      "and\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "metre\n",
      "''\n",
      "metre\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "aiken\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "schroeder\n",
      "stefani\n",
      "apologise\n",
      "berlusconi\n",
      "stefani\n",
      "silvio\n",
      "berlusconi\n",
      "openmanage\n",
      "and\n",
      "and\n",
      "''\n",
      "moriarty\n",
      "moriarty\n",
      "''\n",
      "lurd\n",
      "ja'neh\n",
      "and\n",
      "kedo\n",
      ":\n",
      "''\n",
      "tiburtina\n",
      "and\n",
      "tiburtina\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "bloomfield\n",
      "taubman\n",
      "cgt\n",
      "and\n",
      "cgt\n",
      "and\n",
      "and\n",
      "and\n",
      "benning\n",
      "benning\n",
      "gambian\n",
      "gambian\n",
      "roush\n",
      "and\n",
      "diarrhoea\n",
      "and\n",
      "kiernan\n",
      "seifert\n",
      "seifert\n",
      "mccloskey\n",
      "mccloskey\n",
      "and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "meps\n",
      "and\n",
      "and\n",
      "honourable\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "favourable\n",
      "igc\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "plough\n",
      "kostunica\n",
      "and\n",
      "koitunica\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "recognise\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "plough\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "'no\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      ";\n",
      "unipersonnelle\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      ";\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "reliques\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumiãšre\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "gayssot\n",
      "gayssot\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "'model\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "flexibilisation\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "uvre\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "organise\n",
      "organise\n",
      "and\n",
      "and\n",
      "meps\n",
      "esb\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "vigour\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "disgracieuse\n",
      "omc\n",
      "organise\n",
      "organise\n",
      "and\n",
      "defence\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "'\n",
      "and\n",
      "fusse\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "organise\n",
      "organise\n",
      "language‒describes\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "réconfortants\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "programme\n",
      "and\n",
      "programme\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "'\n",
      "and\n",
      "plough\n",
      ";\n",
      "unipersonnelle\n",
      "'\n",
      "and\n",
      "fusse\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "overarch\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "disgracieuse\n",
      "and\n",
      "programme\n",
      "programme\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marã­n\n",
      "galicia\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "d'impact\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "meps\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "defence\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "stabilisation\n",
      "and\n",
      "stabilisation\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "'\n",
      "and\n",
      "and\n",
      "stabilisation\n",
      "and\n",
      "recognise\n",
      "stabilisation\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      "neighbourly\n",
      "and\n",
      "neighbourhood\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "defence\n",
      "and\n",
      "meps\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "meps\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ";\n",
      ":\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "fulfil\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "favour\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "organise\n",
      "and\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "margallo\n",
      "marfil\n",
      "meps\n",
      "récréationnelles\n",
      "“\n",
      "”\n",
      "and\n",
      "and\n",
      "uvre\n",
      "plough\n",
      ";\n",
      "unipersonnelle\n",
      ":\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "organisation\n",
      "and\n",
      "organisation\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "disgracieuse\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "and\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "plough\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "'\n",
      "meps\n",
      "and\n",
      "and\n",
      "plough\n",
      "and\n",
      ":\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "and\n",
      "karamanou\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "albania\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "and\n",
      "and\n",
      "organise\n",
      "organise\n",
      "and\n",
      "and\n",
      "favour\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "“\n",
      "”\n",
      "and\n",
      "?\n",
      "plough\n",
      "and\n",
      ":\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "!\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "galicia\n",
      "and\n",
      "and\n",
      "gayssot\n",
      "gayssot\n",
      "''\n",
      "organise\n",
      "'no\n",
      "'\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "igc\n",
      "neighbourly\n",
      "and\n",
      "and\n",
      "'no\n",
      "'\n",
      "and\n",
      "ijaw\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "and\n",
      "defence\n",
      "and\n",
      "defence\n",
      "and\n",
      "and\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "organise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "modernised\n",
      "!\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "montenegro\n",
      "yugoslavia\n",
      "recognise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "slovakia\n",
      "and\n",
      "slovakia\n",
      "and\n",
      ":\n",
      "nº\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      ";\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "récréationnelles\n",
      "and\n",
      "and\n",
      "obtiendraient\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "and\n",
      "?\n",
      "'austrian\n",
      "'\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "modernised\n",
      "modernisation\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      ";\n",
      "unipersonnelle\n",
      "centre\n",
      "and\n",
      "and\n",
      "centre\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "and\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "favour\n",
      "and\n",
      "favour\n",
      "and\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "favourable\n",
      "igc\n",
      "and\n",
      "programme\n",
      "programme\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      "jesuit\n",
      ";\n",
      "ortega\n",
      "linkohr\n",
      "and\n",
      ";\n",
      "favour\n",
      "and\n",
      "favour\n",
      "programme\n",
      "programme\n",
      "kostunica\n",
      "and\n",
      "kostunica\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "marín\n",
      "galicia\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "kostunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "lalumière\n",
      "'\n",
      "and\n",
      "lalumière\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "himara\n",
      "democratisation\n",
      "and\n",
      "albania\n",
      "labour\n",
      "igc\n",
      "labour\n",
      "favourable\n",
      "igc\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "‘\n",
      "'model\n",
      "and\n",
      "?\n",
      "and\n",
      "rã©crã©ationnelles\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "favour\n",
      "garcía\n",
      "margallo\n",
      "marfil\n",
      "and\n",
      "defence\n",
      "and\n",
      "programme\n",
      "programme\n",
      ";\n",
      "unipersonal\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "labelling\n",
      "and\n",
      "labelling\n",
      "and\n",
      "tunisian\n",
      "tunisian\n",
      ":\n",
      ":\n",
      "favour\n",
      "and\n",
      "favour\n",
      "karamanou\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "karamanou\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "tunisian\n",
      "tunisian\n",
      "and\n",
      "and\n",
      ":\n",
      "kostunica\n",
      "democratisation\n",
      "and\n",
      ":\n",
      "koitunica\n",
      ";\n",
      "democratisation\n",
      "and\n",
      "and\n",
      "and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "inflectional\n",
      "offence\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "nonacceptance\n",
      ";\n",
      ";\n",
      "diacritic\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "judgement\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "univalent\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "judgement\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "telic\n",
      "and\n",
      ";\n",
      "fulfil\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "programme\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ":\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "enfranchisment\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "significane\n",
      "and\n",
      ";\n",
      ";\n",
      "inquiringly\n",
      "inquiringly\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      "satifaction\n",
      ";\n",
      "elecate\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "stiching\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "deliving\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "gase\n",
      "rna\n",
      "rna\n",
      ";\n",
      ";\n",
      ";\n",
      "thames\n",
      "bershire\n",
      "thames\n",
      ";\n",
      ";\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "and\n",
      ";\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../train/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    '''for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRvid_clean.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.MSRpar.txt')):\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.SMTeuroparl.txt')):\n",
    "        data+=[(calc_features(*sp))]'''\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRvid.txt')\n",
    "    l1= f.readlines()\n",
    "    l1=[float(w.strip()) for w in l1]\n",
    "    \n",
    "    f= open('../../train/STS.gs.MSRpar.txt')\n",
    "    l2= f.readlines()\n",
    "    l2=[float(w.strip()) for w in l2]\n",
    "    \n",
    "    f= open('../../train/STS.gs.SMTeuroparl.txt')\n",
    "    l3= f.readlines()\n",
    "    l3=[float(w.strip()) for w in l3]\n",
    "    \n",
    "    '''f= open('../../test-gold/STS.gs.MSRvid.txt')\n",
    "    l4= f.readlines()\n",
    "    l4=[float(w.strip()) for w in l4]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.MSRpar.txt')\n",
    "    l5= f.readlines()\n",
    "    l5=[float(w.strip()) for w in l5]\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.SMTeuroparl.txt')\n",
    "    l6= f.readlines()\n",
    "    l6=[float(w.strip()) for w in l6]'''\n",
    "    \n",
    "    label= l1 +l2 + l3\n",
    "    \n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    '''gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer)'''\n",
    "    \n",
    "    gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('../../test-gold/STS.input.surprise.OnWN.txt')):\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('../../test-gold/STS.gs.surprise.OnWN.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02770996,  0.09423828, -0.375     ,  0.08398438,  0.09228516,\n",
       "       -0.04248047,  0.12109375, -0.36328125, -0.16699219, -0.15332031,\n",
       "        0.53125   , -0.03051758, -0.09667969, -0.11914062, -0.11035156,\n",
       "        0.02233887, -0.09521484, -0.04541016, -0.07763672, -0.05957031,\n",
       "        0.08398438,  0.12792969, -0.03735352,  0.22753906, -0.07617188,\n",
       "        0.14257812, -0.22265625,  0.10644531,  0.55078125, -0.30273438,\n",
       "        0.08007812,  0.05126953, -0.17089844,  0.27734375, -0.41796875,\n",
       "       -0.11425781, -0.04736328, -0.11621094,  0.12304688, -0.00271606,\n",
       "       -0.16796875, -0.19921875,  0.06933594,  0.44921875,  0.07373047,\n",
       "       -0.2890625 ,  0.13476562, -0.12353516,  0.23730469,  0.125     ,\n",
       "       -0.03442383, -0.06201172,  0.15527344, -0.00473022,  0.07226562,\n",
       "       -0.08837891,  0.09326172, -0.19726562,  0.01611328, -0.1328125 ,\n",
       "       -0.15820312, -0.20507812,  0.10888672, -0.24511719, -0.12011719,\n",
       "       -0.04882812, -0.453125  , -0.23535156,  0.34960938, -0.09570312,\n",
       "        0.23730469,  0.00680542,  0.03735352,  0.25195312,  0.02429199,\n",
       "        0.09082031,  0.07421875, -0.05102539, -0.32226562,  0.06982422,\n",
       "        0.25195312, -0.24414062, -0.20898438,  0.20800781,  0.15234375,\n",
       "        0.10498047, -0.27539062,  0.07568359, -0.15234375, -0.0222168 ,\n",
       "        0.12109375,  0.546875  , -0.11523438,  0.1796875 ,  0.10205078,\n",
       "       -0.30273438,  0.3125    , -0.13574219,  0.2578125 , -0.03662109,\n",
       "        0.03686523, -0.41601562, -0.02539062,  0.140625  ,  0.09277344,\n",
       "        0.23535156,  0.5       ,  0.0378418 , -0.21972656, -0.15429688,\n",
       "       -0.2265625 , -0.10449219,  0.05810547,  0.25195312,  0.22851562,\n",
       "       -0.29296875, -0.0222168 , -0.21679688, -0.08935547,  0.04370117,\n",
       "       -0.25390625, -0.12060547, -0.15917969,  0.19726562,  0.02075195,\n",
       "        0.28125   ,  0.07763672, -0.11474609,  0.06396484, -0.08984375,\n",
       "       -0.07177734, -0.03320312, -0.02331543,  0.09423828, -0.29296875,\n",
       "        0.01928711, -0.25585938,  0.26367188, -0.09912109,  0.45117188,\n",
       "       -0.05541992, -0.27148438,  0.20703125,  0.20507812,  0.33984375,\n",
       "       -0.03759766, -0.20703125,  0.08496094,  0.25      , -0.21777344,\n",
       "        0.11425781,  0.02062988, -0.47460938, -0.04125977, -0.12890625,\n",
       "       -0.28320312,  0.06542969,  0.02612305, -0.33203125, -0.06298828,\n",
       "       -0.265625  ,  0.16210938,  0.078125  ,  0.2578125 , -0.04345703,\n",
       "        0.02453613,  0.11767578, -0.3046875 , -0.3984375 ,  0.19042969,\n",
       "       -0.2109375 ,  0.36523438, -0.18164062, -0.08837891, -0.25585938,\n",
       "        0.19140625,  0.11425781, -0.14453125, -0.13867188, -0.13085938,\n",
       "       -0.8046875 , -0.13378906, -0.22460938,  0.12402344,  0.07275391,\n",
       "       -0.17675781,  0.19335938, -0.10449219,  0.01464844, -0.10107422,\n",
       "       -0.01055908,  0.01068115, -0.05273438,  0.39453125, -0.16503906,\n",
       "        0.5       , -0.0456543 ,  0.29101562, -0.20019531,  0.02001953,\n",
       "        0.2890625 , -0.24023438,  0.09960938,  0.08740234, -0.15234375,\n",
       "        0.17871094,  0.10839844, -0.1953125 , -0.10058594,  0.02001953,\n",
       "        0.05566406,  0.00104523, -0.07714844, -0.23828125, -0.015625  ,\n",
       "       -0.01147461,  0.34765625, -0.01330566,  0.11523438, -0.23339844,\n",
       "       -0.03955078,  0.13183594, -0.03735352,  0.26367188, -0.0100708 ,\n",
       "        0.2578125 ,  0.14355469,  0.13769531, -0.0014267 ,  0.11474609,\n",
       "        0.06542969, -0.25585938, -0.11328125, -0.11083984,  0.41796875,\n",
       "        0.25585938,  0.4453125 ,  0.38085938, -0.21484375, -0.18066406,\n",
       "        0.07373047, -0.18945312, -0.12695312,  0.0168457 ,  0.00256348,\n",
       "        0.11132812, -0.15332031, -0.21875   ,  0.2265625 , -0.05078125,\n",
       "        0.0480957 ,  0.47265625,  0.24414062,  0.0480957 ,  0.04785156,\n",
       "       -0.07128906,  0.09521484, -0.11132812, -0.21484375, -0.44140625,\n",
       "       -0.25976562,  0.02575684,  0.30859375,  0.20800781, -0.18359375,\n",
       "        0.15820312, -0.140625  ,  0.10253906, -0.27734375,  0.26171875,\n",
       "        0.22265625,  0.17382812, -0.17382812,  0.29296875,  0.46679688,\n",
       "       -0.24121094,  0.10253906,  0.1484375 , -0.02368164, -0.05444336,\n",
       "        0.29101562, -0.22851562,  0.05859375, -0.41992188,  0.03417969,\n",
       "        0.13574219, -0.06884766,  0.0144043 , -0.0189209 ,  0.02355957,\n",
       "       -0.30078125,  0.01867676, -0.09423828,  0.09667969,  0.04223633,\n",
       "        0.15625   , -0.15429688, -0.11035156,  0.01379395,  0.06689453],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec['safari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7027632945072444\n"
     ]
    }
   ],
   "source": [
    "with open('full.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('full.txt',))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULT: (Pearson: 0.87911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l='The bird is bathing in the sink.\\tBirdie is washing itself in the water basin.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa, sb = tuple(nltk.word_tokenize(s)\n",
    "                          for s in l.strip().split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "se=[]\n",
    "se.append((nltk.pos_tag(sa), nltk.pos_tag(sb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('The', 'DT'),\n",
       "   ('bird', 'NN'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('bathing', 'VBG'),\n",
       "   ('in', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('sink', 'NN'),\n",
       "   ('.', '.')],\n",
       "  [('Birdie', 'NNP'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('washing', 'VBG'),\n",
       "   ('itself', 'PRP'),\n",
       "   ('in', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('water', 'NN'),\n",
       "   ('basin', 'NN'),\n",
       "   ('.', '.')])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa,sb=se[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lema = get_lemmatized_words(sa)\n",
    "lemb = get_lemmatized_words(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird', 'bathe', 'sink']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['birdie', 'wash', 'itself', 'water', 'basin']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5526149829037504"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=weighted_wvec_sim(lema,lemb)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wvec_sim(la,lb):\n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    for i in la:\n",
    "        try:\n",
    "            veca+= word_vec[i]/norm(word_vec[i]+ 1e-8)\n",
    "        except KeyError:\n",
    "            print(i)\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "    for i in lb:\n",
    "        try:\n",
    "            vecb+= word_vec[i]/norm(word_vec[i] + 1e-8)\n",
    "        except KeyError:\n",
    "            print(i)\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "        \n",
    "    return abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca=lema\n",
    "lcb=lemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05419922,  0.01708984, -0.00527954,  0.33203125, -0.25      ,\n",
       "       -0.01397705, -0.15039062, -0.265625  ,  0.01647949,  0.3828125 ,\n",
       "       -0.03295898, -0.09716797, -0.16308594, -0.04443359,  0.00946045,\n",
       "        0.18457031,  0.03637695,  0.16601562,  0.36328125, -0.25585938,\n",
       "        0.375     ,  0.171875  ,  0.21386719, -0.19921875,  0.13085938,\n",
       "       -0.07275391, -0.02819824,  0.11621094,  0.15332031,  0.09082031,\n",
       "        0.06787109, -0.0300293 , -0.16894531, -0.20800781, -0.03710938,\n",
       "       -0.22753906,  0.26367188,  0.012146  ,  0.18359375,  0.31054688,\n",
       "       -0.10791016, -0.19140625,  0.21582031,  0.13183594, -0.03515625,\n",
       "        0.18554688, -0.30859375,  0.04785156, -0.10986328,  0.14355469,\n",
       "       -0.43554688, -0.0378418 ,  0.10839844,  0.140625  , -0.10595703,\n",
       "        0.26171875, -0.17089844,  0.39453125,  0.12597656, -0.27734375,\n",
       "       -0.28125   ,  0.14746094, -0.20996094,  0.02355957,  0.18457031,\n",
       "        0.00445557, -0.27929688, -0.03637695, -0.29296875,  0.19628906,\n",
       "        0.20703125,  0.2890625 , -0.20507812,  0.06787109, -0.43164062,\n",
       "       -0.10986328, -0.2578125 , -0.02331543,  0.11328125,  0.23144531,\n",
       "       -0.04418945,  0.10839844, -0.2890625 , -0.09521484, -0.10351562,\n",
       "       -0.0324707 ,  0.07763672, -0.13378906,  0.22949219,  0.06298828,\n",
       "        0.08349609,  0.02929688, -0.11474609,  0.00534058, -0.12988281,\n",
       "        0.02514648,  0.08789062,  0.24511719, -0.11474609, -0.296875  ,\n",
       "       -0.59375   , -0.29492188, -0.13378906,  0.27734375, -0.04174805,\n",
       "        0.11621094,  0.28320312,  0.00241089,  0.13867188, -0.00683594,\n",
       "       -0.30078125,  0.16210938,  0.01171875, -0.13867188,  0.48828125,\n",
       "        0.02880859,  0.02416992,  0.04736328,  0.05859375, -0.23828125,\n",
       "        0.02758789,  0.05981445, -0.03857422,  0.06933594,  0.14941406,\n",
       "       -0.10888672, -0.07324219,  0.08789062,  0.27148438,  0.06591797,\n",
       "       -0.37890625, -0.26171875, -0.13183594,  0.09570312, -0.3125    ,\n",
       "        0.10205078,  0.03063965,  0.23632812,  0.00582886,  0.27734375,\n",
       "        0.20507812, -0.17871094, -0.31445312, -0.01586914,  0.13964844,\n",
       "        0.13574219,  0.0390625 , -0.29296875,  0.234375  , -0.33984375,\n",
       "       -0.11816406,  0.10644531, -0.18457031, -0.02099609,  0.02563477,\n",
       "        0.25390625,  0.07275391,  0.13574219, -0.00138092, -0.2578125 ,\n",
       "       -0.2890625 ,  0.10107422,  0.19238281, -0.04882812,  0.27929688,\n",
       "       -0.3359375 , -0.07373047,  0.01879883, -0.10986328, -0.04614258,\n",
       "        0.15722656,  0.06689453, -0.03417969,  0.16308594,  0.08642578,\n",
       "        0.44726562,  0.02026367, -0.01977539,  0.07958984,  0.17773438,\n",
       "       -0.04370117, -0.00952148,  0.16503906,  0.17285156,  0.23144531,\n",
       "       -0.04272461,  0.02355957,  0.18359375, -0.41601562, -0.01745605,\n",
       "        0.16796875,  0.04736328,  0.14257812,  0.08496094,  0.33984375,\n",
       "        0.1484375 , -0.34375   , -0.14160156, -0.06835938, -0.14648438,\n",
       "       -0.02844238,  0.07421875, -0.07666016,  0.12695312,  0.05859375,\n",
       "       -0.07568359, -0.03344727,  0.23632812, -0.16308594,  0.16503906,\n",
       "        0.1484375 , -0.2421875 , -0.3515625 , -0.30664062,  0.00491333,\n",
       "        0.17675781,  0.46289062,  0.14257812, -0.25      , -0.25976562,\n",
       "        0.04370117,  0.34960938,  0.05957031,  0.07617188, -0.02868652,\n",
       "       -0.09667969, -0.01281738,  0.05859375, -0.22949219, -0.1953125 ,\n",
       "       -0.12207031,  0.20117188, -0.42382812,  0.06005859,  0.50390625,\n",
       "        0.20898438,  0.11230469, -0.06054688,  0.33203125,  0.07421875,\n",
       "       -0.05786133,  0.11083984, -0.06494141,  0.05639648,  0.01757812,\n",
       "        0.08398438,  0.13769531,  0.2578125 ,  0.16796875, -0.16894531,\n",
       "        0.01794434,  0.16015625,  0.26171875,  0.31640625, -0.24804688,\n",
       "        0.05371094, -0.0859375 ,  0.17089844, -0.39453125, -0.00156403,\n",
       "       -0.07324219, -0.04614258, -0.16210938, -0.15722656,  0.21289062,\n",
       "       -0.15820312,  0.04394531,  0.28515625,  0.01196289, -0.26953125,\n",
       "       -0.04370117,  0.37109375,  0.04663086, -0.19726562,  0.3046875 ,\n",
       "       -0.36523438, -0.23632812,  0.08056641, -0.04248047, -0.14648438,\n",
       "       -0.06225586, -0.0534668 , -0.05664062,  0.18945312,  0.37109375,\n",
       "       -0.22070312,  0.04638672,  0.02612305, -0.11474609,  0.265625  ,\n",
       "       -0.02453613,  0.11083984, -0.02514648, -0.12060547,  0.05297852,\n",
       "        0.07128906,  0.00063705, -0.36523438, -0.13769531, -0.12890625],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k={1:'d',2:'c',3:'g',4:'r',5:'t'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(k)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1915712.0),\n",
       " (1740, 1915712.0),\n",
       " (1930, 859272.0),\n",
       " (2137, 1055337.0),\n",
       " (2452, 36243.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(brown_ic['n'].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import *\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader('.', ['american-english.txt','british-english.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['american-english.txt', 'british-english.txt']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=wordlists.words('american-english.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151657"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
