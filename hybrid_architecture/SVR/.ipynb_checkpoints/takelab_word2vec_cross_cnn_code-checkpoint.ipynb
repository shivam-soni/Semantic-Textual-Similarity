{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "TKfMxbqm-ALz",
    "outputId": "fc33dd01-3774-42e9-d787-fc90e073c21c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\shugupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shugupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shugupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shugupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "8UwfEH0Q-ALe",
    "outputId": "f0d5f4c1-bbf5-4517-cbff-d46d8cfbb8e0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "okI6BDtDEzp4",
    "outputId": "6f6f373b-a1a3-452f-a384-34743d2b2044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "_kS0gbDZF8wh",
    "outputId": "14fcc6d3-53c8-42c7-ccbc-adaf2aae8996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'drive/My Drive'\n",
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "#cd 'drive/My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCnUcn7F8MA5"
   },
   "source": [
    "# present directory Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "eVodxCLXF_Ih",
    "outputId": "4c833625-ec74-45ab-99ac-95cf5816b15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34m2017_takelab+cnn\u001b[0m/                     sts-test.csv\n",
      " \u001b[01;34m2017_takelab_MLP\u001b[0m/                     sts-train.csv\n",
      " \u001b[01;34m2017_takelab_SVR\u001b[0m/                     takelab_jc.txt\n",
      "\u001b[01;34m'Colab Notebooks'\u001b[0m/                     takelab_lin.txt\n",
      " Copy_word2vec_kl_cnn_code.ipynb       takelab_output_MSRpar.txt\n",
      " full.txt                              temp.ipynb\n",
      "'Getting started.pdf'                  temp_word2vec_kl.ipynb\n",
      " GoogleNews-vectors-negative300.bin    \u001b[01;34mthesis\u001b[0m/\n",
      " jc_lin.txt                            word2vec_cross_cnn_code.ipynb\n",
      " model_cifar.pt                        word2vec_cross_cnn.ipynb\n",
      " processed_full.txt                    word2vec_cross_epoch.pt\n",
      " processed_jc_lin.txt                  word2vec_cross_model.pt\n",
      " processed_takelab_jc.txt              word2vec_kl_cnn_code.ipynb\n",
      " processed_takelab_lin.txt             word2vec_kl_epoch.pt\n",
      " processed_takelab_output_MSRpar.txt   word2vec_kl_model.pt\n",
      " processed_wv_wlsa.txt                 wv_wlsa.txt\n",
      " processed_wv_wwv.txt                  wv_wwv.txt\n",
      " sts-dev.csv\n"
     ]
    }
   ],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.0.0.tar.gz (103 kB)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.13.19.tar.gz (97 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Collecting botocore<1.17.0,>=1.16.19\n",
      "  Downloading botocore-1.16.19-py2.py3-none-any.whl (6.2 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\shugupta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.0.0-py3-none-any.whl size=101346 sha256=7dd253ac5b92b7611ff3a4007aa3094bdd9574ea45e3b248b7fc9f4bd0e890e5\n",
      "  Stored in directory: c:\\users\\shugupta\\appdata\\local\\pip\\cache\\wheels\\bb\\1c\\9c\\412ec03f6d5ac7d41f4b965bde3fc0d1bd201da5ba3e2636de\n",
      "  Building wheel for boto3 (setup.py): started\n",
      "  Building wheel for boto3 (setup.py): finished with status 'done'\n",
      "  Created wheel for boto3: filename=boto3-1.13.19-py2.py3-none-any.whl size=127658 sha256=64e814f3c593b03238224e87869248725c8f4eede7eec306021384432fc66ef9\n",
      "  Stored in directory: c:\\users\\shugupta\\appdata\\local\\pip\\cache\\wheels\\bf\\3a\\84\\9561839c691a06e6f4db54b26aea99f884be1da98dc5cb4222\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.13\n",
      "    Uninstalling Cython-0.29.13:\n",
      "      Successfully uninstalled Cython-0.29.13\n",
      "Successfully installed Cython-0.29.14 boto3-1.13.19 botocore-1.16.19 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWjwev8y-AL8"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFkreosu-AMC"
   },
   "outputs": [],
   "source": [
    "eng_vec_path = '../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "wj5b2nTm-AMJ",
    "outputId": "799f93b4-b677-4fc4-8fa9-54c11e1ff8f6"
   },
   "outputs": [],
   "source": [
    "word_vec = gensim.models.KeyedVectors.load_word2vec_format(eng_vec_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bKLUOEU-AMQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sim:\n",
    "    def __init__(self, words, vectors):\n",
    "        self.word_to_idx = {a: b for b, a in\n",
    "                            enumerate(w.strip() for w in open(words))}\n",
    "        self.mat = np.loadtxt(vectors)\n",
    "\n",
    "    def bow_vec(self, b):\n",
    "        vec = np.zeros(self.mat.shape[1])\n",
    "        for k, v in b.items():\n",
    "            idx = self.word_to_idx.get(k, -1)\n",
    "            if idx >= 0:\n",
    "                vec += self.mat[idx] / (norm(self.mat[idx]) + 1e-8) * v\n",
    "        return vec\n",
    "\n",
    "    def calc(self, b1, b2):\n",
    "        v1 = self.bow_vec(b1)\n",
    "        v2 = self.bow_vec(b2)\n",
    "        return abs(v1.dot(v2) / (norm(v1) + 1e-8) / (norm(v2) + 1e-8))\n",
    "\n",
    "stopwords = set([\n",
    "\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n",
    "\"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \"this\", \"to\",\n",
    "\"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\", \"the\", \"'s\", \"did\",\n",
    "\"have\", \"has\", \"had\", \"were\", \"'ll\"\n",
    "])\n",
    "\n",
    "nyt_sim = Sim('2017_takelab+cnn/nyt_words.txt', '2017_takelab+cnn/nyt_word_vectors.txt')\n",
    "wiki_sim = Sim('2017_takelab+cnn/wikipedia_words.txt', '2017_takelab+cnn/wikipedia_word_vectors.txt')\n",
    "\n",
    "def fix_compounds(a, b):\n",
    "    sb = set(x.lower() for x in b)\n",
    "\n",
    "    a_fix = []\n",
    "    la = len(a)\n",
    "    i = 0\n",
    "    while i < la:\n",
    "        if i + 1 < la:\n",
    "            comb = a[i] + a[i + 1]\n",
    "            if comb.lower() in sb:\n",
    "                a_fix.append(a[i] + a[i + 1])\n",
    "                i += 2\n",
    "                continue\n",
    "        a_fix.append(a[i])\n",
    "        i += 1\n",
    "    return a_fix\n",
    "\n",
    "def load_data(path):\n",
    "    sentences_pos = []\n",
    "    r1 = re.compile(r'\\<([^ ]+)\\>')\n",
    "    r2 = re.compile(r'\\$US(\\d)')\n",
    "    for l in open(path, encoding= 'utf-8'):\n",
    "        #l = l.decode('utf-8')\n",
    "        l = l.replace(u'’', \"'\")\n",
    "        l = l.replace(u'``', '\"')\n",
    "        l = l.replace(u\"''\", '\"')\n",
    "        l = l.replace(u\"—\", '--')\n",
    "        l = l.replace(u\"–\", '--')\n",
    "        l = l.replace(u\"´\", \"'\")\n",
    "        l = l.replace(u\"-\", \" \")\n",
    "        l = l.replace(u\"/\", \" \")\n",
    "        l = r1.sub(r'\\1', l)\n",
    "        l = r2.sub(r'$\\1', l)\n",
    "        s = l.strip().split('\\t')\n",
    "        sa, sb = tuple(nltk.word_tokenize(s)\n",
    "                          for s in l.strip().split('\\t'))\n",
    "        '''sa, sb = ([x.encode('utf-8') for x in sa],\n",
    "                  [x.encode('utf-8') for x in sb])'''\n",
    "\n",
    "        for s in (sa, sb):\n",
    "            for i in range(len(s)):\n",
    "                if s[i] == \"n't\":\n",
    "                    s[i] = \"not\"\n",
    "                elif s[i] == \"'m\":\n",
    "                    s[i] = \"am\"\n",
    "        sa, sb = fix_compounds(sa, sb), fix_compounds(sb, sa)\n",
    "        sentences_pos.append((nltk.pos_tag(sa), nltk.pos_tag(sb)))\n",
    "    return sentences_pos\n",
    "\n",
    "def load_wweight_table(path):\n",
    "    lines = open(path, encoding= 'utf-8').readlines()\n",
    "    wweight = defaultdict(float)\n",
    "    if not len(lines):\n",
    "        return (wweight, 0.)\n",
    "    totfreq = int(lines[0])\n",
    "    for l in lines[1:]:\n",
    "        w, freq = l.split()\n",
    "        freq = float(freq)\n",
    "        if freq < 10:\n",
    "            continue\n",
    "        wweight[w] = math.log(totfreq / freq)\n",
    "\n",
    "    return wweight\n",
    "\n",
    "wweight = load_wweight_table('2017_takelab+cnn/word-frequencies.txt')\n",
    "minwweight = min(wweight.values())\n",
    "\n",
    "def len_compress(l):\n",
    "    return math.log(1. + l)\n",
    "\n",
    "to_wordnet_tag = {\n",
    "        'NN':wordnet.NOUN,\n",
    "        'JJ':wordnet.ADJ,\n",
    "        'VB':wordnet.VERB,\n",
    "        'RB':wordnet.ADV\n",
    "    }\n",
    "\n",
    "word_matcher = re.compile('[^0-9,.(=)\\[\\]/_`]+$')\n",
    "def is_word(w):\n",
    "    return word_matcher.match(w) is not None\n",
    "\n",
    "def get_locase_words(spos):\n",
    "    return [x[0].lower() for x in spos\n",
    "            if is_word(x[0])]\n",
    "\n",
    "def make_ngrams(l, n):\n",
    "    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n",
    "    rez.append(l[n - 1:])\n",
    "    return list(zip(*rez))\n",
    "\n",
    "def wvec_sim(la,lb):\n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    for i in la:\n",
    "        try:\n",
    "            veca+= word_vec[i]/norm(word_vec[i]+ 1e-8)\n",
    "        except KeyError:\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "    for i in lb:\n",
    "        try:\n",
    "            vecb+= word_vec[i]/norm(word_vec[i] + 1e-8)\n",
    "        except:\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "        \n",
    "    return abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def weighted_wvec_sim(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    \n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    \n",
    "    for k,v in wa.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             veca+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "\n",
    "    for k,v in wb.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             vecb+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "        \n",
    "    return  abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def dist_sim(sim, la, lb):\n",
    "    wa = Counter(la)\n",
    "    wb = Counter(lb)\n",
    "    d1 = {x:1 for x in wa}\n",
    "    d2 = {x:1 for x in wb}\n",
    "    return sim.calc(d1, d2)\n",
    "\n",
    "def weighted_dist_sim(sim, lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    return sim.calc(wa, wb)\n",
    "\n",
    "def weighted_word_match(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wsuma = sum(wweight[w] * wa[w] for w in wa)\n",
    "    wsumb = sum(wweight[w] * wb[w] for w in wb)\n",
    "    wsum = 0.\n",
    "\n",
    "    for w in wa:\n",
    "        wd = min(wa[w], wb[w])\n",
    "        wsum += wweight[w] * wd\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if wsuma > 0 and wsum > 0:\n",
    "        p = wsum / wsuma\n",
    "    if wsumb > 0 and wsum > 0:\n",
    "        r = wsum / wsumb\n",
    "    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "wpathsimcache = {}\n",
    "def wpathsim(a, b):\n",
    "    \n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache:\n",
    "        return wpathsimcache[p]\n",
    "    if a == b:\n",
    "        wpathsimcache[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    mx = max([wa.path_similarity(wb)\n",
    "              for wa in sa\n",
    "              for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n",
    "    wpathsimcache[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_jc = {}\n",
    "def wpathsim_jc(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_jc:\n",
    "        return wpathsimcache_jc[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_jc[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.jcn_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_jc[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_lin = {}\n",
    "def wpathsim_lin(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_lin:\n",
    "        return wpathsimcache_lin[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_lin[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.lin_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_lin[p] = mx\n",
    "    return mx\n",
    "\n",
    "def calc_wn_prec(lema, lemb,val):\n",
    "    rez = 0.\n",
    "    for a in lema:\n",
    "        ms = 0.\n",
    "        for b in lemb:\n",
    "            if(val==0):\n",
    "                ms = max(ms, wpathsim(a, b))\n",
    "            elif(val==1):\n",
    "                ms = max(ms, wpathsim_jc(a, b))\n",
    "            else:\n",
    "                ms = max(ms, wpathsim_lin(a, b))\n",
    "                \n",
    "        rez += ms\n",
    "    return rez / len(lema)\n",
    "\n",
    "def wn_sim_match(lema, lemb,val):\n",
    "    f1 = 1.\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if len(lema) > 0 and len(lemb) > 0:\n",
    "        p = calc_wn_prec(lema, lemb,val)\n",
    "        r = calc_wn_prec(lemb, lema,val)\n",
    "        f1 = 2. * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def ngram_match(sa, sb, n):\n",
    "    nga = make_ngrams(sa, n)\n",
    "    ngb = make_ngrams(sb, n)\n",
    "    matches = 0\n",
    "    c1 = Counter(nga)\n",
    "    for ng in ngb:\n",
    "        if c1[ng] > 0:\n",
    "            c1[ng] -= 1\n",
    "            matches += 1\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    f1 = 1.\n",
    "    if len(nga) > 0 and len(ngb) > 0:\n",
    "        p = matches / float(len(nga))\n",
    "        r = matches / float(len(ngb))\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def get_lemmatized_words(sa):\n",
    "    rez = []\n",
    "    for w, wpos in sa:\n",
    "        w = w.lower()\n",
    "        if w in stopwords or not is_word(w):\n",
    "            continue\n",
    "        wtag = to_wordnet_tag.get(wpos[:2])\n",
    "        if wtag is None:\n",
    "            wlem = w\n",
    "        else:\n",
    "            wlem = wordnet.morphy(w, wtag) or w\n",
    "        rez.append(wlem)\n",
    "    return rez\n",
    "\n",
    "def is_stock_tick(w):\n",
    "    return w[0] == '.' and len(w) > 1 and w[1:].isupper()\n",
    "\n",
    "def stocks_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa if is_stock_tick(x[0]))\n",
    "    cb = set(x[0] for x in sb if is_stock_tick(x[0]))\n",
    "    isect = len(ca.intersection(cb))\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "def case_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    cb = set(x[0] for x in sb[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "    isect = len(ca.intersection(cb))\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "risnum = re.compile(r'^[0-9,./-]+$')\n",
    "rhasdigit = re.compile(r'[0-9]')\n",
    "\n",
    "def match_number(xa, xb):\n",
    "    if xa == xb:\n",
    "        return True\n",
    "    xa = xa.replace(',', '')\n",
    "    xb = xb.replace(',', '')\n",
    "\n",
    "    try:\n",
    "        va = int(float(xa))\n",
    "        vb = int(float(xb))\n",
    "        if (va == 0 or vb == 0) and va != vb:\n",
    "            return False\n",
    "        fxa = float(xa)\n",
    "        fxb = float(xb)\n",
    "        if abs(fxa - fxb) > 1:\n",
    "            return False\n",
    "        diga = xa.find('.')\n",
    "        digb = xb.find('.')\n",
    "        diga = 0 if diga == -1 else len(xa) - diga - 1\n",
    "        digb = 0 if digb == -1 else len(xb) - digb - 1\n",
    "        if diga > 0 and digb > 0 and va != vb:\n",
    "            return False\n",
    "        dmin = min(diga, digb)\n",
    "        if dmin == 0:\n",
    "            if abs(round(fxa, 0) - round(fxb, 0)) < 1e-5:\n",
    "                return True\n",
    "            return va == vb\n",
    "        return abs(round(fxa, dmin) - round(fxb, dmin)) < 1e-5\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "def number_features(sa, sb):\n",
    "    numa = set(x[0] for x in sa if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    numb = set(x[0] for x in sb if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    isect = 0\n",
    "    for na in numa:\n",
    "        if na in numb:\n",
    "            isect += 1\n",
    "            continue\n",
    "        for nb in numb:\n",
    "            if match_number(na, nb):\n",
    "                isect += 1\n",
    "                break\n",
    "\n",
    "    la, lb = len(numa), len(numb)\n",
    "\n",
    "    f = 1.\n",
    "    subset = 0.\n",
    "    if la + lb > 0:\n",
    "        if isect == la or isect == lb:\n",
    "            subset = 1.\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2. * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f, subset)\n",
    "\n",
    "def relative_len_difference(lca, lcb):\n",
    "    la, lb = len(lca), len(lcb)\n",
    "    return abs(la - lb) / float(max(la, lb) + 1e-5)\n",
    "\n",
    "def relative_ic_difference(lca, lcb):\n",
    "    #wa = sum(wweight[x] for x in lca)\n",
    "    #wb = sum(wweight[x] for x in lcb)\n",
    "    wa = sum(max(0., wweight[x] - minwweight) for x in lca)\n",
    "    wb = sum(max(0., wweight[x] - minwweight) for x in lcb)\n",
    "    return abs(wa - wb) / (max(wa, wb) + 1e-5)\n",
    "\n",
    "def my_custom_function(y_true, y_predict):\n",
    "    corr,_ = pearsonr(y_true, y_predict)\n",
    "    return corr\n",
    "\n",
    "    \n",
    " #   gs = GridSearchCV(clf, param_grid={'kernel': [ 'rbf'],'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='roc_auc')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQ_guUmm-AMZ"
   },
   "source": [
    "# Incorporating CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJ7PwTXp-AMb"
   },
   "outputs": [],
   "source": [
    "c = dict()\n",
    "c['num_runs']   = 3\n",
    "c['num_epochs'] = 64\n",
    "c['num_batchs'] = 2\n",
    "c['batch_size'] = 3000\n",
    "c['wordvectdim']  = 300\n",
    "c['sentencepad']  = 60\n",
    "c['num_classes']  = 6\n",
    "c['cnnfilters']     = {1: 1800}\n",
    "c['cnninitial']     = 'he_uniform'\n",
    "c['cnnactivate']    = 'relu'\n",
    "c['densedimension'] = list([1800])\n",
    "c['denseinitial']   = 'he_uniform'\n",
    "c['denseactivate']  = 'tanh'\n",
    "c['optimizer']  = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7M4zoUq-zQ5"
   },
   "source": [
    "## Defining CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zy3Raybu92Gw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random,copy,string\n",
    "from numpy import linalg\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iK8lfly9784"
   },
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "  def __init__(self,nh):\n",
    "    super(CNN_Model, self).__init__()\n",
    "    self.cnn= nn.Sequential(nn.Conv1d(300,1800,1),              #(bs,300,60) -> (bs, 1800, 60)\n",
    "                            nn.LeakyReLU(0.01),\n",
    "                            nn.MaxPool1d(kernel_size=60))       #(bs,1800,60)-> (bs, 1800, 1)\n",
    "\n",
    "    self.Linear= nn.Sequential(nn.Linear(3600,nh),            #(bs, 3600)  -> (bs, 1800 )\n",
    "                               nn.LeakyReLU(0.01),\n",
    "                               nn.Linear(nh,6),               #(bs,1800)   -> (bs,6)\n",
    "                               nn.Softmax(dim=1))\n",
    "                            \n",
    "    \n",
    "\n",
    "  def forward(self,input1, input2):\n",
    "    input1= self.cnn(input1)\n",
    "    input2= self.cnn(input2)\n",
    "    #print(\"input1=\",input1.shape)\n",
    "    #print(\"input2=\",input2.shape)\n",
    "    input1= torch.flatten(input1,1)\n",
    "    input2= torch.flatten(input2,1)\n",
    "    #print(\"input1=\",input1.shape)\n",
    "    #print(\"input2=\",input2.shape)\n",
    "    absdiff= abs(input1 - input2)\n",
    "    mulDifference= input1 * input2\n",
    "    concatenate = torch.cat((absdiff,mulDifference),1)\n",
    "    #print(concatenate.shape)\n",
    "    output= self.Linear(concatenate)\n",
    "    #print(output.shape)\n",
    "    return (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiPtFQegBPC1"
   },
   "source": [
    "## loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLXpKk4E-fjY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvD2gYHSBjvF"
   },
   "source": [
    "## Defining Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-3T_GQiBiyq"
   },
   "outputs": [],
   "source": [
    "def matrixize(sentencelist, sentencepad):\n",
    "  padding= np.zeros(300)\n",
    "  matrix= np.zeros((len(sentencelist),sentencepad,300 ))\n",
    "  m=0\n",
    "  s=1\n",
    "\n",
    "  for i in range(len(sentencelist)):\n",
    "    for j in range(len(sentencelist[i])):\n",
    "      try:\n",
    "        matrix[i][j]= word_vec[sentencelist[i][j]]\n",
    "      except:\n",
    "        #print(sentencelist[i][j])\n",
    "        matrix[i][j]= np.random.normal(m,s,300)/linalg.norm(np.random.normal(m,s,300) + 1e-8)\n",
    "\n",
    "  return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3kyXgqbBxsj"
   },
   "outputs": [],
   "source": [
    "def _load_data(filename):\n",
    "        s0,s1,labels = [],[],[]\n",
    "        lines=open(filename,'r', encoding='utf-8').read().splitlines()\n",
    "        for line in lines:\n",
    "            _,_,_,_, label, s0x, s1x = line.rstrip().split('\\t')[:7]\n",
    "            labels.append(float(label))\n",
    "            s0.append([word.lower() for word in word_tokenize(s0x) if word not in string.punctuation])\n",
    "            s1.append([word.lower() for word in word_tokenize(s1x) if word not in string.punctuation])\n",
    "\n",
    "        m0 = matrixize(s0, c['sentencepad'])\n",
    "        m1 = matrixize(s1, c['sentencepad'])\n",
    "        classes = np.zeros((len(labels),c['num_classes']))\n",
    "        for i, label in enumerate(labels): # making probability distribution of classes\n",
    "            if np.floor(label) + 1 < c['num_classes']:\n",
    "                classes[i, int(np.floor(label)) + 1] = label - np.floor(label)\n",
    "            classes[i, int(np.floor(label))] = np.floor(label) - label + 1\n",
    "            \n",
    "        return {'labels': labels, 's0': s0, 's1': s1, 'classes': classes, 'm0': m0, 'm1': m1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZaN-cmfCOse"
   },
   "source": [
    "## Sampling Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70Mf-YCDCSl4"
   },
   "outputs": [],
   "source": [
    "def _sample_pairs(data,batch_size):\n",
    "  datacopy={}\n",
    "  for i in data.keys():\n",
    "    datacopy[i]= []\n",
    "  for i in range(batch_size):\n",
    "    index = np.random.randint(len(data['labels']))\n",
    "    #print(index)\n",
    "    for key,value in data.items():\n",
    "      datacopy[key].append(value[index])\n",
    "  datacopy['classes']= torch.tensor(datacopy['classes'])\n",
    "  datacopy['m0']= torch.tensor(datacopy['m0'])\n",
    "  datacopy['m1']= torch.tensor(datacopy['m1'])\n",
    "  return datacopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nnwSpFC_Az3"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_MrAPMI-KPd"
   },
   "outputs": [],
   "source": [
    "def eval_setup(net,batch_size,data, device = 'cpu'):\n",
    "  \n",
    "  net.load_state_dict(torch.load('word2vec_kl_adam_model.pt'))\n",
    "  net.eval().to(device )\n",
    "  testload= _load_data(data)\n",
    "  test_n_batch= int(len(testload['labels'])/batch_size)\n",
    "  #print(len(testload['labels']))\n",
    "  test_loss_arr = np.zeros(test_n_batch+ 1)\n",
    "\n",
    "\n",
    "  data= _sample_pairs(testload,batch_size)\n",
    "  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype= torch.float32)\n",
    "  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype= torch.float32)\n",
    "  pred_output= (net(input1,input2)).to('cpu')\n",
    "  pred= pred_output.detach().numpy()\n",
    "  prediction = np.dot(np.array(pred),np.arange(c['num_classes']))\n",
    "  print('pred_shape',prediction.shape)\n",
    "  goldlabels = data['labels']\n",
    "  result=round((pearsonr(prediction, goldlabels)[0]),4)\n",
    "  print(\"result\",result)\n",
    "\n",
    "  return (prediction,result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fs5pQpe1Cpwm"
   },
   "source": [
    "## Loading And Evaluating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCaPK9MaGMFK"
   },
   "outputs": [],
   "source": [
    "net= CNN_Model(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "Ml7V5QyID6Ax",
    "outputId": "6c09d0fa-54bd-447b-ceea-bd673fa7d6bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_shape (5749,)\n",
      "result 0.6958\n"
     ]
    }
   ],
   "source": [
    "train_pred= eval_setup(net,batch_size = 5749,data='sts-train.csv', device ='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "4e98t3fvFpD_",
    "outputId": "d1a1eb7b-07f3-4fb8-da0e-70ac7cb55e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_shape (1379,)\n",
      "result 0.7\n"
     ]
    }
   ],
   "source": [
    "test_pred = eval_setup(net,batch_size = 1379, data='sts-test.csv',device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BpwX1WuKWGv"
   },
   "source": [
    "# Reshaping scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RLJ0TUaJJV_"
   },
   "outputs": [],
   "source": [
    "train_cnn=train_pred[0].reshape(5749,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-1MmIvVL2Mo"
   },
   "outputs": [],
   "source": [
    "test_cnn= test_pred[0].reshape(1379,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXun5j-eMn5C"
   },
   "source": [
    "# Post processing of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnUq0PcU-ANk"
   },
   "outputs": [],
   "source": [
    "def post_process(path):\n",
    "    import sys\n",
    "\n",
    "\n",
    "    orig = []\n",
    "    for l in open('2017_takelab+cnn/test/sts_test.txt',encoding='utf-8'):\n",
    "        orig.append([x.strip() for x in l.lower().split(\"\\t\")])\n",
    "\n",
    "    scores = list(map(float, open(path).readlines()))\n",
    "\n",
    "    if len(orig) != len(scores):\n",
    "        print (sys.stderr, \"Error: inputs should have the same number of lines\")\n",
    "        exit(1)\n",
    "    processed=[]\n",
    "    f = open('processed_'+path, 'w')\n",
    "    for i, s in enumerate(scores):\n",
    "        if orig[0] == orig[1]:\n",
    "            s = 5.\n",
    "        if s > 5:\n",
    "            s = 5.\n",
    "        if s < 0:\n",
    "            s = 0.\n",
    "        processed.append(s)\n",
    "        #print (s)\n",
    "        f.write(\"%s\\n\" % s)\n",
    "    f.close()\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTepr7Bc-ANp"
   },
   "source": [
    "# takelab_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKiIYR5z-ANr"
   },
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)\n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "\n",
    "\n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt', encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wLPk-9_qLDS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 22)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ8xG1cJpWEN"
   },
   "outputs": [],
   "source": [
    "param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "gs.fit(data,label)\n",
    "m=gs.best_estimator_\n",
    "model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOLW8M0Fx2mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 1e-05, 'batch_size': 256, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_iter': 1000, 'solver': 'adam', 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46reO7QUp0iB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "test_data=[]\n",
    "for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "    #y = 0. if scores is None else scores[idx]\n",
    "    test_data+=[(calc_features(*sp))]\n",
    "    \n",
    "test_data=np.array(test_data)\n",
    "test_data=np.append(test_data,test_cnn,axis=1)\n",
    "\n",
    "test_data= scaler.transform(test_data)\n",
    "\n",
    "print(test_data.shape)\n",
    "\n",
    "f= open('2017_takelab+cnn/test/sts_test_scores.txt')\n",
    "test_label= f.readlines()\n",
    "test_label=[float(w.strip()) for w in test_label]\n",
    "test_label=np.array(test_label).reshape(len(test_label),)\n",
    "\n",
    "'''test_data=[]\n",
    "for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "    y = 0. if scores is None else scores[idx]\n",
    "    test_data+=[(calc_features(*sp))]\n",
    "    \n",
    "test_data=np.array(test_data)\n",
    "\n",
    "f= open('trial1/STS.gs.txt')\n",
    "test_label= f.readlines()\n",
    "test_label=[float(w.strip()) for w in test_label]\n",
    "test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "\n",
    "predictions= model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VQN1GHVW-AN7",
    "outputId": "e7a486b2-076b-4b52-8f6c-f71099142762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301996744803232\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_output_MSRpar.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_output_MSRpar.txt'))\n",
    "\n",
    "\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzyJnDpn-AOA"
   },
   "source": [
    "# Using JC Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0XPMGB8-AOB"
   },
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-AmeVZRFUFcM",
    "outputId": "8e947bdc-7448-4214-bff6-c85455e64e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "gs.fit(data,label)\n",
    "m=gs.best_estimator_\n",
    "model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "\n",
    "test_data=[]\n",
    "for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "    #y = 0. if scores is None else scores[idx]\n",
    "    test_data+=[(calc_features(*sp))]\n",
    "    \n",
    "test_data=np.array(test_data)\n",
    "test_data=np.append(test_data,test_cnn,axis=1)\n",
    "print(test_data.shape)\n",
    "\n",
    "test_data= scaler.transform(test_data)\n",
    "\n",
    "f= open('2017_takelab+cnn/test/sts_test_scores.txt')\n",
    "test_label= f.readlines()\n",
    "test_label=[float(w.strip()) for w in test_label]\n",
    "test_label=np.array(test_label).reshape(len(test_label),)\n",
    "\n",
    "'''test_data=[]\n",
    "for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "    y = 0. if scores is None else scores[idx]\n",
    "    test_data+=[(calc_features(*sp))]\n",
    "    \n",
    "test_data=np.array(test_data)\n",
    "\n",
    "f= open('trial1/STS.gs.txt')\n",
    "test_label= f.readlines()\n",
    "test_label=[float(w.strip()) for w in test_label]\n",
    "test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "\n",
    "predictions= model.predict(test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAAwDxK_Uh2q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dxxR7rSh-AOH",
    "outputId": "9b37d2c7-6e14-414a-e8c3-4036e052bc55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7219041432845712\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_jc.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('takelab_jc.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ev2CYJUR-AOM"
   },
   "source": [
    "# Using Lin Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ls5xsIId-AON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt',encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "    gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,test_cnn,axis=1)\n",
    "    print(test_data.shape)\n",
    "\n",
    "    test_data= scaler.transform(test_data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/test/sts_test_scores.txt',encoding='utf-8')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TmJLHMg-AOT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7246928812576686\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqznzT8l-AOY"
   },
   "source": [
    "# word2vec with weighted_LSA_vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI-2hr_W-AOZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt', encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "    gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,test_cnn,axis=1)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    test_data= scaler.transform(test_data)\n",
    "\n",
    "    f= open('2017_takelab+cnn/test/sts_test_scores.txt',encoding='utf-8')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssbdsRbR-AOe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071254897988921\n"
     ]
    }
   ],
   "source": [
    "with open('wv_wlsa.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('wv_wlsa.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icvis7Dd-AOk"
   },
   "source": [
    "# JC AND LIN Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ocqjz_Eh-AOl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 21)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb),\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt', encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "    gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,test_cnn,axis=1)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    test_data= scaler.transform(test_data)\n",
    "\n",
    "    f= open('2017_takelab+cnn/test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIrGBG4Y-AOs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7119137113107068\n"
     ]
    }
   ],
   "source": [
    "with open('jc_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('jc_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bJvGKmI-AOx"
   },
   "source": [
    "# word2vec with weighted_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4Ccvhp6-AOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "pankda\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "someoen\n",
      "sitring\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "kangroo\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "tortila\n",
      "someoen\n",
      "smeone\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "stenograph\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "paino\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "abulance\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "slced\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "oots\n",
      "and\n",
      "drivong\n",
      "and\n",
      "ridint\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "seadoo\n",
      "and\n",
      "smeone\n",
      "and\n",
      "descale\n",
      "haki\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "sitiing\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "someoen\n",
      "swinge\n",
      "sledghammer\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "acouch\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "esso\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "areomexico\n",
      "aeromexico\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "wwii\n",
      "and\n",
      "and\n",
      "and\n",
      "outlooking\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kitchendiner\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dressedin\n",
      "and\n",
      "swinge\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "equpitment\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "scalling\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "nussle\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "headand\n",
      "and\n",
      "and\n",
      "and\n",
      "tourquoise\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "?\n",
      "''\n",
      "?\n",
      ":\n",
      "'\n",
      "?\n",
      ":\n",
      "'\n",
      "?\n",
      "?\n",
      "?\n",
      "!\n",
      "!\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "?\n",
      "**smooches\n",
      "ran**\n",
      "?\n",
      "''\n",
      "and\n",
      "zimmerman\n",
      "zimmerman\n",
      "nswp\n",
      "odin\n",
      "nswp\n",
      "''\n",
      "dickshot\n",
      "and\n",
      "neener\n",
      "''\n",
      "neener\n",
      "''\n",
      "wiggies\n",
      "!\n",
      "''\n",
      "treesome\n",
      "?\n",
      "''\n",
      "zimmerman\n",
      "?\n",
      "''\n",
      "tafb\n",
      "and\n",
      "dvorak\n",
      "tafb\n",
      "and\n",
      "''\n",
      "''\n",
      "?\n",
      "sexuaity\n",
      "?\n",
      "pfa\n",
      "'\n",
      "fundimental\n",
      "?\n",
      "?\n",
      "'punished\n",
      "'\n",
      "?\n",
      "?\n",
      "zimmerman\n",
      "!\n",
      "?\n",
      "!\n",
      "?\n",
      "''\n",
      "!\n",
      "!\n",
      "and\n",
      "?\n",
      "?\n",
      "berkman\n",
      "?\n",
      "berkman\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "springhallconvert\n",
      "?\n",
      "giva\n",
      "and\n",
      "slutstamp\n",
      "!\n",
      "and\n",
      "?\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "?\n",
      "?\n",
      "neander\n",
      "neander\n",
      "''\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      "''\n",
      "!\n",
      "and\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      "''\n",
      "''\n",
      "heine\n",
      "and\n",
      "coors\n",
      "xanis\n",
      "xanis\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "neener\n",
      "''\n",
      "neener\n",
      "''\n",
      "dutchy\n",
      "?\n",
      "nickfun\n",
      "dutchy\n",
      "?\n",
      "and\n",
      "?\n",
      ":\n",
      "and\n",
      "?\n",
      "!\n",
      "and\n",
      "!\n",
      "!\n",
      "''\n",
      "?\n",
      "'led\n",
      "'\n",
      "?\n",
      "solinvictus\n",
      "messahe\n",
      "solinvictus\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "?\n",
      ":\n",
      ":\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "anabelle\n",
      "?\n",
      "'say\n",
      "'\n",
      "?\n",
      "'\n",
      "terral\n",
      ":\n",
      "and\n",
      "and\n",
      "!\n",
      "''\n",
      "''\n",
      "'and\n",
      "'\n",
      "and\n",
      "''\n",
      "''\n",
      "!\n",
      "terral\n",
      "?\n",
      "terral\n",
      ":\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "milligraming\n",
      ":\n",
      "?\n",
      "?\n",
      "!\n",
      "!\n",
      "and\n",
      "?\n",
      "'cares\n",
      "'\n",
      "?\n",
      "sarg\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "landon\n",
      ":\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "lustig\n",
      "andrei\n",
      "and\n",
      "?\n",
      "trayvon\n",
      "and\n",
      "?\n",
      "and\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "!\n",
      "factchecking\n",
      ":\n",
      "thatcherite\n",
      "thatcherites\n",
      "?\n",
      "?\n",
      "nutrax\n",
      "?\n",
      "nutrax\n",
      "and\n",
      "moloch\n",
      "moloch\n",
      "lustig\n",
      "andrei\n",
      "lyndon\n",
      "and\n",
      "?\n",
      "edgarblythe\n",
      "lyndon\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "drpizza\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      ":\n",
      ":\n",
      "bienging\n",
      "unaccptable\n",
      "melman\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      "hilited\n",
      "cryptolutheran\n",
      "and\n",
      "dreamtech\n",
      "?\n",
      "?\n",
      "mathis\n",
      "isnt\n",
      "?\n",
      ":\n",
      "?\n",
      ":\n",
      ":\n",
      "?\n",
      "?\n",
      ":\n",
      "?\n",
      "?\n",
      "''\n",
      "?\n",
      "''\n",
      ":\n",
      "croce\n",
      "zimmerman\n",
      "?\n",
      "''\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      "''\n",
      "and\n",
      "fantasise\n",
      "?\n",
      "?\n",
      "?\n",
      "''\n",
      "hilited\n",
      "unconstuitutional\n",
      "?\n",
      "''\n",
      "and\n",
      "!\n",
      "and\n",
      ":\n",
      "?\n",
      "?\n",
      "and\n",
      "doesnt\n",
      "and\n",
      "and\n",
      "and\n",
      "?\n",
      "''\n",
      "and\n",
      "'whats\n",
      "?\n",
      "'\n",
      "lustig\n",
      "andrei\n",
      "lustig\n",
      "andrei\n",
      "zimmerman\n",
      "?\n",
      "''\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      "?\n",
      "buwhahahaha\n",
      "and\n",
      "and\n",
      "and\n",
      "apisa\n",
      "cyracuz\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      ":\n",
      "and\n",
      "doesnt\n",
      "and\n",
      ":\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "damascus\n",
      "?\n",
      "damascus\n",
      "!\n",
      "jond\n",
      ":\n",
      ":\n",
      "!\n",
      "and\n",
      "?\n",
      "''\n",
      "didnt\n",
      "''\n",
      "''\n",
      "?\n",
      "?\n",
      "unconstuitutional\n",
      "cyclopes\n",
      "''\n",
      "''\n",
      "?\n",
      "''\n",
      "?\n",
      "landon\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      "?\n",
      ":\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      ":\n",
      "omsigdavid\n",
      "gingrich\n",
      "sozobe\n",
      "gingrich\n",
      "!\n",
      "?\n",
      "!\n",
      "?\n",
      "?\n",
      "?\n",
      "eots\n",
      ":\n",
      "?\n",
      "''\n",
      "''\n",
      "?\n",
      "?\n",
      "spikdboy\n",
      "''\n",
      "?\n",
      "?\n",
      "diachronico\n",
      "?\n",
      "?\n",
      "and\n",
      "foxe\n",
      "foxe\n",
      "apisa\n",
      "and\n",
      "and\n",
      "landon\n",
      "and\n",
      "?\n",
      "buwhahahaha\n",
      "!\n",
      "!\n",
      "'cares\n",
      "'\n",
      "?\n",
      "?\n",
      "''\n",
      "?\n",
      "?\n",
      ":\n",
      "!\n",
      "!\n",
      "voracek\n",
      "and\n",
      "?\n",
      "''\n",
      "and\n",
      "''\n",
      "!\n",
      "!\n",
      "trayvon\n",
      "trayvon\n",
      "and\n",
      "?\n",
      "?\n",
      "!\n",
      "!\n",
      "and\n",
      "nugent\n",
      "''\n",
      "and\n",
      "nazarite\n",
      "nazarite\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "?\n",
      "!\n",
      "!\n",
      "?\n",
      ":\n",
      "vitchilo\n",
      "!\n",
      "?\n",
      "?\n",
      "lonny\n",
      "?\n",
      "and\n",
      "zimmerman\n",
      "''\n",
      "''\n",
      "''\n",
      ":\n",
      ":\n",
      ":\n",
      "qur'an\n",
      "bhagavad\n",
      "ghita\n",
      "'right\n",
      "'\n",
      "?\n",
      "?\n",
      "''\n",
      "''\n",
      ":\n",
      ":\n",
      "!\n",
      "and\n",
      "?\n",
      "?\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      ";\n",
      ";\n",
      "''\n",
      "and\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "unconsititutional\n",
      "!\n",
      "addie\n",
      "lashl\n",
      ":\n",
      "lashl\n",
      ":\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "''\n",
      "?\n",
      "''\n",
      "?\n",
      "and\n",
      "?\n",
      "?\n",
      ":\n",
      "donaldson\n",
      "''\n",
      "''\n",
      "aung\n",
      "suu\n",
      "yangon\n",
      "aung\n",
      "suu\n",
      "dopp\n",
      "heine\n",
      "and\n",
      "ftse\n",
      "ftse\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dunlap\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "cheske\n",
      "cheske\n",
      "gruner\n",
      "jahr\n",
      "g+j\n",
      "and\n",
      "connectix\n",
      "connectix\n",
      "'\n",
      "tompkinsville\n",
      "and\n",
      "ursel\n",
      "reisen\n",
      "ursel\n",
      "reisen\n",
      "basra\n",
      "organisation\n",
      "basra\n",
      "and\n",
      "khatami\n",
      "and\n",
      "''\n",
      "''\n",
      "millville\n",
      "millville\n",
      "kraynak\n",
      "and\n",
      "kraynak\n",
      "and\n",
      "and\n",
      "centimetre\n",
      "and\n",
      "centimetre\n",
      "and\n",
      "and\n",
      "lewinsky\n",
      "''\n",
      "lewinsky\n",
      "''\n",
      "and\n",
      "morgenthau\n",
      "and\n",
      "and\n",
      "allahu\n",
      "''\n",
      "and\n",
      "allahu\n",
      "''\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      "''\n",
      "ousmane\n",
      "coulibaly\n",
      "and\n",
      "''\n",
      "ousmane\n",
      "coulibaly\n",
      "and\n",
      "strayhorn\n",
      "carole\n",
      "keeton\n",
      "strayhorn\n",
      "kkcs\n",
      "and\n",
      "and\n",
      "and\n",
      "arafat\n",
      "and\n",
      "arafat\n",
      "and\n",
      "kazan\n",
      ":\n",
      "''\n",
      "''\n",
      "kazan\n",
      "and\n",
      ":\n",
      "gilroy\n",
      "and\n",
      "gehring\n",
      "gehring\n",
      "scribner\n",
      "scribner\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "larsen\n",
      "larsen\n",
      "glaxosmithkline\n",
      "glaxosmithkline\n",
      "and\n",
      "granada\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "brandeis\n",
      "''\n",
      "brandeis\n",
      "'\n",
      "nterprise\n",
      "nterprise\n",
      "proliant\n",
      "''\n",
      "''\n",
      "''\n",
      "“\n",
      "”\n",
      "lyman\n",
      "and\n",
      "varian\n",
      "lyman\n",
      "and\n",
      "varian\n",
      "''\n",
      "''\n",
      "and\n",
      ";\n",
      "and\n",
      "and\n",
      "centre\n",
      "mitzna\n",
      "and\n",
      "ophir\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      ":\n",
      "lendingtree\n",
      "lendingtree\n",
      "''\n",
      "rohrbough\n",
      "rohrbough\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "dotson\n",
      "and\n",
      "dennehy\n",
      "dotson\n",
      "dennehy\n",
      "peoplesoft\n",
      "peoplesoft\n",
      "mcauliffe\n",
      "mcauliffe\n",
      "''\n",
      "giovanni\n",
      "bisignani\n",
      "''\n",
      "giovanni\n",
      "bisignani\n",
      "and\n",
      "and\n",
      "ridder\n",
      "ridder\n",
      "and\n",
      "and\n",
      "{\n",
      "and\n",
      "and\n",
      "furyk\n",
      "lemelson\n",
      "and\n",
      "syndia\n",
      "''\n",
      "syndia\n",
      "lemelson\n",
      "and\n",
      "and\n",
      "and\n",
      "morrill\n",
      "and\n",
      "bondeson\n",
      "morrill\n",
      "and\n",
      "bondeson\n",
      "grassley\n",
      "and\n",
      "baucus\n",
      "grassley\n",
      "and\n",
      "baucus\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "bayoumi\n",
      "bayoumi\n",
      "''\n",
      "santangelo\n",
      "''\n",
      "santangelo\n",
      "and\n",
      "and\n",
      "brabazon\n",
      "and\n",
      "and\n",
      "stubbs\n",
      "''\n",
      "lupas\n",
      "stubbs\n",
      "''\n",
      "akron\n",
      "akron\n",
      "botswana\n",
      "malawi\n",
      "botswana\n",
      "malawi\n",
      "''\n",
      "and\n",
      "''\n",
      "moqtada\n",
      "''\n",
      "koufa\n",
      "najaf\n",
      "and\n",
      "counterpane\n",
      "and\n",
      "idefense\n",
      "ernst\n",
      "and\n",
      "ernst\n",
      "and\n",
      "''\n",
      "wolfowitz\n",
      "wolfowitz\n",
      "''\n",
      "''\n",
      "guidant\n",
      "and\n",
      "guidant\n",
      "''\n",
      "and\n",
      "and\n",
      "obus\n",
      "tshabalala\n",
      "msimang\n",
      "tshabalala\n",
      "msimang\n",
      "and\n",
      "coyne\n",
      "coyne\n",
      "balinese\n",
      "balinese\n",
      "and\n",
      "nlc\n",
      "and\n",
      "mobilise\n",
      "and\n",
      "eae\n",
      "and\n",
      "sapp\n",
      "chequer\n",
      "dunedin\n",
      "and\n",
      "and\n",
      "isabel\n",
      "isabel\n",
      "and\n",
      "and\n",
      "adora\n",
      "nweze\n",
      "adora\n",
      "nweze\n",
      "and\n",
      "wathen\n",
      "and\n",
      "''\n",
      "wathen\n",
      "and\n",
      "and\n",
      "oge\n",
      "strunk\n",
      "strunk\n",
      "''\n",
      "and\n",
      "and\n",
      "torsella\n",
      "torsella\n",
      "and\n",
      "and\n",
      "dillard\n",
      "and\n",
      "''\n",
      "''\n",
      "ruffner\n",
      "ruffner\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "geller\n",
      "cibc\n",
      "velcade\n",
      "''\n",
      "geller\n",
      "cibc\n",
      "and\n",
      "liberia\n",
      "and\n",
      "loyald\n",
      "liberia\n",
      "selenski\n",
      "olivet\n",
      "kerkowski\n",
      "and\n",
      "fassett\n",
      "selenski\n",
      "olivet\n",
      "and\n",
      "and\n",
      "kline\n",
      "callebaut\n",
      "callebaut\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "dolores\n",
      "mahoy\n",
      "dolores\n",
      "mahoy\n",
      "'\n",
      "bair\n",
      ":\n",
      "lendingtree\n",
      "and\n",
      "lendingtree\n",
      "and\n",
      "and\n",
      "and\n",
      "costello\n",
      "gyorgy\n",
      "heizler\n",
      "gyorgy\n",
      "heizler\n",
      "joaquin\n",
      "and\n",
      "and\n",
      "yeates\n",
      "fleischer\n",
      ":\n",
      "qa'ida\n",
      "''\n",
      "qaida\n",
      "and\n",
      "and\n",
      "rehnquist\n",
      "and\n",
      "clarence\n",
      "rehnquist\n",
      "and\n",
      "antonin\n",
      "scalia\n",
      "clarence\n",
      "and\n",
      "''\n",
      "fleischer\n",
      "fleischer\n",
      "and\n",
      "newbury\n",
      "''\n",
      "newbury\n",
      "and\n",
      "porchia\n",
      "''\n",
      "porchia\n",
      "''\n",
      "''\n",
      "unifi\n",
      "o'neill\n",
      "unifi\n",
      "o'neill\n",
      ":\n",
      "selwyn\n",
      "and\n",
      "formby\n",
      "and\n",
      "westpac\n",
      ":\n",
      "kazan\n",
      "and\n",
      "''\n",
      "kazan\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "gillette\n",
      "gillette\n",
      "''\n",
      "briton\n",
      "and\n",
      "sampson\n",
      "kirkintilloch\n",
      "and\n",
      "sampson\n",
      "hadley\n",
      "and\n",
      ":\n",
      "hadley\n",
      ":\n",
      "catalina\n",
      "catalina\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "natsemi\n",
      "and\n",
      "matamoros\n",
      "brownsville\n",
      "and\n",
      "matamoros\n",
      "brownsville\n",
      "nasd\n",
      "and\n",
      "nasd\n",
      "''\n",
      "baldacci\n",
      "''\n",
      "baldacci\n",
      "and\n",
      "and\n",
      "bustamante\n",
      "dittemore\n",
      "dittemore\n",
      "selenski\n",
      "selenski\n",
      "maddox\n",
      "carnes\n",
      "maddox\n",
      "''\n",
      "rolfe\n",
      "and\n",
      "pinal\n",
      "vanderpool\n",
      "and\n",
      "pinal\n",
      "vanderpool\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "vanderpool\n",
      "vanderpool\n",
      "annika\n",
      "sorenstam\n",
      "annika\n",
      "sorenstam\n",
      "and\n",
      "and\n",
      "and\n",
      "klebold\n",
      "and\n",
      "klebold\n",
      "''\n",
      "muqtada\n",
      "and\n",
      "''\n",
      "moqtada\n",
      "chechen\n",
      "chechen\n",
      "organisation\n",
      "and\n",
      "and\n",
      ":\n",
      "hurlbert\n",
      "and\n",
      "fishman\n",
      "fishman\n",
      "and\n",
      "weida\n",
      "meester\n",
      "and\n",
      "meester\n",
      "and\n",
      "rosenthal\n",
      ":\n",
      "and\n",
      "rosenthal\n",
      ":\n",
      "pataki\n",
      "and\n",
      "basra\n",
      "and\n",
      "basra\n",
      "muireagáin\n",
      "''\n",
      "''\n",
      "and\n",
      "britz\n",
      "and\n",
      "kinney\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "niels\n",
      "nielsen\n",
      "allyson\n",
      "and\n",
      "and\n",
      "and\n",
      "vietnow\n",
      "invercargill\n",
      "and\n",
      "d'archiac\n",
      "and\n",
      "d'archiac\n",
      "mckenzie\n",
      "geoghan\n",
      "baranowski\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "shelia\n",
      "chaney\n",
      "worshipper\n",
      "shelia\n",
      "chaney\n",
      "kirkwood\n",
      "miodrag\n",
      "zivkovic\n",
      "montenegro\n",
      "dragan\n",
      "hajdukovic\n",
      "miodrag\n",
      "zivkovic\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "oestrogen\n",
      "goodrich\n",
      "and\n",
      "staffenberg\n",
      "aguirres\n",
      "goodrich\n",
      "and\n",
      "staffenberg\n",
      "and\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "'\n",
      "and\n",
      "zuccarini\n",
      "ashcroft\n",
      "and\n",
      "and\n",
      "ashcroft\n",
      "tucci\n",
      "and\n",
      "tucci\n",
      "and\n",
      "masaka\n",
      "armoured\n",
      "aronda\n",
      "nyakairima\n",
      "yoweri\n",
      "aronda\n",
      "nyakairima\n",
      "and\n",
      "mailblocks\n",
      "and\n",
      "and\n",
      "and\n",
      "vladimir\n",
      "and\n",
      "''\n",
      "vladimir\n",
      "and\n",
      "''\n",
      "mcgrath\n",
      "mcgrath\n",
      "mckinlay\n",
      "''\n",
      "mckinlay\n",
      ":\n",
      "yarralumla\n",
      "jeffery\n",
      "yarralumla\n",
      "piniella\n",
      "and\n",
      "aubrey\n",
      "piniella\n",
      "and\n",
      "and\n",
      "tnk\n",
      "and\n",
      "libeskind\n",
      "libeskind\n",
      "and\n",
      "and\n",
      "and\n",
      "colour\n",
      "and\n",
      "colour\n",
      "organise\n",
      "and\n",
      "marsden\n",
      "and\n",
      "devries\n",
      "suse\n",
      "formalise\n",
      "nuremberg\n",
      "suse\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "''\n",
      "shinseki\n",
      "and\n",
      "bosnia\n",
      "shinseki\n",
      "and\n",
      "bosnia\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "garber\n",
      "rowland\n",
      "and\n",
      "garber\n",
      "rowland\n",
      "amrozi\n",
      "''\n",
      "fraley\n",
      "''\n",
      "fraley\n",
      "monsanto\n",
      "elbaradei\n",
      "elbaradei\n",
      "guangdong\n",
      "guandong\n",
      "druce\n",
      "druce\n",
      "markakis\n",
      "markakis\n",
      "and\n",
      "and\n",
      "''\n",
      "alhart\n",
      "and\n",
      "''\n",
      "hollingworth\n",
      "hollingworth\n",
      "''\n",
      "''\n",
      "earnhardt\n",
      "ferrero\n",
      "ferrero\n",
      "and\n",
      "and\n",
      "calvert\n",
      "calvert\n",
      "peoplesoft\n",
      "''\n",
      "ellison\n",
      "''\n",
      "''\n",
      "''\n",
      "andreassi\n",
      "andreassi\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "luzerne\n",
      "lupas\n",
      "''\n",
      "luzerne\n",
      "lupas\n",
      "''\n",
      "'\n",
      "and\n",
      "''\n",
      "''\n",
      "nicolo\n",
      "connelly\n",
      "and\n",
      "nicolo\n",
      "connelly\n",
      ":\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "'\n",
      "qaida\n",
      "dvorak\n",
      "jdvorak\n",
      "bcooper\n",
      "hardin\n",
      "and\n",
      "''\n",
      "alesha\n",
      "badgley\n",
      "and\n",
      "''\n",
      "alesha\n",
      "badgley\n",
      "''\n",
      "fiorina\n",
      "''\n",
      "thames\n",
      "and\n",
      "thames\n",
      "echostar\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "weinshall\n",
      "''\n",
      "cooley\n",
      "malvo\n",
      "malvo\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "“\n",
      "and\n",
      "barbey\n",
      "healey\n",
      "and\n",
      "healey\n",
      "and\n",
      "and\n",
      "frankfort\n",
      "and\n",
      "frankfort\n",
      "aritonang\n",
      "and\n",
      "magelang\n",
      "aritonang\n",
      "and\n",
      "magelang\n",
      "barbour\n",
      "avants\n",
      "barbour\n",
      "avants\n",
      "and\n",
      "and\n",
      "and\n",
      "sattler\n",
      "and\n",
      "sattler\n",
      "hewlett\n",
      "packard\n",
      "hewlett\n",
      "packard\n",
      "ratliff\n",
      "thode\n",
      "''\n",
      "thode\n",
      ";\n",
      "schiavo\n",
      "''\n",
      "''\n",
      "francois\n",
      "mattei\n",
      "schiavo\n",
      "schiavo\n",
      "schindler\n",
      "palumbo\n",
      "procter\n",
      "folgers\n",
      "and\n",
      "pantene\n",
      "eastman\n",
      "palumbo\n",
      "procter\n",
      "mcdevitt\n",
      "solomons\n",
      "mcdevitt\n",
      "morrell\n",
      "rockingham\n",
      "''\n",
      "tisha\n",
      "kresler\n",
      ";\n",
      "''\n",
      ":\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "virada\n",
      "nirapathpongporn\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "strier\n",
      "celnik\n",
      "strier\n",
      "celnik\n",
      "lubanski\n",
      "kenrick\n",
      "lubanski\n",
      "and\n",
      ":\n",
      "and\n",
      "''\n",
      "gehman\n",
      "nima\n",
      "gehman\n",
      "and\n",
      "devries\n",
      "devries\n",
      "fabian\n",
      "and\n",
      "agassi\n",
      "rusedski\n",
      "agassi\n",
      "levine\n",
      "levine\n",
      "bankhead\n",
      "''\n",
      "gbi\n",
      "bankhead\n",
      "''\n",
      "''\n",
      "''\n",
      "sundwell\n",
      "atstake\n",
      "''\n",
      "sundwall\n",
      "egyptologist\n",
      "nefertiti\n",
      "egyptologist\n",
      "nefertiti\n",
      "treasury\u0012s\n",
      "interactivecorp\n",
      "''\n",
      "''\n",
      "defence\n",
      "and\n",
      "and\n",
      "lendingtree\n",
      "and\n",
      "kuwaiti\n",
      "and\n",
      "jordanian\n",
      "kuwaiti\n",
      "and\n",
      "jordanian\n",
      "and\n",
      "and\n",
      "sorkin\n",
      "and\n",
      "sorkin\n",
      "and\n",
      "woodside\n",
      "''\n",
      "''\n",
      "sriyanto\n",
      "'\n",
      "sriyanto\n",
      "''\n",
      "rehnquist\n",
      "and\n",
      "o'connor\n",
      "scalia\n",
      "rehnquist\n",
      "and\n",
      "o'connor\n",
      "antonin\n",
      "scalia\n",
      "''\n",
      "weisselberg\n",
      "''\n",
      "weisselberg\n",
      "and\n",
      "and\n",
      "tranz\n",
      "messagelabs\n",
      "messagelabs\n",
      "''\n",
      "and\n",
      ":\n",
      "delainey\n",
      "delainey\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "dallager\n",
      "dallager\n",
      "''\n",
      "karanja\n",
      "karanja\n",
      "and\n",
      "and\n",
      "swartz\n",
      "swartz\n",
      "and\n",
      "españa\n",
      "and\n",
      "espaa\n",
      "and\n",
      "plofsky\n",
      "plofsky\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "episcopalian\n",
      "and\n",
      "episcopalian\n",
      "squyres\n",
      "athena\n",
      "squyres\n",
      "'\n",
      "and\n",
      "and\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "shevaun\n",
      "pennington\n",
      "studabaker\n",
      "and\n",
      "and\n",
      "''\n",
      "'\n",
      "vba\n",
      "studdard\n",
      "aiken\n",
      "studdard\n",
      "aiken\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "imclone\n",
      "erbitux\n",
      "and\n",
      "imclone\n",
      "sensenbrenner\n",
      "sensenbrenner\n",
      "and\n",
      "and\n",
      "and\n",
      "strayhorn\n",
      "strayhorn\n",
      "sahel\n",
      "ntsb\n",
      "nhtsa\n",
      "and\n",
      "jazeera\n",
      "jazeera\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "cavender\n",
      ":\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "cripps\n",
      "prawak\n",
      "'\n",
      "and\n",
      "morgenthau\n",
      "morgenthau\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ";\n",
      "chante\n",
      "chante\n",
      "jawaon\n",
      "and\n",
      "and\n",
      "phipps\n",
      "and\n",
      "mclamb\n",
      "monteiro\n",
      "monteiro\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "obetz\n",
      "obetz\n",
      "and\n",
      "centre\n",
      "taegu\n",
      "centre\n",
      "and\n",
      "kadyrov\n",
      "itar\n",
      "and\n",
      "and\n",
      "verisign\n",
      "verisign\n",
      "''\n",
      "druce\n",
      "and\n",
      "druce\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "bakr\n",
      "azdi\n",
      ";\n",
      "freitas\n",
      "'\n",
      "mertel\n",
      "mertel\n",
      "kleiner\n",
      "caufield\n",
      "byers\n",
      "and\n",
      "and\n",
      "kleiner\n",
      "mindanao\n",
      "neighbour\n",
      "mindanao\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "watertown\n",
      "saugus\n",
      "and\n",
      "framingham\n",
      "and\n",
      "''\n",
      "davidowitz\n",
      "''\n",
      "davidowitz\n",
      "and\n",
      "and\n",
      "reveller\n",
      "and\n",
      "and\n",
      "kurd\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "armoured\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "pelloux\n",
      "''\n",
      "covello\n",
      "and\n",
      "''\n",
      "covello\n",
      "fujitsu\n",
      "spansion\n",
      "spansion\n",
      "and\n",
      "fujitsu\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "pataki\n",
      "and\n",
      "lipa\n",
      "kessel\n",
      "lipa\n",
      "kessel\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ncri\n",
      "safavi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "ncri\n",
      "safavi\n",
      "prodi\n",
      "halabi\n",
      "halabi\n",
      "waksal\n",
      ":\n",
      "waksal\n",
      "and\n",
      "and\n",
      "jayroe\n",
      "organisation\n",
      ":\n",
      "and\n",
      "organisation\n",
      "and\n",
      "'\n",
      "''\n",
      "baystar\n",
      "baystar\n",
      ";\n",
      "''\n",
      "''\n",
      "and\n",
      "lleyton\n",
      "lleyton\n",
      ":\n",
      "''\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "edna\n",
      "marissa\n",
      "jaret\n",
      "winokur\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "rhode\n",
      "adrs\n",
      "and\n",
      "bremer\n",
      "and\n",
      "bremer\n",
      "and\n",
      "prodi\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "corixa\n",
      "corixa\n",
      "and\n",
      "shukrijumah\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "''\n",
      "programme\n",
      "''\n",
      ":\n",
      "programme\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "novellus\n",
      "and\n",
      "novellus\n",
      "''\n",
      "rwanda\n",
      "and\n",
      "rwanda\n",
      "and\n",
      "and\n",
      "melanesian\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ";\n",
      "''\n",
      "''\n",
      "javaone\n",
      "'\n",
      "javaone\n",
      "and\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "and\n",
      "abdullah\n",
      "kawasme\n",
      "hebron\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ellison\n",
      "peoplesoft\n",
      "ellison\n",
      "peoplesoft\n",
      "almeida\n",
      "''\n",
      "''\n",
      "wyden\n",
      "and\n",
      "dorgan\n",
      "''\n",
      "milunovich\n",
      "milunovich\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "veneman\n",
      "and\n",
      "liberian\n",
      "liberian\n",
      "audubon\n",
      "audubon\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "shumaker\n",
      "''\n",
      "shumaker\n",
      ";\n",
      "!\n",
      "''\n",
      ":\n",
      "!\n",
      "''\n",
      "and\n",
      ":\n",
      "matsushita\n",
      "hitachi\n",
      "and\n",
      "celf\n",
      "hitachi\n",
      "matsushita\n",
      "and\n",
      "?\n",
      "''\n",
      "ropeik\n",
      "''\n",
      "ropeik\n",
      "bergin\n",
      "baywatch\n",
      "''\n",
      "bergin\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "gemstar\n",
      "gemstar\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "hewlett\n",
      "packard\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertson\n",
      "favourable\n",
      "and\n",
      "mackey\n",
      "and\n",
      "''\n",
      "mackey\n",
      "and\n",
      "trenton\n",
      "somerville\n",
      "trenton\n",
      "esco\n",
      "esco\n",
      "and\n",
      "and\n",
      "'\n",
      "and\n",
      "griffith\n",
      "manteo\n",
      "griffith\n",
      "and\n",
      "manteo\n",
      "''\n",
      "''\n",
      "bremer\n",
      "mccartney\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "barbour\n",
      "barbour\n",
      "and\n",
      "epicentre\n",
      "emile\n",
      "laroza\n",
      "epicentre\n",
      "rusch\n",
      "lastings\n",
      "milledge\n",
      "lakewood\n",
      "lastings\n",
      "milledge\n",
      ";\n",
      "albertsons\n",
      "and\n",
      "kroger\n",
      "ralphs\n",
      "kroger\n",
      "ralphs\n",
      "and\n",
      "albertsons\n",
      "and\n",
      "and\n",
      "yorktown\n",
      "and\n",
      "ishtar\n",
      "sheraton\n",
      "and\n",
      "ishtar\n",
      "sheraton\n",
      "zhaoxing\n",
      "zhaoxing\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "mcgreevey\n",
      "''\n",
      "''\n",
      "'\n",
      "''\n",
      "and\n",
      "'\n",
      "''\n",
      "gann\n",
      "gann\n",
      "russin\n",
      ";\n",
      "and\n",
      "russin\n",
      "and\n",
      "armidale\n",
      "''\n",
      "armidale\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "reyna\n",
      "and\n",
      "altria\n",
      "altria\n",
      "''\n",
      "tambe\n",
      "''\n",
      "tambe\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "cwa\n",
      "and\n",
      "and\n",
      "annika\n",
      "sorenstam\n",
      "annika\n",
      "sorenstam\n",
      "and\n",
      "'\n",
      "concordes\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "aftra\n",
      "aftra\n",
      "malawi\n",
      "qaida\n",
      "malawi\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "alibek\n",
      ":\n",
      "''\n",
      "alibek\n",
      "'\n",
      "and\n",
      "'\n",
      "''\n",
      "and\n",
      "bogden\n",
      "and\n",
      "bogden\n",
      "and\n",
      "and\n",
      "somers\n",
      ";\n",
      "and\n",
      "somers\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "colgate\n",
      "colgate\n",
      "and\n",
      "and\n",
      "mahmoud\n",
      "jordanian\n",
      "aqaba\n",
      "and\n",
      "mahmoud\n",
      "aqaba\n",
      "''\n",
      "and\n",
      "''\n",
      "theoctober\n",
      "''\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "rambus\n",
      ":\n",
      "rmbs\n",
      "''\n",
      "mabry\n",
      "''\n",
      "mabry\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      ":\n",
      "littleton\n",
      "echostar\n",
      "echostar\n",
      ":\n",
      "and\n",
      "mclean\n",
      "guillermo\n",
      "and\n",
      "netherlander\n",
      "verkerk\n",
      "guillermo\n",
      "and\n",
      "verkerk\n",
      "?\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "cancuen\n",
      "vanderbilt\n",
      "and\n",
      "cancun\n",
      "vanderbilt\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "meagre\n",
      "imclone\n",
      "erbitux\n",
      "imclone\n",
      "levine\n",
      "levine\n",
      "airtran\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "pingeon\n",
      "'\n",
      "pingeon\n",
      "assan\n",
      "and\n",
      "pnc\n",
      "''\n",
      "and\n",
      "rohr\n",
      "rohr\n",
      "and\n",
      "pnc\n",
      "ccag\n",
      "rowland\n",
      "and\n",
      "congolese\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "nimitz\n",
      "baldacci\n",
      "''\n",
      "baldacci\n",
      "and\n",
      "healthpark\n",
      "and\n",
      "and\n",
      "''\n",
      "''\n",
      "davey\n",
      "and\n",
      "davey\n",
      "and\n",
      "''\n",
      "''\n",
      "schindler\n",
      "schiavo\n",
      "johnnie\n",
      "''\n",
      "johnnie\n",
      "''\n",
      "and\n",
      "and\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "mikhail\n",
      "khodorkovsky\n",
      "yukos\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dewhurst\n",
      "''\n",
      "''\n",
      "and\n",
      "antetonitrus\n",
      "antetonitrus\n",
      "and\n",
      "olvera\n",
      "olvera\n",
      "mohsen\n",
      "zubaidi\n",
      "mohsen\n",
      "zubaidi\n",
      "and\n",
      "''\n",
      "buell\n",
      "and\n",
      "''\n",
      "buell\n",
      "inuit\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "jemaah\n",
      "islamiyah\n",
      "jemaah\n",
      "islamiyah\n",
      "''\n",
      "nayef\n",
      "nayef\n",
      "millette\n",
      "doud\n",
      "doud\n",
      "and\n",
      "and\n",
      "pribbenow\n",
      "'\n",
      "pribbenow\n",
      "and\n",
      "clemons\n",
      "'\n",
      "clemons\n",
      "''\n",
      "o'malley\n",
      "o'malley\n",
      "and\n",
      "silvester\n",
      "silvester\n",
      "and\n",
      "capps\n",
      "and\n",
      "''\n",
      "capps\n",
      "and\n",
      "''\n",
      "hovan\n",
      "speranza\n",
      "''\n",
      "hovan\n",
      "''\n",
      "speranza\n",
      "''\n",
      "daschle\n",
      "''\n",
      "daschle\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "ntt\n",
      "verio\n",
      "and\n",
      "infospace\n",
      "teya\n",
      "teya\n",
      "jemaah\n",
      "islamiyah\n",
      "qa'eda\n",
      "samudra\n",
      "qaida\n",
      "jemaah\n",
      "islamiyah\n",
      "ortega\n",
      "''\n",
      "ortega\n",
      "''\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ernst\n",
      "ernst\n",
      "kerrigan\n",
      "kosovan\n",
      "kosovan\n",
      "grassley\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ";\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "samudra\n",
      "and\n",
      "and\n",
      "and\n",
      "halliburton\n",
      "kbr\n",
      "and\n",
      "''\n",
      "kbr\n",
      "and\n",
      "''\n",
      "halliburton\n",
      "elecia\n",
      "elecia\n",
      "and\n",
      "dbtel\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "''\n",
      "''\n",
      "barrenas\n",
      "colpin\n",
      "centre\n",
      "olaf\n",
      "centre\n",
      "and\n",
      "and\n",
      "ballmer\n",
      "ballmer\n",
      "talabani\n",
      "and\n",
      "''\n",
      "talabani\n",
      "and\n",
      "''\n",
      "organise\n",
      "rosenblatt\n",
      "and\n",
      "and\n",
      "and\n",
      "rucker\n",
      "and\n",
      "''\n",
      "wolfcale\n",
      "wolfcale\n",
      "and\n",
      "''\n",
      "'\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "''\n",
      "and\n",
      "''\n",
      "comdex\n",
      "comdex\n",
      "and\n",
      "tatar\n",
      "tehuacan\n",
      "teotihuacan\n",
      "and\n",
      "''\n",
      "marce\n",
      "''\n",
      "mirant\n",
      "marce\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "allaire\n",
      "and\n",
      "thoman\n",
      "romeril\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "chera\n",
      "larkins\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "donaldson\n",
      "and\n",
      "and\n",
      "darvish\n",
      "bijan\n",
      "darvish\n",
      "and\n",
      "and\n",
      "grasso\n",
      "grasso\n",
      "moriarty\n",
      "and\n",
      "carella\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "cadbury\n",
      "schweppes\n",
      "and\n",
      "''\n",
      "''\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "tvt\n",
      "and\n",
      "and\n",
      "idj\n",
      "and\n",
      "siebel\n",
      "siebel\n",
      "and\n",
      "and\n",
      "''\n",
      "jimenez\n",
      "and\n",
      "''\n",
      "jiménez\n",
      "and\n",
      "and\n",
      "rebell\n",
      "''\n",
      "rebell\n",
      "dotson\n",
      "dennehy\n",
      "dotson\n",
      "dennehy\n",
      "and\n",
      "and\n",
      "jiuquan\n",
      "and\n",
      "and\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "monrovia\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "metre\n",
      "''\n",
      "metre\n",
      "and\n",
      "and\n",
      "?\n",
      "and\n",
      "schroeder\n",
      "stefani\n",
      "apologise\n",
      "berlusconi\n",
      "stefani\n",
      "silvio\n",
      "berlusconi\n",
      "openmanage\n",
      "and\n",
      "and\n",
      "''\n",
      "moriarty\n",
      "moriarty\n",
      "''\n",
      "lurd\n",
      "ja'neh\n",
      "and\n",
      "kedo\n",
      ":\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "bloomfield\n",
      "taubman\n",
      "cgt\n",
      "and\n",
      "cgt\n",
      "and\n",
      "and\n",
      "and\n",
      "benning\n",
      "benning\n",
      "gambian\n",
      "gambian\n",
      "roush\n",
      "mccloskey\n",
      "mccloskey\n",
      "and\n",
      "'\n",
      "osce\n",
      "osce\n",
      "gorgich\n",
      "and\n",
      "pashtoon\n",
      "gorgich\n",
      "and\n",
      "pashtoon\n",
      "sistan\n",
      "baluchestan\n",
      "bourada\n",
      "djamel\n",
      "badaoui\n",
      "and\n",
      "and\n",
      "fulvio\n",
      "berghella\n",
      "and\n",
      "fulvio\n",
      "berghella\n",
      "and\n",
      "italiane\n",
      "ingo\n",
      "kober\n",
      "ingo\n",
      "kober\n",
      "and\n",
      "and\n",
      "moldova\n",
      "and\n",
      "moldova\n",
      "janeiro\n",
      "janeiro\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "micheline\n",
      "and\n",
      "micheline\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "antonov\n",
      "vladimir\n",
      "huallaga\n",
      "cabezas\n",
      "managua\n",
      "nicaragua\n",
      "and\n",
      "and\n",
      "nikolai\n",
      "urakov\n",
      "nikolai\n",
      "urakov\n",
      "alstom\n",
      "and\n",
      "alstom\n",
      "and\n",
      "and\n",
      "nsg\n",
      "anatoly\n",
      "sokolov\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "alstom\n",
      "alstom\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "moldova\n",
      "dniester\n",
      "and\n",
      "moldova\n",
      "helmand\n",
      "helmand\n",
      "zambian\n",
      "felipe\n",
      "and\n",
      "sinaloa\n",
      "felipe\n",
      "and\n",
      "sinaloa\n",
      "osce\n",
      "osce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stockwell\n",
      "myong\n",
      "and\n",
      "myong\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kadhafi\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "yugoslavia\n",
      "and\n",
      "and\n",
      "tskhinvali\n",
      "and\n",
      "tirana\n",
      "and\n",
      "and\n",
      "abdel\n",
      "shalgam\n",
      "shalgam\n",
      "cambodia\n",
      "and\n",
      "and\n",
      "bruguiere\n",
      "zabol\n",
      "khoramabad\n",
      "and\n",
      "iguaran\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "offence\n",
      "and\n",
      "lavrov\n",
      "condoleezza\n",
      "condoleezza\n",
      "and\n",
      "and\n",
      "maseko\n",
      "themba\n",
      "maseko\n",
      "dmitry\n",
      "medvedev\n",
      "and\n",
      "and\n",
      "turkmenistan\n",
      "sistan\n",
      "baluchestan\n",
      "and\n",
      "and\n",
      "shingarev\n",
      "donohue\n",
      "tunisia\n",
      "and\n",
      "mozambique\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "fitzpatrick\n",
      "fitzpatrick\n",
      "and\n",
      "and\n",
      "estonian\n",
      "vladimir\n",
      "estonia\n",
      "andrus\n",
      "ansip\n",
      "vladimir\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "raghad\n",
      "raghad\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "panamanian\n",
      "and\n",
      "panamanian\n",
      "and\n",
      "and\n",
      "and\n",
      "cambodia\n",
      "and\n",
      "and\n",
      "saferworld\n",
      "and\n",
      "isbister\n",
      "saferworld\n",
      "and\n",
      "isbister\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "bulgaria\n",
      "balkan\n",
      "balkan\n",
      "elbaradei\n",
      "and\n",
      "and\n",
      "and\n",
      "nicaragua\n",
      "sandinista\n",
      "anastasio\n",
      "somoza\n",
      "nicaragua\n",
      "sandinista\n",
      "kurd\n",
      "kurdistan\n",
      "'\n",
      "pkk\n",
      "kurd\n",
      "kurdistan\n",
      "'\n",
      "pkk\n",
      "osce\n",
      "and\n",
      "baluyevsky\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "ecuadorian\n",
      "opaq\n",
      "and\n",
      "and\n",
      "nicaragua\n",
      "sandinista\n",
      "nicaragua\n",
      "sandinista\n",
      "vladimir\n",
      "and\n",
      "vladimir\n",
      "and\n",
      "tno\n",
      "detijd\n",
      "tno\n",
      "and\n",
      "and\n",
      "estonian\n",
      "honduran\n",
      "bulgarian\n",
      "ozawa\n",
      "and\n",
      "and\n",
      "and\n",
      "saferworld\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "?\n",
      "naeem\n",
      "quang\n",
      "ninh\n",
      "quang\n",
      "ninh\n",
      "karras\n",
      "dhillon\n",
      "durban\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "alh\n",
      "and\n",
      "and\n",
      "and\n",
      "mohanty\n",
      "and\n",
      "organisation\n",
      "mohanty\n",
      "finalise\n",
      "and\n",
      "organisation\n",
      "elbaradei\n",
      "elbaradei\n",
      "and\n",
      "tajik\n",
      "tajikistan\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "estonia\n",
      "latvia\n",
      "lithuania\n",
      "and\n",
      "slovakia\n",
      "estonia\n",
      "latvia\n",
      "lithuania\n",
      "and\n",
      "slovakia\n",
      "merwe\n",
      "geiges\n",
      "merwe\n",
      "geiges\n",
      "'\n",
      "zahedan\n",
      "and\n",
      "and\n",
      "mangabeira\n",
      "unger\n",
      "and\n",
      "unger\n",
      "and\n",
      "and\n",
      "alh\n",
      ";\n",
      "and\n",
      "alh\n",
      "and\n",
      "and\n",
      "ilmi\n",
      "and\n",
      "ilmi\n",
      "and\n",
      "and\n",
      "and\n",
      "blix\n",
      "and\n",
      "blix\n",
      "and\n",
      "helmand\n",
      "areva\n",
      "and\n",
      "mirjaveh\n",
      "and\n",
      "danilov\n",
      "valentin\n",
      "danilov\n",
      "and\n",
      "quy\n",
      "and\n",
      "grenell\n",
      "heinrich\n",
      "pierer\n",
      "heinrich\n",
      "pierer\n",
      "gerhard\n",
      "schroeder\n",
      "osce\n",
      "osce\n",
      "kazakhstan\n",
      "elbaradei\n",
      "elbarade\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "riyadh\n",
      "qaida\n",
      "and\n",
      "'\n",
      "qaida\n",
      "and\n",
      "jodran\n",
      "topol\n",
      "naeem\n",
      "zabol\n",
      "sistan\n",
      "baluchestan\n",
      "zahedan\n",
      "sistan\n",
      "baluchestan\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "jordanian\n",
      "maaruf\n",
      "bakhit\n",
      "raghdad\n",
      "and\n",
      "abdullah\n",
      "jordanian\n",
      "maaruf\n",
      "bakhit\n",
      "raghdad\n",
      "and\n",
      "abdullah\n",
      "dalai\n",
      "nicolas\n",
      "dalai\n",
      "and\n",
      "and\n",
      "gholam\n",
      "haddad\n",
      "adel\n",
      "gholam\n",
      "haddad\n",
      "adel\n",
      "and\n",
      "dalai\n",
      "godzhayev\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "afriforum\n",
      "afriforum\n",
      "and\n",
      "and\n",
      "and\n",
      "moldova\n",
      "and\n",
      "moldova\n",
      "and\n",
      "tskhinvali\n",
      "and\n",
      "''\n",
      "junichiro\n",
      "koizumi\n",
      "junichiro\n",
      "koizumi\n",
      "and\n",
      "'\n",
      "and\n",
      "and\n",
      "and\n",
      "helmand\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "vladimir\n",
      "vladimir\n",
      "and\n",
      "tunisian\n",
      "folco\n",
      "mejri\n",
      "estonia\n",
      "and\n",
      "estonia\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "dosari\n",
      "hossein\n",
      "safarlou\n",
      "and\n",
      "tajikistan\n",
      "and\n",
      "uzbekistan\n",
      "kyrgyzstan\n",
      "and\n",
      "chechnya\n",
      "and\n",
      "fulfil\n",
      "and\n",
      "kimball\n",
      "kimball\n",
      "and\n",
      "and\n",
      "korobeynichev\n",
      "livermore\n",
      "korobeynichev\n",
      "and\n",
      "nunn\n",
      "and\n",
      "nunn\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "albania\n",
      "bosnia\n",
      "herzegowina\n",
      "montenegro\n",
      "balkan\n",
      "balkans\n",
      "albania\n",
      "bosnia\n",
      "and\n",
      "herzegowina\n",
      "montenegro\n",
      "and\n",
      "and\n",
      "alh\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "kamynin\n",
      "kamynin\n",
      "and\n",
      "shalgam\n",
      "and\n",
      "and\n",
      "iguaran\n",
      "and\n",
      "iguaran\n",
      "‘\n",
      "'\n",
      "homs\n",
      "assad\n",
      "fidel\n",
      "'messianic\n",
      "'\n",
      ":\n",
      ":\n",
      "damascus\n",
      "damascus\n",
      "'suicide\n",
      "'\n",
      ":\n",
      "'wins\n",
      "'\n",
      "sandusky\n",
      "sandusky\n",
      ":\n",
      ":\n",
      "annan\n",
      "annan\n",
      "deir\n",
      "estia\n",
      ":\n",
      "savile\n",
      "savile\n",
      ":\n",
      "vladimir\n",
      "vladimir\n",
      "and\n",
      ":\n",
      "coptic\n",
      "coptic\n",
      "malawi\n",
      "malawi\n",
      "mutharika\n",
      "tuareg\n",
      "'azawad\n",
      "'\n",
      "mccann\n",
      "?\n",
      ":\n",
      "isaf\n",
      ":\n",
      "''\n",
      "ducati\n",
      ":\n",
      ":\n",
      "?\n",
      "'iran\n",
      "'\n",
      ":\n",
      ":\n",
      "kirkuk\n",
      "and\n",
      "and\n",
      "brazzaville\n",
      ":\n",
      "homs\n",
      "rimsha\n",
      "masih\n",
      "mandela\n",
      "sumatra\n",
      "sumatra\n",
      ":\n",
      "usgs\n",
      ";\n",
      "kirkuk\n",
      "'capture\n",
      "'\n",
      "papua\n",
      ":\n",
      "usgs\n",
      "papua\n",
      ":\n",
      "?\n",
      "?\n",
      ":\n",
      "'no\n",
      "'\n",
      ";\n",
      "halliburton\n",
      "halliburton\n",
      "tunisia\n",
      "'massacre\n",
      "'\n",
      "'anti\n",
      "'\n",
      "iaf\n",
      "'iran\n",
      "aviv\n",
      "'\n",
      ":\n",
      "aviv\n",
      "gusmao\n",
      "timor\n",
      "nieto\n",
      ":\n",
      ":\n",
      "homs\n",
      ":\n",
      "acapulco\n",
      "haqqani\n",
      "haqqani\n",
      "erdogan\n",
      "and\n",
      ":\n",
      "panetta\n",
      "panetta\n",
      ":\n",
      ":\n",
      "infiniti\n",
      ":\n",
      "gusmao\n",
      "timor\n",
      "gusmao\n",
      "timor\n",
      "'massacre\n",
      "'\n",
      ":\n",
      "aleppo\n",
      "gazans\n",
      ";\n",
      "gazans\n",
      ";\n",
      "demjanjuk\n",
      "demjanjuk\n",
      "breivik\n",
      "breivik\n",
      "eastenders\n",
      "mccluskie\n",
      ":\n",
      "'hearing\n",
      "'\n",
      ":\n",
      "hollande\n",
      "mandela\n",
      "mandela\n",
      "morsi\n",
      "shater\n",
      "dagestan\n",
      "'\n",
      "'\n",
      "rowling\n",
      "aleppo\n",
      "algerian\n",
      "algerian\n",
      ";\n",
      ":\n",
      ":\n",
      "rasmussen\n",
      ":\n",
      "woodside\n",
      "eshkol\n",
      ";\n",
      "eshkol\n",
      ";\n",
      "'for\n",
      "'\n",
      "waziristan\n",
      ":\n",
      "salvador\n",
      "salvador\n",
      "?\n",
      "nicolas\n",
      "tunis\n",
      "tunisian\n",
      "dominique\n",
      "waziristan\n",
      "capriati\n",
      "ictu\n",
      "ibec\n",
      "armenia\n",
      "sheryl\n",
      "sheryl\n",
      ":\n",
      ":\n",
      ":\n",
      "gauck\n",
      "joachim\n",
      "gauck\n",
      "'peace\n",
      "'\n",
      "and\n",
      "and\n",
      "falklands\n",
      "falklands\n",
      ":\n",
      "qpr\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "'stand\n",
      "'\n",
      "montréal\n",
      "mulayam\n",
      ":\n",
      "mulayam\n",
      ":\n",
      "digvijay\n",
      "firestone\n",
      "furyk\n",
      ":\n",
      "aleppo\n",
      "annan\n",
      "'shoddy\n",
      "'\n",
      "newsnight\n",
      "coptic\n",
      ";\n",
      "aust\n",
      ":\n",
      ":\n",
      "hrw\n",
      ":\n",
      ":\n",
      "and\n",
      "qaida\n",
      "qaida\n",
      "aviv\n",
      ":\n",
      "nightline\n",
      "houla\n",
      "houla\n",
      ":\n",
      "senegal\n",
      "grameen\n",
      "'wins\n",
      "'\n",
      "damascus\n",
      "assad\n",
      "assad\n",
      ";\n",
      "homs\n",
      "beersheba\n",
      "'more\n",
      "timbuktu\n",
      "'\n",
      "timbuktu\n",
      "vatileaks\n",
      "''\n",
      "repsol\n",
      "ypf\n",
      "repsol\n",
      "aung\n",
      "suu\n",
      "aung\n",
      "suu\n",
      ":\n",
      "mansoureh\n",
      "hosseini\n",
      "'promise\n",
      "'\n",
      "'restore\n",
      "'\n",
      "'kony\n",
      "'\n",
      "mandela\n",
      "nasrallah\n",
      "djokovic\n",
      ":\n",
      "djokovic\n",
      "'costing\n",
      "'\n",
      ";\n",
      ";\n",
      ":\n",
      "yemeni\n",
      "'did\n",
      "'\n",
      "savile\n",
      "savile\n",
      "bskyb\n",
      "bskyb\n",
      "bradbury\n",
      "bradbury\n",
      ":\n",
      ":\n",
      ":\n",
      "defence\n",
      "correa\n",
      ":\n",
      "aceh\n",
      "bagram\n",
      "bagram\n",
      "'overstepped\n",
      "'\n",
      "spad\n",
      "blitzbokke\n",
      "blitzbokke\n",
      "and\n",
      "'grenade\n",
      "'\n",
      "and\n",
      "'scores\n",
      "'\n",
      "bairstow\n",
      "and\n",
      "eoin\n",
      "bairstow\n",
      "'use\n",
      "'\n",
      "'tricks\n",
      "'\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      "aleppo\n",
      "damascus\n",
      "hollande\n",
      "aust\n",
      "fatah\n",
      "sylvester\n",
      "stallone\n",
      "sylvester\n",
      "stalloneâ€™s\n",
      "stallone\n",
      ":\n",
      "hollande\n",
      "timor\n",
      ":\n",
      "“\n",
      "”\n",
      "'attractive\n",
      "'\n",
      ":\n",
      "morsi\n",
      "eygptian\n",
      "mursi\n",
      "khartoum\n",
      "allenby\n",
      ":\n",
      "and\n",
      "?\n",
      ":\n",
      "napolitano\n",
      ":\n",
      "'whitey\n",
      "'\n",
      "bulger\n",
      "''\n",
      "bulger\n",
      "mauritanian\n",
      "mauritanian\n",
      "breivik\n",
      "breivik\n",
      "okinawa\n",
      "okinawa\n",
      ":\n",
      ":\n",
      "and\n",
      "'pop\n",
      "'\n",
      "'pop\n",
      "'\n",
      ":\n",
      ":\n",
      ";\n",
      "korobka\n",
      "algerian\n",
      ":\n",
      "'hunger\n",
      "'\n",
      "'wins\n",
      "'\n",
      "'wins\n",
      "'\n",
      ":\n",
      "rohtak\n",
      "rohtak\n",
      "haryana\n",
      "zimmerman\n",
      "'to\n",
      "'\n",
      "trayvon\n",
      "zimmerman\n",
      "trayvon\n",
      ":\n",
      ":\n",
      "'life\n",
      "'\n",
      "'framework\n",
      "'\n",
      ":\n",
      "histadrut\n",
      ":\n",
      ":\n",
      "npp\n",
      "ryanair\n",
      "lingus\n",
      "ryanair\n",
      "arqule\n",
      "daiichi\n",
      "arqule\n",
      "and\n",
      "papua\n",
      "usgs\n",
      ":\n",
      "and\n",
      ":\n",
      ":\n",
      "annan\n",
      "'flytilla\n",
      "'\n",
      "'flytilla\n",
      "'\n",
      "spacex\n",
      ":\n",
      "?\n",
      "spacex\n",
      "'million\n",
      "'\n",
      "ahmadinejad\n",
      "ahmadinejad\n",
      ":\n",
      ":\n",
      "'biggest\n",
      "'\n",
      "'regret\n",
      "'\n",
      ":\n",
      ";\n",
      "aung\n",
      "suu\n",
      "aung\n",
      "suu\n",
      "waitangi\n",
      "breivik\n",
      "aust\n",
      ":\n",
      ":\n",
      ":\n",
      "''\n",
      ":\n",
      "'honor\n",
      "'\n",
      "honour\n",
      "'ca\n",
      "'\n",
      "‘\n",
      "'\n",
      "homs\n",
      ":\n",
      ":\n",
      ":\n",
      "?\n",
      "gazzara\n",
      "gazzara\n",
      ":\n",
      ";\n",
      "'constructive\n",
      "'\n",
      ":\n",
      "savile\n",
      ":\n",
      ":\n",
      "savile\n",
      "'massacre\n",
      "'\n",
      ":\n",
      "suu\n",
      "suu\n",
      "aust\n",
      "homs\n",
      "homs\n",
      "lavrov\n",
      "honours\n",
      "petraeus\n",
      "petraeus\n",
      "nkorea\n",
      "''\n",
      "maldives\n",
      "maldives\n",
      "'kills\n",
      "'\n",
      "qaida\n",
      "yemeni\n",
      "abyan\n",
      "'jihad\n",
      "'\n",
      "rathband\n",
      ":\n",
      "'bestest\n",
      "'\n",
      "miliband\n",
      ":\n",
      "and\n",
      "labour\n",
      "aleppo\n",
      "aleppo\n",
      "apologise\n",
      "'faked\n",
      "'\n",
      "homs\n",
      "homs\n",
      "tunisia\n",
      "tunisia\n",
      "assad\n",
      "assad\n",
      "and\n",
      ":\n",
      "contador\n",
      ":\n",
      "usain\n",
      ":\n",
      "'india\n",
      "'\n",
      ":\n",
      "armenia\n",
      "!\n",
      ":\n",
      ":\n",
      "usgs\n",
      "sunda\n",
      ":\n",
      "usgs\n",
      "haqqani\n",
      "haqqani\n",
      ":\n",
      "dealbook\n",
      ":\n",
      "homs\n",
      "homs\n",
      "'more\n",
      "'\n",
      "'die\n",
      "'\n",
      "eshkol\n",
      ";\n",
      "eshkol\n",
      "'betrayed\n",
      "'\n",
      "remploy\n",
      ":\n",
      ":\n",
      "nhc\n",
      "elkeib\n",
      "koichiro\n",
      "gemba\n",
      "barrick\n",
      "'less\n",
      "'\n",
      "'less\n",
      "'\n",
      ":\n",
      "usgs\n",
      "recognise\n",
      ":\n",
      ";\n",
      "yvette\n",
      "'gang\n",
      "'\n",
      "keita\n",
      "'\n",
      "'postpones\n",
      "'\n",
      "'who\n",
      "'\n",
      "'survives\n",
      "'\n",
      "schumacher\n",
      "schumacher\n",
      "and\n",
      "abrams\n",
      "kazan\n",
      "'determined\n",
      "'\n",
      ":\n",
      "jagan\n",
      "seemandhra\n",
      ":\n",
      "jagan\n",
      ";\n",
      "seemandhra\n",
      "maracana\n",
      "maracana\n",
      ":\n",
      "+video\n",
      "mandela\n",
      "mandela\n",
      "tunis\n",
      "morsi\n",
      ";\n",
      "celac\n",
      "celac\n",
      ":\n",
      ":\n",
      "malian\n",
      "qatada\n",
      "qatada\n",
      "kaesong\n",
      ":\n",
      "''\n",
      "zimmerman\n",
      "zimmerman\n",
      "rouhani\n",
      "damascus\n",
      "and\n",
      "gabbana\n",
      "and\n",
      "gabbana\n",
      ":\n",
      "cenc\n",
      "and\n",
      "assad\n",
      "?\n",
      "jenni\n",
      "jenni\n",
      ":\n",
      ":\n",
      "'activities\n",
      "'\n",
      "rouhani\n",
      "rouhani\n",
      "'final\n",
      "'\n",
      "kaesong\n",
      "''\n",
      "morsi\n",
      "yacimovich\n",
      "'\n",
      ":\n",
      "'ban\n",
      "'\n",
      ":\n",
      "'are\n",
      "'\n",
      "morsi\n",
      "morsi\n",
      ":\n",
      "hrithik\n",
      "sussanne\n",
      "hrithik\n",
      "sussanne\n",
      "deraa\n",
      "x'mas\n",
      "and\n",
      "fayed\n",
      "and\n",
      "fayed\n",
      "snowden\n",
      "snowden\n",
      "narendra\n",
      "joran\n",
      "sloots\n",
      ":\n",
      ":\n",
      "'hit\n",
      "'\n",
      "and\n",
      "ryanair\n",
      "stansted\n",
      ":\n",
      "ryanair\n",
      ":\n",
      "and\n",
      ";\n",
      "mandela\n",
      "and\n",
      "mandela\n",
      "damascus\n",
      "'friends\n",
      "'\n",
      "abvp\n",
      "jantar\n",
      "mantar\n",
      "jantar\n",
      "mantar\n",
      "offence\n",
      "fermin\n",
      "fermin\n",
      "briton\n",
      "lapd\n",
      "'blasphemy\n",
      "'\n",
      "stonesoft\n",
      "stonesoft\n",
      ":\n",
      "benghazi\n",
      "benghazi\n",
      "'roadmap\n",
      "'\n",
      ":\n",
      ":\n",
      ";\n",
      ":\n",
      "macau\n",
      "macau\n",
      "mandela\n",
      "mandela\n",
      ":\n",
      "'determined\n",
      "'\n",
      "and\n",
      "finney\n",
      "finney\n",
      "and\n",
      "benghazi\n",
      "benghazi\n",
      "and\n",
      "timbuktu\n",
      "timbuktu\n",
      "'beaten\n",
      "'\n",
      "'tremendous\n",
      "'\n",
      "sarangani\n",
      "cotabato\n",
      ":\n",
      "'insider\n",
      "'\n",
      "tunisian\n",
      "tunisia\n",
      ";\n",
      "zimmerman\n",
      "trayvon\n",
      "and\n",
      "zimmerman\n",
      "trayvon\n",
      "snowden\n",
      "snowden\n",
      "capriles\n",
      "bezos\n",
      "bezos\n",
      "mandela\n",
      "mandela\n",
      "paralyse\n",
      "bezos\n",
      "bezos\n",
      ":\n",
      "mursi\n",
      "mursi\n",
      "usgs\n",
      ":\n",
      "'credibility\n",
      "'\n",
      ":\n",
      "'freezes\n",
      "'\n",
      ":\n",
      "kyiv\n",
      "lenin\n",
      "waziristan\n",
      "and\n",
      "briton\n",
      "volgograd\n",
      "mandela\n",
      "mandela\n",
      "'unstable\n",
      "'\n",
      "cypriot\n",
      ":\n",
      "hushen\n",
      "hushen\n",
      "benghazi\n",
      "benghazi\n",
      "nra\n",
      "honduran\n",
      "honduras\n",
      "‘\n",
      "'\n",
      ":\n",
      "sinai\n",
      "sinai\n",
      "morsi\n",
      "morsi\n",
      "pistorius\n",
      "pistorius\n",
      "vinnie\n",
      "vinnie\n",
      "hakimullah\n",
      "mehsud\n",
      "hakimullah\n",
      "'dozens\n",
      "'\n",
      ":\n",
      "mandela\n",
      "'inspiration\n",
      "'\n",
      "mandela\n",
      "'to\n",
      "'\n",
      "'to\n",
      "and\n",
      "'\n",
      "'concrete\n",
      "'\n",
      "monti\n",
      "'palestinian\n",
      "fayyad\n",
      "'\n",
      "'schizophrenic\n",
      "'\n",
      "mandela\n",
      "'mistake\n",
      "'\n",
      "mandela\n",
      "''\n",
      "bethlehem\n",
      "bethlehem\n",
      "abyei\n",
      ":\n",
      ":\n",
      "equatoguineans\n",
      ":\n",
      "'insider\n",
      "'\n",
      "hebron\n",
      "sinai\n",
      "sinai\n",
      ":\n",
      "qaida\n",
      ":\n",
      "'in\n",
      "'\n",
      "asiana\n",
      ":\n",
      "buckingham\n",
      "buckingham\n",
      "'\n",
      ":\n",
      "hagel\n",
      "''\n",
      "defence\n",
      "hagel\n",
      ":\n",
      "morsi\n",
      "fitow\n",
      "fitow\n",
      "mandela\n",
      "mandela\n",
      "navalny\n",
      "diyala\n",
      "froome\n",
      "belarus\n",
      "latvia\n",
      "briton\n",
      "sinai\n",
      "briton\n",
      "sinai\n",
      "'\n",
      "kweifiya\n",
      "?\n",
      ";\n",
      "snowden\n",
      "snowden\n",
      "taleban\n",
      "videotron\n",
      "videotron\n",
      "berlusconi\n",
      "bulgaria\n",
      ":\n",
      ":\n",
      "mandela\n",
      "mandela\n",
      "modis\n",
      "technion\n",
      "maldives\n",
      "maldives\n",
      "badie\n",
      ":\n",
      "'israeli\n",
      "'\n",
      "mossad\n",
      "hebron\n",
      "xilai\n",
      "'near\n",
      "'\n",
      ":\n",
      "'alone\n",
      "'\n",
      ":\n",
      "and\n",
      "'wealth\n",
      "'\n",
      ":\n",
      "kubiak\n",
      "kubiak\n",
      "speakes\n",
      "mandela\n",
      "mandela\n",
      "peres\n",
      "peres\n",
      "tunisia\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      "pakista\n",
      "mekong\n",
      "mekong\n",
      "'to\n",
      "'\n",
      "merkel\n",
      "merkel\n",
      "hagel\n",
      "and\n",
      "and\n",
      "cambodian\n",
      "rainsy\n",
      "cambodia\n",
      ":\n",
      "sinai\n",
      ":\n",
      ":\n",
      ":\n",
      "sudanese\n",
      "''\n",
      "mandela\n",
      "mandela\n",
      "lampedusa\n",
      "mansour\n",
      "adly\n",
      "mansour\n",
      ":\n",
      ":\n",
      "centre\n",
      "ebute\n",
      "'war\n",
      "'\n",
      ":\n",
      "‘\n",
      "'\n",
      "merkel\n",
      "merkel\n",
      "mcdonnell\n",
      "mcdonnell\n",
      "and\n",
      "asafa\n",
      "honshu\n",
      ":\n",
      "cenc\n",
      ":\n",
      "cenc\n",
      "and\n",
      "rowhani\n",
      "rohani\n",
      "'groundless\n",
      "'\n",
      "dzhokhar\n",
      "tsarnaev\n",
      "dzhokhar\n",
      "tsarnaev\n",
      ":\n",
      "'\n",
      "timor\n",
      "timor\n",
      "taleban\n",
      "morsi\n",
      "mandela\n",
      "mandela\n",
      "'loud\n",
      "'\n",
      "and\n",
      "suu\n",
      "suu\n",
      "zambian\n",
      "yellen\n",
      "yellen\n",
      "ojukwu\n",
      "apga\n",
      "ojukwu\n",
      "apga\n",
      "'conclusively\n",
      "'\n",
      ":\n",
      "snowden\n",
      "snowden\n",
      "yellen\n",
      "yellen\n",
      "'large\n",
      "and\n",
      "'\n",
      "and\n",
      ":\n",
      "malala\n",
      "malala\n",
      "and\n",
      "shawal\n",
      "miranshah\n",
      "fallujah\n",
      "mandela\n",
      "and\n",
      "mandela\n",
      "ahmadinejad\n",
      "ahmadinejad\n",
      "miliband\n",
      ":\n",
      ":\n",
      "cambodian\n",
      "klci\n",
      "hollande\n",
      "'threatens\n",
      "'\n",
      "françois\n",
      "hollande\n",
      "'blade\n",
      "'\n",
      "pistorius\n",
      "pistorius\n",
      "hebron\n",
      "'judge\n",
      "'\n",
      "defence\n",
      "bopha\n",
      "yellen\n",
      "yellen\n",
      "hakimullah\n",
      "mehsud\n",
      "hakimullah\n",
      "mehsud\n",
      "baquba\n",
      "assad\n",
      "algeria\n",
      "maldives\n",
      "liberman\n",
      "xaver\n",
      "talbot\n",
      "talbot\n",
      "'sex\n",
      "'\n",
      ";\n",
      ";\n",
      "rebelheld\n",
      "rebelheld\n",
      "morsi\n",
      "morsi\n",
      "'shutdown\n",
      "'\n",
      "garzon\n",
      "isaf\n",
      "'insider\n",
      "'\n",
      "‘\n",
      "'\n",
      ":\n",
      "phailin\n",
      "moily\n",
      "benghazi\n",
      "benghazi\n",
      "kyiv\n",
      "bhutan\n",
      "tian'anmen\n",
      "tiananmen\n",
      "'whitey\n",
      "'\n",
      "bulger\n",
      "bulger\n",
      ":\n",
      "algeria\n",
      ";\n",
      "algeria\n",
      "morsi\n",
      ":\n",
      "morsi\n",
      "mandela\n",
      ":\n",
      "mandela\n",
      "taksim\n",
      "taksim\n",
      "guatemala\n",
      "'compares\n",
      "'\n",
      "'compares\n",
      "vladimir\n",
      "adolf\n",
      "'\n",
      "damascus\n",
      "morsi\n",
      ":\n",
      "morsi\n",
      "snowden\n",
      "'given\n",
      "'\n",
      "snowden\n",
      "president‚äôs\n",
      "'cautiously\n",
      "'\n",
      "'suspicious\n",
      "'\n",
      "snowden\n",
      "snowden\n",
      "ramallah\n",
      "'taliban\n",
      "'\n",
      "'kill\n",
      "'\n",
      "morsi\n",
      "morsi\n",
      "doris\n",
      "lessing\n",
      "doris\n",
      "lessing\n",
      "zimbabwe‚äôs\n",
      "mugabe‚äôs\n",
      "lenin\n",
      "mandela\n",
      "mandela\n",
      "|\n",
      ":\n",
      "nypd\n",
      "nypd\n",
      "colman\n",
      "bafta\n",
      "baftas\n",
      ":\n",
      "colman\n",
      "ankeet\n",
      "chavan\n",
      "ankeet\n",
      "chavan\n",
      "‚äėtitillating‚äô\n",
      "''\n",
      "‚äėglee‚äô\n",
      "monteith\n",
      "monteith\n",
      ":\n",
      "‚äėglee‚äô\n",
      "'unity\n",
      "'\n",
      ":\n",
      "'has\n",
      "'\n",
      "sinai\n",
      "sinai\n",
      "crimea\n",
      "merkel\n",
      ":\n",
      "mandela\n",
      "mandela\n",
      "'to\n",
      "'\n",
      "'buried\n",
      "'\n",
      "donetsk\n",
      "and\n",
      "roache\n",
      "'amazing\n",
      "'\n",
      ":\n",
      "'kills\n",
      "'\n",
      ":\n",
      "tehelka\n",
      "tehelka\n",
      "morsi\n",
      "morsi\n",
      "'morning\n",
      "'\n",
      ";\n",
      "lenin\n",
      "lenin\n",
      "'thwarts\n",
      "'\n",
      "and\n",
      ":\n",
      "and\n",
      "airalgerie\n",
      "airalgerie\n",
      "and\n",
      "khurshid‚äôs\n",
      "ministers‚äô\n",
      "'steered\n",
      "'\n",
      "shereka\n",
      ":\n",
      "shereka\n",
      "'bled\n",
      "'\n",
      "'killed\n",
      "'\n",
      "gandolfini\n",
      "mandela\n",
      "'severe\n",
      "'\n",
      ":\n",
      "'serious\n",
      "'\n",
      "?\n",
      "'ladies\n",
      "'\n",
      "'ladies\n",
      "'\n",
      ":\n",
      "assad\n",
      "assad\n",
      "mandela\n",
      "mandela\n",
      "houthi\n",
      ":\n",
      "crimean\n",
      ":\n",
      ":\n",
      "and\n",
      "yanukovych\n",
      "yanukovich\n",
      "yellen\n",
      "yellen\n",
      "rouhani\n",
      "mandela\n",
      "'selfie\n",
      "'\n",
      "mandela\n",
      ":\n",
      "crimea\n",
      "crimean\n",
      "sewol\n",
      ":\n",
      "mandela\n",
      "mandela\n",
      ":\n",
      "kyiv\n",
      ":\n",
      ":\n",
      "'yes\n",
      "'\n",
      "hague\n",
      "'concerns\n",
      "'\n",
      ":\n",
      "'grave\n",
      "'\n",
      "desmarais\n",
      ";\n",
      "'western\n",
      "'\n",
      "bohol\n",
      "'objects\n",
      "'\n",
      "haiyan\n",
      "haiyan\n",
      "'\n",
      "and\n",
      "'\n",
      ";\n",
      "jiroemon\n",
      ":\n",
      "and\n",
      "‚äď\n",
      "and\n",
      "‚äď\n",
      "'it\n",
      "'\n",
      ":\n",
      "kazakhstan\n",
      "'has\n",
      "'\n",
      ":\n",
      "'to\n",
      "'\n",
      "'\n",
      "'\n",
      "crimea\n",
      ":\n",
      ":\n",
      "seamus\n",
      "heaney\n",
      "and\n",
      "seamus\n",
      "heaney\n",
      "and\n",
      "bangladeshi\n",
      "'apostasy\n",
      "'\n",
      "kerry‚äôs\n",
      "'quiet\n",
      "'\n",
      ":\n",
      "mubarak‚äôs\n",
      ":\n",
      ";\n",
      ":\n",
      "'worse\n",
      "'\n",
      "nkorea\n",
      "'\n",
      "colourful\n",
      "merkel\n",
      "mandela\n",
      "mandela\n",
      "'intensive\n",
      "'\n",
      "sewol\n",
      "dprk\n",
      "solomons\n",
      ":\n",
      ":\n",
      "and\n",
      "?\n",
      "and\n",
      "and\n",
      "algerian\n",
      "algerian\n",
      "sinai\n",
      "sinai\n",
      "didier\n",
      "reynders\n",
      ":\n",
      ":\n",
      "‚äď\n",
      "and\n",
      "and\n",
      "nokia‚äôs\n",
      "italy‚äôs\n",
      "berlusconi\n",
      ":\n",
      "mandela\n",
      "‚äėserious\n",
      "stable‚äô\n",
      "mandela\n",
      "‚äėserious\n",
      "stable‚äô\n",
      "‚äď\n",
      "!\n",
      "‚äď\n",
      "!\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      ":\n",
      "world‚äôs\n",
      "aleppo\n",
      ":\n",
      ":\n",
      "africalive\n",
      "phailin\n",
      "tunisia\n",
      "tunisia\n",
      "mandela\n",
      "mandela\n",
      "caucasus\n",
      ":\n",
      ":\n",
      ":\n",
      "xilai\n",
      "xilai\n",
      "briton\n",
      "amsterdammers\n",
      "magnitsky\n",
      "magnitsky\n",
      "'trayvon\n",
      "'\n",
      "trayvon\n",
      ":\n",
      "erdogan\n",
      "erdogan\n",
      "mandela\n",
      "mandela\n",
      ":\n",
      ":\n",
      "'kills\n",
      "'\n",
      "hagel\n",
      "‚äėunity\n",
      "talks‚äô\n",
      "‚äėwar\n",
      "arms‚äô\n",
      "bhutto\n",
      "panda‚äôs\n",
      "india‚äôs\n",
      "'costing\n",
      "'\n",
      "janeiro\n",
      "badie\n",
      "'catches\n",
      "and\n",
      "'\n",
      "mandela\n",
      "mandela\n",
      "jordanian\n",
      ";\n",
      "kaesong\n",
      "algerian\n",
      "waqar\n",
      "younis\n",
      "waqar\n",
      "younis\n",
      "crimea\n",
      "crimea\n",
      "''\n",
      "assad\n",
      "'heavy\n",
      "'\n",
      "snowden\n",
      "'no\n",
      "'\n",
      "snowden\n",
      "''\n",
      ":\n",
      ":\n",
      "legalise\n",
      "legalise\n",
      "senegalese\n",
      "and\n",
      "'crashes\n",
      "'\n",
      "monteith\n",
      "'exploratory\n",
      "'\n",
      "assad\n",
      "homs\n",
      "assad\n",
      "westgate\n",
      "westgate\n",
      "honshu\n",
      ":\n",
      "cenc\n",
      ":\n",
      "cenc\n",
      "lenin\n",
      "'monitoring\n",
      "'\n",
      "farage\n",
      "'no\n",
      "'\n",
      "peres\n",
      "gonsalves\n",
      ":\n",
      ":\n",
      "morsi\n",
      "morsi\n",
      ":\n",
      "and\n",
      "favourite\n",
      "griff\n",
      "rhys\n",
      ":\n",
      "'\n",
      ":\n",
      ":\n",
      "ukraine‚äôs\n",
      "sudanese\n",
      "tikrit\n",
      "tikrit\n",
      ":\n",
      ":\n",
      "shankill\n",
      "monteith\n",
      "'glee\n",
      "'\n",
      "monteith\n",
      "yolanda\n",
      "khamenei\n",
      "and\n",
      "world‚äôs\n",
      "italy‚äôs\n",
      "centre\n",
      ":\n",
      ":\n",
      "'an\n",
      "'\n",
      "crimea\n",
      "farage\n",
      ":\n",
      ":\n",
      "‚äėwill\n",
      "interests‚äô\n",
      "‚äėdirect‚äô\n",
      "mandela\n",
      ":\n",
      "zimmerman\n",
      "zimmerman\n",
      "offence\n",
      "offence\n",
      ":\n",
      "ftse\n",
      ":\n",
      "ftse\n",
      ":\n",
      ":\n",
      "mandela\n",
      "'could\n",
      "'\n",
      "winchester\n",
      "'iron\n",
      "'\n",
      "'iron\n",
      "'\n",
      "'shirk\n",
      "'\n",
      "maldives\n",
      "keita\n",
      "'fires\n",
      "'\n",
      "donegal\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "'accept\n",
      "'\n",
      "xinjiang\n",
      "mko\n",
      "'unnatural\n",
      "'\n",
      "rumour\n",
      "'unnatural\n",
      "'\n",
      "'ready\n",
      ":\n",
      "'\n",
      ":\n",
      "mandela\n",
      "labour\n",
      ":\n",
      "abdullah\n",
      ":\n",
      "yemeni\n",
      "and\n",
      "astrazeneca\n",
      "squibb\n",
      "astrazeneca\n",
      "ehud\n",
      "olmert\n",
      "ehud\n",
      "olmert\n",
      "brahimi\n",
      "apologise\n",
      "morsi\n",
      "sochi\n",
      "hague\n",
      ":\n",
      "'sensible\n",
      "'\n",
      ";\n",
      "'objects\n",
      "'\n",
      "kenya‚äôs\n",
      "man‚äôs\n",
      "sinai\n",
      "sinai\n",
      ":\n",
      "doris\n",
      "lessing\n",
      "doris\n",
      "lessing\n",
      "authorise\n",
      "'set\n",
      "'\n",
      "crimea\n",
      "crimea\n",
      "aviv\n",
      "aviv\n",
      ":\n",
      ":\n",
      "merkel\n",
      "malawi\n",
      "alawite\n",
      "haiyan\n",
      ":\n",
      "assad\n",
      "assad\n",
      "'sorry\n",
      "'\n",
      "mursi\n",
      "morsi\n",
      "yellen\n",
      "yellen\n",
      "mayawati\n",
      "mayawati\n",
      "and\n",
      "programme\n",
      "silvio\n",
      "berlusconi\n",
      "silvio\n",
      "berlusconi\n",
      "'respond\n",
      "'\n",
      "hosni\n",
      "qatari\n",
      "tamim\n",
      "'friday\n",
      "'\n",
      "morsi\n",
      "morsi\n",
      "doris\n",
      "lessing\n",
      "doris\n",
      "lessing\n",
      "sumatra\n",
      "usgs\n",
      "?\n",
      "?\n",
      ":\n",
      "hizbullah\n",
      "and\n",
      "psni\n",
      "ruc\n",
      "psni\n",
      "'failed\n",
      "ruc\n",
      "'\n",
      "'very\n",
      "'\n",
      "mandela\n",
      "'improved\n",
      "'\n",
      "mandela\n",
      "'worsened\n",
      "'\n",
      "'gas\n",
      "'\n",
      "tymoshenko\n",
      "napolitano\n",
      "napolitano\n",
      "bosnian\n",
      "croat\n",
      "bosnians\n",
      "croat\n",
      "'justice\n",
      "trayvon\n",
      "'\n",
      "'justice\n",
      "trayvon\n",
      "'\n",
      "homs\n",
      "homs\n",
      "hagel\n",
      "dempsey\n",
      "hagel\n",
      "''\n",
      "defence\n",
      "and\n",
      "axe\n",
      ":\n",
      "mandela\n",
      "mandela\n",
      "and\n",
      "'\n",
      "zimmerman\n",
      "trayvon\n",
      "zimmerman\n",
      "waterford\n",
      "mandela\n",
      "mandela\n",
      ":\n",
      "mandela\n",
      "'incorrect\n",
      "'\n",
      "mandela\n",
      "and\n",
      "?\n",
      "britains\n",
      ":\n",
      ":\n",
      "dprk\n",
      "'special\n",
      "'\n",
      "erdogan\n",
      "''\n",
      "?\n",
      "'jihadi\n",
      "?\n",
      "'\n",
      "kalmaegi\n",
      "kalmaegi\n",
      "yemeni\n",
      "raiwind\n",
      "houthis\n",
      ":\n",
      ":\n",
      "aqsa\n",
      "boko\n",
      "boko\n",
      "?\n",
      "'concern\n",
      "'\n",
      "tunisia\n",
      "tunis\n",
      ":\n",
      ":\n",
      "guinean\n",
      "micronesia\n",
      "bolivia\n",
      "tajikistan\n",
      "'exploits\n",
      "'\n",
      ":\n",
      "ipi\n",
      "jazeera\n",
      "jazeera\n",
      ":\n",
      ":\n",
      ":\n",
      "burundi\n",
      "montenegrin\n",
      "ashya\n",
      "ashya\n",
      "kranjcar\n",
      "qpr\n",
      "pistorius\n",
      "'bookkeeper\n",
      "auschwitz\n",
      "'\n",
      "menendez\n",
      "ftii\n",
      "boko\n",
      "nigeria\\\n",
      "turnbull\n",
      "belarus\n",
      "?\n",
      "lesotho\n",
      ":\n",
      "'panicked\n",
      "'\n",
      "johor\n",
      ":\n",
      "narendra\n",
      "'photoshopped\n",
      "'\n",
      "narendra\n",
      "and\n",
      "clodagh\n",
      "merkel\n",
      "latvia\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "and\n",
      "and\n",
      "and\n",
      "axe\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lrb\n",
      "rrb\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "swinge\n",
      "swinge\n",
      "and\n",
      "bathingsuit\n",
      "grey\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "atvs\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "''\n",
      "and\n",
      ":\n",
      ":\n",
      ":\n",
      "!\n",
      "!\n",
      "and\n",
      "?\n",
      "?\n",
      ":\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "?\n",
      ":\n",
      ":\n",
      ":\n",
      "?\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      "?\n",
      "?\n",
      "and\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      ":\n",
      ":\n",
      ";\n",
      ":\n",
      "nepalese\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      ":\n",
      "?\n",
      "?\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "!\n",
      "!\n",
      ":\n",
      "?\n",
      "?\n",
      "zurich\n",
      ":\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "?\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      ":\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "''\n",
      ":\n",
      ":\n",
      "bremer\n",
      "and\n",
      "bremer\n",
      "and\n",
      "claudette\n",
      "claudette\n",
      "pepsico\n",
      ":\n",
      "alcoa\n",
      ":\n",
      "liberia\n",
      "and\n",
      "liberia\n",
      "odette\n",
      "and\n",
      "and\n",
      "and\n",
      "chiron\n",
      "powderject\n",
      "chiron\n",
      "powderject\n",
      "''\n",
      "gilles\n",
      "diebold\n",
      "''\n",
      "gilles\n",
      "hewlett\n",
      "packard\n",
      "springdale\n",
      "springdale\n",
      "harrigan\n",
      "'\n",
      "and\n",
      "harrigan\n",
      "'\n",
      "and\n",
      "and\n",
      "heatley\n",
      "and\n",
      "heatley\n",
      "and\n",
      "''\n",
      "popovich\n",
      "''\n",
      "popovich\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "moffitt\n",
      "moffitt\n",
      ":\n",
      "?\n",
      "mandalay\n",
      "mandalay\n",
      "and\n",
      "{\n",
      "suu\n",
      "aung\n",
      "suu\n",
      "and\n",
      "and\n",
      "eddington\n",
      "''\n",
      "''\n",
      "eddington\n",
      "''\n",
      "''\n",
      "goodrich\n",
      "montefiore\n",
      "recognise\n",
      "realise\n",
      "and\n",
      "and\n",
      "loral\n",
      ";\n",
      "loral\n",
      "and\n",
      "and\n",
      "''\n",
      "anatolian\n",
      "anatolian\n",
      "and\n",
      "and\n",
      "and\n",
      "barbini\n",
      "and\n",
      "veneman\n",
      "and\n",
      "lawtey\n",
      "lawtey\n",
      "and\n",
      "palmsource\n",
      "and\n",
      "palmsource\n",
      "socorro\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "enrol\n",
      "enrol\n",
      "''\n",
      "''\n",
      "and\n",
      "instinet\n",
      "burrell\n",
      "burrell\n",
      "and\n",
      "gillespie\n",
      "moonves\n",
      "gillespie\n",
      "moonves\n",
      "''\n",
      "blondel\n",
      "ouvriere\n",
      "and\n",
      "''\n",
      "blondel\n",
      "ouvriere\n",
      "peoplesoft\n",
      "and\n",
      "''\n",
      "ellison\n",
      "peoplesoft\n",
      "and\n",
      "''\n",
      "and\n",
      "ibc\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "bolivia\n",
      "bolivia\n",
      "and\n",
      "and\n",
      "skagit\n",
      "skagit\n",
      "and\n",
      "and\n",
      ";\n",
      "o'connor\n",
      "rehnquist\n",
      "souter\n",
      "bader\n",
      "ginsburg\n",
      "and\n",
      "breyer\n",
      "o'connor\n",
      "souter\n",
      "bader\n",
      "ginsburg\n",
      "and\n",
      "breyer\n",
      "and\n",
      "fearon\n",
      "idec\n",
      "and\n",
      "biogen\n",
      "idec\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "quattrone\n",
      "and\n",
      "quattrone\n",
      "and\n",
      "''\n",
      "and\n",
      "qarase\n",
      "fijian\n",
      "qarase\n",
      "buymusic\n",
      "lingle\n",
      "and\n",
      "lingle\n",
      "and\n",
      "and\n",
      "''\n",
      "ebbers\n",
      "'\n",
      "weingarten\n",
      "and\n",
      "''\n",
      "''\n",
      "and\n",
      "desailly\n",
      "desailly\n",
      "and\n",
      "and\n",
      "and\n",
      "khodorkovsky\n",
      "novosibirsk\n",
      "refuelling\n",
      "khodorkovsky\n",
      "novosibirsk\n",
      "umts\n",
      "and\n",
      "umts\n",
      "and\n",
      "and\n",
      "''\n",
      "comstock\n",
      "parseghian\n",
      "parseghian\n",
      "mukhlas\n",
      "farida\n",
      "and\n",
      "''\n",
      "mukhlas\n",
      "''\n",
      "barclay\n",
      "tagg\n",
      "barclay\n",
      "tagg\n",
      "''\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "d'alessandro\n",
      ":\n",
      "and\n",
      "''\n",
      "tonapi\n",
      "and\n",
      "vasant\n",
      "pitke\n",
      "and\n",
      "''\n",
      ":\n",
      "shockey\n",
      ":\n",
      "sumitomo\n",
      "mitsui\n",
      ":\n",
      "anadarko\n",
      "tikrit\n",
      "and\n",
      "baath\n",
      "tikrit\n",
      "and\n",
      "baath\n",
      "miodrag\n",
      "zivkovic\n",
      "montenegro\n",
      "dragan\n",
      "hajdukovic\n",
      "miodrag\n",
      "zivkovic\n",
      "peoplesoft\n",
      "yucaipa\n",
      "dominick\n",
      "yucaipa\n",
      "dominick\n",
      "and\n",
      "villafranca\n",
      "and\n",
      "ratcliffe\n",
      "laredo\n",
      "slover\n",
      "laredo\n",
      "and\n",
      "gromer\n",
      "jeffers\n",
      "and\n",
      "webvpn\n",
      "smarnet\n",
      "hilsenrath\n",
      "and\n",
      "klarman\n",
      "klarman\n",
      "and\n",
      "''\n",
      "and\n",
      "bucklew\n",
      "bucklew\n",
      "waksal\n",
      "and\n",
      "waksal\n",
      "and\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "realnetworks\n",
      "'\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "galvin\n",
      "sunncomm\n",
      "mediamax\n",
      "'advertised\n",
      "'\n",
      "mediamax\n",
      "''\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "lafferty\n",
      "ezzell\n",
      ":\n",
      "''\n",
      "ezzell\n",
      "klarman\n",
      "klarman\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "''\n",
      "''\n",
      "sayliyah\n",
      "sayliyah\n",
      "mcbride\n",
      "fontana\n",
      "fontana\n",
      ":\n",
      "dennehy\n",
      "baylor\n",
      "dennehy\n",
      "baylor\n",
      "mahendradatta\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "lorna\n",
      "schofield\n",
      "toepfer\n",
      "and\n",
      "and\n",
      "and\n",
      ":\n",
      "and\n",
      ":\n",
      "and\n",
      "and\n",
      "and\n",
      "''\n",
      "?\n",
      "?\n",
      "mediaq\n",
      "and\n",
      "mediaq\n",
      "and\n",
      "strasbourg\n",
      "strasbourg\n",
      "and\n",
      ":\n",
      "''\n",
      "riyadh\n",
      "riyadh\n",
      "qaida\n",
      "and\n",
      "and\n",
      "and\n",
      "hrt\n",
      "''\n",
      "''\n",
      "and\n",
      "lagrou\n",
      "and\n",
      "and\n",
      "and\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      "bremer\n",
      "and\n",
      "kissinger\n",
      "and\n",
      "centre\n",
      "centre\n",
      "brendsel\n",
      "and\n",
      "vaughn\n",
      "and\n",
      "and\n",
      "and\n",
      "plmo\n",
      "and\n",
      "psrc\n",
      "palmsource\n",
      ":\n",
      "psrc\n",
      "and\n",
      "''\n",
      "and\n",
      "''\n",
      "‘\n",
      "taikong\n",
      "''\n",
      "taikong\n",
      "and\n",
      "''\n",
      ":\n",
      "kollar\n",
      "kotelly\n",
      "and\n",
      "''\n",
      "''\n",
      "junya\n",
      "tanase\n",
      "junya\n",
      "tanase\n",
      "''\n",
      "and\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "falco\n",
      "and\n",
      "''\n",
      "''\n",
      "qasim\n",
      "''\n",
      "kernan\n",
      "o'bannon\n",
      "kernan\n",
      "o'bannon\n",
      "and\n",
      "fujitsu\n",
      "fujitsu\n",
      "'\n",
      "corixa\n",
      "corixa\n",
      "citicorp\n",
      "and\n",
      "citicorp\n",
      "and\n",
      "and\n",
      "''\n",
      "and\n",
      "and\n",
      "navistar\n",
      "navistar\n",
      "and\n",
      "and\n",
      "cronyn\n",
      "cronyn\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "deirdre\n",
      "hisler\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "and\n",
      "strutt\n",
      "and\n",
      "strutt\n",
      "caracas\n",
      "phoberomys\n",
      "'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caracas\n",
      "mcdonnell\n",
      "mcdonnell\n",
      "and\n",
      "''\n",
      "'\n",
      "and\n",
      "supoyo\n",
      "and\n",
      "''\n",
      ":\n",
      "geraldine\n",
      "geraldine\n",
      "'\n",
      "and\n",
      "hammersmith\n",
      "and\n",
      "''\n",
      "greenberg\n",
      "''\n",
      "greenberg\n",
      "peoplesoft\n",
      "and\n",
      "and\n",
      "peoplesoft\n",
      "''\n",
      "''\n",
      "and\n",
      "''\n",
      "and\n",
      "mcgill\n",
      "mcgill\n",
      "and\n",
      "tiburtina\n",
      "and\n",
      "tiburtina\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "worldcom\n",
      "and\n",
      "kiernan\n",
      "seifert\n",
      "seifert\n",
      "honduras\n",
      ":\n",
      "gazans\n",
      ";\n",
      ";\n",
      "berri\n",
      "congolese\n",
      "congolese\n",
      "pkk\n",
      "schiphol\n",
      "schiphol\n",
      "cmie\n",
      ":\n",
      "damascus\n",
      "homs\n",
      ":\n",
      ":\n",
      "and\n",
      "and\n",
      "aleppo\n",
      "damascus\n",
      ":\n",
      "erdogan\n",
      "erdogan\n",
      "kailai\n",
      "kailai\n",
      "'hiding\n",
      "'\n",
      ":\n",
      "'hiding\n",
      "'\n",
      ":\n",
      ":\n",
      "mursi\n",
      "morsi\n",
      ":\n",
      "davy\n",
      ":\n",
      "davy\n",
      "and\n",
      "bahraini\n",
      "and\n",
      "authentec\n",
      ":\n",
      "authentec\n",
      "dmz\n",
      ":\n",
      ":\n",
      "timor\n",
      "colvin\n",
      "'killed\n",
      "'\n",
      ":\n",
      "hollande\n",
      "francois\n",
      "hollande\n",
      "'arrest\n",
      "'\n",
      "maldives\n",
      "maldives\n",
      "hrw\n",
      ":\n",
      ":\n",
      ";\n",
      "'to\n",
      "and\n",
      "'\n",
      ":\n",
      "'green\n",
      "and\n",
      "'\n",
      "'blasphemy\n",
      "'\n",
      "annan\n",
      "annan\n",
      ":\n",
      "cambodian\n",
      "'\n",
      "cambodia\n",
      "alyokhina\n",
      "alyokhina\n",
      "asiana\n",
      "'left\n",
      "'\n",
      "fertiliser\n",
      "snowden\n",
      "'\n",
      "?\n",
      "beatrix\n",
      "beatrix\n",
      "favour\n",
      "utor\n",
      "'on\n",
      "'\n",
      "sochi\n",
      "morsi\n",
      "morsi\n",
      "morsi\n",
      ":\n",
      "'vile\n",
      "'\n",
      "mandela\n",
      ":\n",
      "mandela\n",
      "algeria\n",
      "algeria\n",
      "'doubles\n",
      "'\n",
      "dhawan\n",
      "dhawan\n",
      "pistorius\n",
      "'\n",
      "reeva\n",
      "steenkamp\n",
      "pistorius\n",
      "'shot\n",
      "steenkamp\n",
      "'\n",
      "waterford\n",
      "wawrinka\n",
      "ankeet\n",
      "chavan\n",
      "ankeet\n",
      "chavan\n",
      "fukushima\n",
      "fukushima\n",
      "'\n",
      "algerian\n",
      "algerian\n",
      "ferrer\n",
      "ferrer\n",
      "nishikori\n",
      "and\n",
      "legalise\n",
      "legalise\n",
      "damascus\n",
      "damascus\n",
      "doris\n",
      "lessing\n",
      "doris\n",
      "lessing\n",
      "morsi\n",
      ":\n",
      "mandela\n",
      "mandela\n",
      "'provocation\n",
      "'\n",
      "malala\n",
      "snowden\n",
      "belarusian\n",
      "sakharov\n",
      "snowden\n",
      "elbaradei\n",
      "elbaradei\n",
      "tamerlan\n",
      "tsarnaev\n",
      "qaida\n",
      ";\n",
      ":\n",
      "shinzo\n",
      ":\n",
      "‚äėpariah\n",
      "state‚äô\n",
      "palumbo\n",
      "pistorius\n",
      ":\n",
      "?\n",
      "'flee\n",
      "'\n",
      "hokianga\n",
      "rouhani\n",
      ":\n",
      "shenzhen\n",
      "shenzhen\n",
      "mandela\n",
      ":\n",
      "guatemalan\n",
      "guatemala\n",
      "'useful\n",
      "'\n",
      ";\n",
      "neymar\n",
      "neymar\n",
      "benghazi\n",
      "benghazi\n",
      "and\n",
      ":\n",
      "ŕä\n",
      "and\n",
      ":\n",
      "videotron\n",
      "videotron\n",
      "'rises\n",
      "'\n",
      "pervez\n",
      "pervez\n",
      "'kills\n",
      "'\n",
      "'may\n",
      "'\n",
      "'fast\n",
      ";\n",
      "'\n",
      ":\n",
      "and\n",
      "luxembourg\n",
      "luxembourg\n",
      ":\n",
      ":\n",
      ":\n",
      "magaluf\n",
      "magaluf\n",
      "waziristan\n",
      "yellen\n",
      "yellen\n",
      ":\n",
      "cii\n",
      "gox\n",
      "'forgotten\n",
      "'\n",
      "bitcoin\n",
      "gox\n",
      "'forgotten\n",
      "'\n",
      "bitcoins\n",
      "fertiliser\n",
      "redford\n",
      "redford\n",
      "and\n",
      "and\n",
      ":\n",
      "abbvie\n",
      "abbvie\n",
      "balochistan\n",
      "afgan\n",
      ":\n",
      ":\n",
      "waba\n",
      "nlc\n",
      "kurd\n",
      "?\n",
      "(1379, 23)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt',encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "   \n",
    "    param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu','tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "    gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,test_cnn,axis=1)\n",
    "    #print(test_data.shape)\n",
    "\n",
    "    test_data= scaler.transform(test_data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiDv1K1Q-AO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7154858484788047\n"
     ]
    }
   ],
   "source": [
    "with open('wv_wwv.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('wv_wwv.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmnlH16n-AO9"
   },
   "source": [
    "# word2vec_wwv_jc_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2eOioqW-AO-"
   },
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data= scaler.transform(data)\n",
    "    \n",
    "    f= open('2017_takelab+cnn/train/sts_train_scores.txt',encoding='utf-8')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    param_grid={'solver': ['adam'],'alpha': 10.0 ** -np.arange(1, 6),\n",
    "            'learning_rate':['constant','adaptive'], 'learning_rate_init': [0.001],\n",
    "            'hidden_layer_sizes': [25,50,100],'activation': ['relu', 'tanh'],\n",
    "            'batch_size': [256],'max_iter':[1000], 'warm_start':[True]}\n",
    "    gs = GridSearchCV(MLPRegressor(),param_grid,scoring=my_scorer,n_jobs=-1,cv=10)\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('2017_takelab+cnn/test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,test_cnn,axis=1)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    test_data= scaler.transform(test_data)\n",
    "\n",
    "    f= open('2017_takelab+cnn/test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    " \n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT6_AGv6-APE"
   },
   "outputs": [],
   "source": [
    "with open('full.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('full.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MquUE3B7-APJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "takelab_word2vec_kl_cnn_code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
