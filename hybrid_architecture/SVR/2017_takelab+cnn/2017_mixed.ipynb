{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"2017_mixed.ipynb","provenance":[],"collapsed_sections":["LTepr7Bc-ANp","IzyJnDpn-AOA","Ev2CYJUR-AOM","TqznzT8l-AOY","icvis7Dd-AOk","9bJvGKmI-AOx","dmnlH16n-AO9"],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"TKfMxbqm-ALz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"status":"ok","timestamp":1590491741379,"user_tz":-330,"elapsed":3586,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"65c03e84-dec0-4956-bfcd-18f18e8139d5"},"source":["import nltk\n","nltk.download('wordnet_ic')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"8UwfEH0Q-ALe","colab_type":"code","colab":{}},"source":["import math\n","from nltk.corpus import wordnet\n","from collections import Counter, defaultdict\n","import sys\n","import re\n","import numpy as np\n","from numpy.linalg import norm\n","from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","from scipy.stats import pearsonr\n","from nltk.corpus import wordnet_ic\n","brown_ic = wordnet_ic.ic('ic-brown.dat')\n","brown_ic = wordnet_ic.ic('ic-brown.dat')\n","semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n","from sklearn.metrics.scorer import make_scorer\n","from sklearn.neural_network import MLPRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LoFUZjUq0vO","colab_type":"code","colab":{}},"source":["import cnn_sts_temp as cn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rdzf44J8-ALp","colab_type":"code","colab":{}},"source":["#import cnn_sts as cn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWjwev8y-AL8","colab_type":"code","colab":{}},"source":["import gensim\n","from sklearn.manifold import TSNE\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import pandas\n","#% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDcSxtL5sI2g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1590491910896,"user_tz":-330,"elapsed":28402,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"80335f16-6060-48b5-9879-e36a83939eed"},"source":["'''from google.colab import drive\n","drive.mount('/content/drive')'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aguKJ21QtL2m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1590491926108,"user_tz":-330,"elapsed":1407,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"d400b870-5cee-47ae-94d4-9a9712683da1"},"source":["#cd 'drive/My Drive'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"To_cwAURtNiK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"status":"ok","timestamp":1590491936679,"user_tz":-330,"elapsed":4661,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"676518f1-e7eb-42a0-9bb6-3d219abf6316"},"source":["#ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34m2017_takelab+cnn\u001b[0m/                    takelab_word2vec_cross.ipynb\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/                    temp.ipynb\n","'Copy of cnn_sts.py'                  \u001b[01;34mthesis\u001b[0m/\n"," Copy_word2vec_kl_cnn_code.ipynb      word2vec_cross_cnn_code.ipynb\n","'Getting started.pdf'                 word2vec_cross_cnn.ipynb\n"," GoogleNews-vectors-negative300.bin   word2vec_cross_epoch.pt\n"," model_cifar.pt                       word2vec_cross_model.pt\n"," sts-dev.csv                          word2vec_kl_epoch.pt\n"," sts-test.csv                         word2vec_kl_model.pt\n"," sts-train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CFkreosu-AMC","colab_type":"code","colab":{}},"source":["eng_vec_path = '../../../GoogleNews-vectors-negative300.bin'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wj5b2nTm-AMJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1590492017671,"user_tz":-330,"elapsed":71724,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"46bded76-5361-430d-8fae-0f19462db754"},"source":["word_vec = gensim.models.KeyedVectors.load_word2vec_format(eng_vec_path, binary=True, unicode_errors='ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1bKLUOEU-AMQ","colab_type":"code","colab":{}},"source":["\n","class Sim:\n","    def __init__(self, words, vectors):\n","        self.word_to_idx = {a: b for b, a in\n","                            enumerate(w.strip() for w in open(words))}\n","        self.mat = np.loadtxt(vectors)\n","\n","    def bow_vec(self, b):\n","        vec = np.zeros(self.mat.shape[1])\n","        for k, v in b.items():\n","            idx = self.word_to_idx.get(k, -1)\n","            if idx >= 0:\n","                vec += self.mat[idx] / (norm(self.mat[idx]) + 1e-8) * v\n","        return vec\n","\n","    def calc(self, b1, b2):\n","        v1 = self.bow_vec(b1)\n","        v2 = self.bow_vec(b2)\n","        return abs(v1.dot(v2) / (norm(v1) + 1e-8) / (norm(v2) + 1e-8))\n","\n","stopwords = set([\n","\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n","\"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \"this\", \"to\",\n","\"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\", \"the\", \"'s\", \"did\",\n","\"have\", \"has\", \"had\", \"were\", \"'ll\"\n","])\n","\n","nyt_sim = Sim('2017_takelab+cnn/nyt_words.txt', '2017_takelab+cnn/nyt_word_vectors.txt')\n","wiki_sim = Sim('2017_takelab+cnn/wikipedia_words.txt', '2017_takelab+cnn/wikipedia_word_vectors.txt')\n","\n","def fix_compounds(a, b):\n","    sb = set(x.lower() for x in b)\n","\n","    a_fix = []\n","    la = len(a)\n","    i = 0\n","    while i < la:\n","        if i + 1 < la:\n","            comb = a[i] + a[i + 1]\n","            if comb.lower() in sb:\n","                a_fix.append(a[i] + a[i + 1])\n","                i += 2\n","                continue\n","        a_fix.append(a[i])\n","        i += 1\n","    return a_fix\n","\n","def load_data(path):\n","    sentences_pos = []\n","    r1 = re.compile(r'\\<([^ ]+)\\>')\n","    r2 = re.compile(r'\\$US(\\d)')\n","    for l in open(path):\n","        #l = l.decode('utf-8')\n","        l = l.replace(u'’', \"'\")\n","        l = l.replace(u'``', '\"')\n","        l = l.replace(u\"''\", '\"')\n","        l = l.replace(u\"—\", '--')\n","        l = l.replace(u\"–\", '--')\n","        l = l.replace(u\"´\", \"'\")\n","        l = l.replace(u\"-\", \" \")\n","        l = l.replace(u\"/\", \" \")\n","        l = r1.sub(r'\\1', l)\n","        l = r2.sub(r'$\\1', l)\n","        s = l.strip().split('\\t')\n","        sa, sb = tuple(nltk.word_tokenize(s)\n","                          for s in l.strip().split('\\t'))\n","        '''sa, sb = ([x.encode('utf-8') for x in sa],\n","                  [x.encode('utf-8') for x in sb])'''\n","\n","        for s in (sa, sb):\n","            for i in range(len(s)):\n","                if s[i] == \"n't\":\n","                    s[i] = \"not\"\n","                elif s[i] == \"'m\":\n","                    s[i] = \"am\"\n","        sa, sb = fix_compounds(sa, sb), fix_compounds(sb, sa)\n","        sentences_pos.append((nltk.pos_tag(sa), nltk.pos_tag(sb)))\n","    return sentences_pos\n","\n","def load_wweight_table(path):\n","    lines = open(path).readlines()\n","    wweight = defaultdict(float)\n","    if not len(lines):\n","        return (wweight, 0.)\n","    totfreq = int(lines[0])\n","    for l in lines[1:]:\n","        w, freq = l.split()\n","        freq = float(freq)\n","        if freq < 10:\n","            continue\n","        wweight[w] = math.log(totfreq / freq)\n","\n","    return wweight\n","\n","wweight = load_wweight_table('2017_takelab+cnn/word-frequencies.txt')\n","minwweight = min(wweight.values())\n","\n","def len_compress(l):\n","    return math.log(1. + l)\n","\n","to_wordnet_tag = {\n","        'NN':wordnet.NOUN,\n","        'JJ':wordnet.ADJ,\n","        'VB':wordnet.VERB,\n","        'RB':wordnet.ADV\n","    }\n","\n","word_matcher = re.compile('[^0-9,.(=)\\[\\]/_`]+$')\n","def is_word(w):\n","    return word_matcher.match(w) is not None\n","\n","def get_locase_words(spos):\n","    return [x[0].lower() for x in spos\n","            if is_word(x[0])]\n","\n","def make_ngrams(l, n):\n","    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n","    rez.append(l[n - 1:])\n","    return list(zip(*rez))\n","\n","def wvec_sim(la,lb):\n","    veca=np.zeros(300)\n","    vecb=np.zeros(300)\n","    m=0\n","    s=1\n","    for i in la:\n","        try:\n","            veca+= word_vec[i]/norm(word_vec[i]+ 1e-8)\n","        except KeyError:\n","            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n","    for i in lb:\n","        try:\n","            vecb+= word_vec[i]/norm(word_vec[i] + 1e-8)\n","        except:\n","            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n","        \n","    return abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n","\n","def weighted_wvec_sim(lca, lcb):\n","    wa = Counter(lca)\n","    wb = Counter(lcb)\n","    wa = {x: wweight[x] * wa[x] for x in wa}\n","    wb = {x: wweight[x] * wb[x] for x in wb}\n","    \n","    veca=np.zeros(300)\n","    vecb=np.zeros(300)\n","    m=0\n","    s=1\n","    \n","    for k,v in wa.items():\n","        #print(k,v)\n","        try:\n","             veca+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n","        except KeyError:\n","            print(k)\n","            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n","\n","    for k,v in wb.items():\n","        #print(k,v)\n","        try:\n","             vecb+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n","        except KeyError:\n","            print(k)\n","            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n","        \n","    return  abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n","\n","def dist_sim(sim, la, lb):\n","    wa = Counter(la)\n","    wb = Counter(lb)\n","    d1 = {x:1 for x in wa}\n","    d2 = {x:1 for x in wb}\n","    return sim.calc(d1, d2)\n","\n","def weighted_dist_sim(sim, lca, lcb):\n","    wa = Counter(lca)\n","    wb = Counter(lcb)\n","    wa = {x: wweight[x] * wa[x] for x in wa}\n","    wb = {x: wweight[x] * wb[x] for x in wb}\n","    return sim.calc(wa, wb)\n","\n","def weighted_word_match(lca, lcb):\n","    wa = Counter(lca)\n","    wb = Counter(lcb)\n","    wsuma = sum(wweight[w] * wa[w] for w in wa)\n","    wsumb = sum(wweight[w] * wb[w] for w in wb)\n","    wsum = 0.\n","\n","    for w in wa:\n","        wd = min(wa[w], wb[w])\n","        wsum += wweight[w] * wd\n","    p = 0.\n","    r = 0.\n","    if wsuma > 0 and wsum > 0:\n","        p = wsum / wsuma\n","    if wsumb > 0 and wsum > 0:\n","        r = wsum / wsumb\n","    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n","    return f1\n","\n","wpathsimcache = {}\n","def wpathsim(a, b):\n","    \n","    if a > b:\n","        b, a = a, b\n","    p = (a, b)\n","    if p in wpathsimcache:\n","        return wpathsimcache[p]\n","    if a == b:\n","        wpathsimcache[p] = 1.\n","        return 1.\n","    sa = wordnet.synsets(a)\n","    sb = wordnet.synsets(b)\n","    '''print(a)\n","    print(sa)\n","    print(b)\n","    print(sb)'''\n","    mx = max([wa.path_similarity(wb)\n","              for wa in sa\n","              for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n","    wpathsimcache[p] = mx\n","    return mx\n","\n","wpathsimcache_jc = {}\n","def wpathsim_jc(a, b):\n","    if a > b:\n","        b, a = a, b\n","    p = (a, b)\n","    if p in wpathsimcache_jc:\n","        return wpathsimcache_jc[p]\n","    if a == b:\n","        wpathsimcache_jc[p] = 1.\n","        return 1.\n","    sa = wordnet.synsets(a)\n","    sb = wordnet.synsets(b)\n","    '''print(a)\n","    print(sa)\n","    print(b)\n","    print(sb)'''\n","    #print(a, b)\n","    mx_list=[]\n","    for wa in sa:\n","        for wb in sb:\n","            if (wa.pos()=='s' or wb.pos()=='s'):\n","                continue\n","            if (wa.pos()=='a' or wb.pos()=='a'):\n","                continue\n","            \n","            if (wa.pos()=='r' or wb.pos()=='r'):\n","                continue\n","            \n","            if (wa.pos() == wb.pos()):\n","                t=wa.jcn_similarity(wb,semcor_ic)\n","                if(t<0.001):\n","                    t1=0.001\n","                    mx_list.append(t1)\n","                elif (t>1):\n","                    t1=1\n","                    mx_list.append(t1)\n","                else:\n","                    mx_list.append(wa.jcn_similarity(wb,semcor_ic))\n","    mx_list=mx_list +[0.0]\n","    mx= max(mx_list)\n","    wpathsimcache_jc[p] = mx\n","    return mx\n","\n","wpathsimcache_lin = {}\n","def wpathsim_lin(a, b):\n","    if a > b:\n","        b, a = a, b\n","    p = (a, b)\n","    if p in wpathsimcache_lin:\n","        return wpathsimcache_lin[p]\n","    if a == b:\n","        wpathsimcache_lin[p] = 1.\n","        return 1.\n","    sa = wordnet.synsets(a)\n","    sb = wordnet.synsets(b)\n","    '''print(a)\n","    print(sa)\n","    print(b)\n","    print(sb)'''\n","    #print(a, b)\n","    mx_list=[]\n","    for wa in sa:\n","        for wb in sb:\n","            if (wa.pos()=='s' or wb.pos()=='s'):\n","                continue\n","            if (wa.pos()=='a' or wb.pos()=='a'):\n","                continue\n","            \n","            if (wa.pos()=='r' or wb.pos()=='r'):\n","                continue\n","            \n","            if (wa.pos() == wb.pos()):\n","                t=wa.jcn_similarity(wb,semcor_ic)\n","                if(t<0.001):\n","                    t1=0.001\n","                    mx_list.append(t1)\n","                elif (t>1):\n","                    t1=1\n","                    mx_list.append(t1)\n","                else:\n","                    mx_list.append(wa.lin_similarity(wb,semcor_ic))\n","    mx_list=mx_list +[0.0]\n","    mx= max(mx_list)\n","    wpathsimcache_lin[p] = mx\n","    return mx\n","\n","def calc_wn_prec(lema, lemb,val):\n","    rez = 0.\n","    for a in lema:\n","        ms = 0.\n","        for b in lemb:\n","            if(val==0):\n","                ms = max(ms, wpathsim(a, b))\n","            elif(val==1):\n","                ms = max(ms, wpathsim_jc(a, b))\n","            else:\n","                ms = max(ms, wpathsim_lin(a, b))\n","                \n","        rez += ms\n","    return rez / len(lema)\n","\n","def wn_sim_match(lema, lemb,val):\n","    f1 = 1.\n","    p = 0.\n","    r = 0.\n","    if len(lema) > 0 and len(lemb) > 0:\n","        p = calc_wn_prec(lema, lemb,val)\n","        r = calc_wn_prec(lemb, lema,val)\n","        f1 = 2. * p * r / (p + r) if p + r > 0 else 0.\n","    return f1\n","\n","def ngram_match(sa, sb, n):\n","    nga = make_ngrams(sa, n)\n","    ngb = make_ngrams(sb, n)\n","    matches = 0\n","    c1 = Counter(nga)\n","    for ng in ngb:\n","        if c1[ng] > 0:\n","            c1[ng] -= 1\n","            matches += 1\n","    p = 0.\n","    r = 0.\n","    f1 = 1.\n","    if len(nga) > 0 and len(ngb) > 0:\n","        p = matches / float(len(nga))\n","        r = matches / float(len(ngb))\n","        f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n","    return f1\n","\n","def get_lemmatized_words(sa):\n","    rez = []\n","    for w, wpos in sa:\n","        w = w.lower()\n","        if w in stopwords or not is_word(w):\n","            continue\n","        wtag = to_wordnet_tag.get(wpos[:2])\n","        if wtag is None:\n","            wlem = w\n","        else:\n","            wlem = wordnet.morphy(w, wtag) or w\n","        rez.append(wlem)\n","    return rez\n","\n","def is_stock_tick(w):\n","    return w[0] == '.' and len(w) > 1 and w[1:].isupper()\n","\n","def stocks_matches(sa, sb):\n","    ca = set(x[0] for x in sa if is_stock_tick(x[0]))\n","    cb = set(x[0] for x in sb if is_stock_tick(x[0]))\n","    isect = len(ca.intersection(cb))\n","    la = len(ca)\n","    lb = len(cb)\n","\n","    f = 1.\n","    if la > 0 and lb > 0:\n","        if isect > 0:\n","            p = float(isect) / la\n","            r = float(isect) / lb\n","            f = 2 * p * r / (p + r)\n","        else:\n","            f = 0.\n","    return (len_compress(la + lb), f)\n","\n","def case_matches(sa, sb):\n","    ca = set(x[0] for x in sa[1:] if x[0][0].isupper()\n","            and x[0][-1] != '.')\n","    cb = set(x[0] for x in sb[1:] if x[0][0].isupper()\n","            and x[0][-1] != '.')\n","    la = len(ca)\n","    lb = len(cb)\n","    isect = len(ca.intersection(cb))\n","\n","    f = 1.\n","    if la > 0 and lb > 0:\n","        if isect > 0:\n","            p = float(isect) / la\n","            r = float(isect) / lb\n","            f = 2 * p * r / (p + r)\n","        else:\n","            f = 0.\n","    return (len_compress(la + lb), f)\n","\n","risnum = re.compile(r'^[0-9,./-]+$')\n","rhasdigit = re.compile(r'[0-9]')\n","\n","def match_number(xa, xb):\n","    if xa == xb:\n","        return True\n","    xa = xa.replace(',', '')\n","    xb = xb.replace(',', '')\n","\n","    try:\n","        va = int(float(xa))\n","        vb = int(float(xb))\n","        if (va == 0 or vb == 0) and va != vb:\n","            return False\n","        fxa = float(xa)\n","        fxb = float(xb)\n","        if abs(fxa - fxb) > 1:\n","            return False\n","        diga = xa.find('.')\n","        digb = xb.find('.')\n","        diga = 0 if diga == -1 else len(xa) - diga - 1\n","        digb = 0 if digb == -1 else len(xb) - digb - 1\n","        if diga > 0 and digb > 0 and va != vb:\n","            return False\n","        dmin = min(diga, digb)\n","        if dmin == 0:\n","            if abs(round(fxa, 0) - round(fxb, 0)) < 1e-5:\n","                return True\n","            return va == vb\n","        return abs(round(fxa, dmin) - round(fxb, dmin)) < 1e-5\n","    except:\n","        pass\n","\n","    return False\n","\n","def number_features(sa, sb):\n","    numa = set(x[0] for x in sa if risnum.match(x[0]) and\n","            rhasdigit.match(x[0]))\n","    numb = set(x[0] for x in sb if risnum.match(x[0]) and\n","            rhasdigit.match(x[0]))\n","    isect = 0\n","    for na in numa:\n","        if na in numb:\n","            isect += 1\n","            continue\n","        for nb in numb:\n","            if match_number(na, nb):\n","                isect += 1\n","                break\n","\n","    la, lb = len(numa), len(numb)\n","\n","    f = 1.\n","    subset = 0.\n","    if la + lb > 0:\n","        if isect == la or isect == lb:\n","            subset = 1.\n","        if isect > 0:\n","            p = float(isect) / la\n","            r = float(isect) / lb\n","            f = 2. * p * r / (p + r)\n","        else:\n","            f = 0.\n","    return (len_compress(la + lb), f, subset)\n","\n","def relative_len_difference(lca, lcb):\n","    la, lb = len(lca), len(lcb)\n","    return abs(la - lb) / float(max(la, lb) + 1e-5)\n","\n","def relative_ic_difference(lca, lcb):\n","    #wa = sum(wweight[x] for x in lca)\n","    #wb = sum(wweight[x] for x in lcb)\n","    wa = sum(max(0., wweight[x] - minwweight) for x in lca)\n","    wb = sum(max(0., wweight[x] - minwweight) for x in lcb)\n","    return abs(wa - wb) / (max(wa, wb) + 1e-5)\n","\n","def my_custom_function(y_true, y_predict):\n","    corr,_ = pearsonr(y_true, y_predict)\n","    return corr\n","\n","    \n"," #   gs = GridSearchCV(clf, param_grid={'kernel': [ 'rbf'],'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='roc_auc')\n","    \n","    \n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQ_guUmm-AMZ","colab_type":"text"},"source":["## Incorporating CNN"]},{"cell_type":"code","metadata":{"id":"MJ7PwTXp-AMb","colab_type":"code","colab":{}},"source":["c = dict()\n","c['num_runs']   = 3\n","c['num_epochs'] = 64\n","c['num_batchs'] = 2\n","c['batch_size'] = 3000\n","c['wordvectdim']  = 300\n","c['sentencepad']  = 60\n","c['num_classes']  = 6\n","c['cnnfilters']     = {1: 1800}\n","c['cnninitial']     = 'he_uniform'\n","c['cnnactivate']    = 'relu'\n","c['densedimension'] = list([1800])\n","c['denseinitial']   = 'he_uniform'\n","c['denseactivate']  = 'tanh'\n","c['optimizer']  = 'adam'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0XEix8L-AMk","colab_type":"code","colab":{}},"source":["tsk= cn.STSTask(c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfGKI_H8-AMq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1590492488950,"user_tz":-330,"elapsed":1470,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"36c8745d-c7fa-43a6-a19c-15ff783e1374"},"source":["tsk.create_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["inside convo: \n","(None, 60, 300)\n","(None, 60, 300)\n","(None, 60, 1800)\n","(None, 60, 1800)\n","inside max\n","(None, 1, 1800)\n","(None, 1, 1800)\n","inside flat\n","(None, 1800)\n","(None, 1800)\n","(None, 1800)\n","(None, 3600)\n","inside dense\n","(None, 1800)\n","kd\n","(None, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CiNnxVI0-AMw","colab_type":"code","colab":{},"outputId":"837bbed8-49a8-450b-f6a5-00270a912ba9"},"source":["tsk.load_resc('glove.840B.300d.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading GloVe...(This might take one or two minutes.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nDflvIXp-AM3","colab_type":"code","colab":{}},"source":["tsk.load_data('sts-train.csv', 'sts-dev.csv', 'sts-test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YG5txnpD-AM_","colab_type":"code","colab":{},"outputId":"e2986cdb-2fd0-49f1-8718-5148e3ce3f51"},"source":["tsk.model.load_weights('weightfile0')\n","l=tsk.eval_model1()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["counter 1\n","sl slice(0, 1379, None)\n","(1379,)\n","[Test]=[0.7762]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N1-Z0kB8-ANL","colab_type":"code","colab":{}},"source":["temp= l[1].reshape(1379,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmPAjdd2-ANR","colab_type":"code","colab":{},"outputId":"e515d044-fb3c-4e4c-9441-717f49518443"},"source":["tsk.eval_model2()[1].shape"],"execution_count":null,"outputs":[{"output_type":"stream","text":["counter 1\n","sl slice(0, 5749, None)\n","[Test]=[0.9398]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(5749,)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"wCPOQEQF-ANW","colab_type":"code","colab":{},"outputId":"5abeb9b9-abd1-4af5-c0bd-7fcbc6e457f4"},"source":["train_cnn=tsk.eval_model2()[1].reshape(5749,1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["counter 1\n","sl slice(0, 5749, None)\n","[Test]=[0.9398]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DJzndUsR-ANc","colab_type":"code","colab":{},"outputId":"5c811803-eb42-420d-e26c-4dd3b7a6dd1b"},"source":["train_cnn.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5749, 1)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"nnUq0PcU-ANk","colab_type":"code","colab":{}},"source":["def post_process(path):\n","    import sys\n","\n","\n","    orig = []\n","    for l in open('test/sts_test.txt'):\n","        orig.append([x.strip() for x in l.lower().split(\"\\t\")])\n","\n","    scores = list(map(float, open(path).readlines()))\n","\n","    if len(orig) != len(scores):\n","        print (sys.stderr, \"Error: inputs should have the same number of lines\")\n","        exit(1)\n","    processed=[]\n","    f = open('processed_'+path, 'w')\n","    for i, s in enumerate(scores):\n","        if orig[0] == orig[1]:\n","            s = 5.\n","        if s > 5:\n","            s = 5.\n","        if s < 0:\n","            s = 0.\n","        processed.append(s)\n","        #print (s)\n","        f.write(\"%s\\n\" % s)\n","    f.close()\n","    return processed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTepr7Bc-ANp","colab_type":"text"},"source":["## takelab_score"]},{"cell_type":"code","metadata":{"id":"cKiIYR5z-ANr","colab_type":"code","colab":{},"outputId":"42878511-93a2-41e1-cef8-94e8613e0766"},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,0),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            #wvec_sim(lema,lemb),\n","            #weighted_wvec_sim(lema,lemb)\n","            dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            weighted_dist_sim(nyt_sim, lema, lemb),\n","            weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)\n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n","    \n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    \n","    '''gs = GridSearchCV(SVR(kernel='rbf',), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape)\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    '''test_data=[]\n","    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n","        y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    \n","    f= open('trial1/STS.gs.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)'''\n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1379, 22)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W8MU79kx-ANw","colab_type":"code","colab":{},"outputId":"43c4435a-3ab4-4551-bbce-cd2437bb8674"},"source":["test_data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1379, 22)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"G9f_QmXY-AN1","colab_type":"code","colab":{},"outputId":"5cf9db0f-a265-40b9-8c13-f2ad1301db9a"},"source":["data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5749, 22)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"VQN1GHVW-AN7","colab_type":"code","colab":{},"outputId":"c2a7bd66-ca96-41d9-97e4-56e7ad148b01"},"source":["with open('takelab_output_MSRpar.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","model_pred=np.array(post_process('takelab_output_MSRpar.txt'))\n","\n","\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7870603748644066\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IzyJnDpn-AOA","colab_type":"text"},"source":["## Using JC Similarity"]},{"cell_type":"code","metadata":{"id":"F0XPMGB8-AOB","colab_type":"code","colab":{},"outputId":"d814fb70-41c4-41c5-8489-09e38ee583a2"},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,1),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            #wvec_sim(lema,lemb),\n","            #weighted_wvec_sim(lema,lemb)\n","            dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            weighted_dist_sim(nyt_sim, lema, lemb),\n","            weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape)\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    '''test_data=[]\n","    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n","        y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    \n","    f= open('trial1/STS.gs.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)'''\n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1379, 22)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dxxR7rSh-AOH","colab_type":"code","colab":{},"outputId":"fece7896-0635-46b1-abbd-5f0c6d4ea16a"},"source":["with open('takelab_jc.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","\n","model_pred=np.array(post_process('takelab_jc.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7872532529776621\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ev2CYJUR-AOM","colab_type":"text"},"source":["## Using Lin Similarity"]},{"cell_type":"code","metadata":{"id":"ls5xsIId-AON","colab_type":"code","colab":{},"outputId":"6b72f989-a2d4-4836-cbf5-439ce28c8bfe"},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,2),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            #wvec_sim(lema,lemb),\n","            #weighted_wvec_sim(lema,lemb)\n","            dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            weighted_dist_sim(nyt_sim, lema, lemb),\n","            weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    \n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    '''test_data=[]\n","    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n","        y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    \n","    f= open('trial1/STS.gs.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)'''\n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-30-b69c90f6ff28>, line 72)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-b69c90f6ff28>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    f= open('test/sts_test_scores.txt')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"0TmJLHMg-AOT","colab_type":"code","colab":{}},"source":["with open('takelab_lin.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","model_pred=np.array(post_process('takelab_lin.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqznzT8l-AOY","colab_type":"text"},"source":["## word2vec with weighted_LSA_vec \n"]},{"cell_type":"code","metadata":{"id":"ZI-2hr_W-AOZ","colab_type":"code","colab":{}},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,0),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            wvec_sim(lema,lemb),\n","            #weighted_wvec_sim(lema,lemb)\n","            #dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            weighted_dist_sim(nyt_sim, lema, lemb),\n","            weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    \n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    '''test_data=[]\n","    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n","        y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    \n","    f= open('trial1/STS.gs.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)'''\n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssbdsRbR-AOe","colab_type":"code","colab":{}},"source":["with open('wv_wlsa.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","model_pred=np.array(post_process('wv_wlsa.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"icvis7Dd-AOk","colab_type":"text"},"source":["## JC AND LIN Together"]},{"cell_type":"code","metadata":{"id":"Ocqjz_Eh-AOl","colab_type":"code","colab":{}},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,1),\n","            wn_sim_match(lema, lemb,2),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            #wvec_sim(lema,lemb),\n","            #weighted_wvec_sim(lema,lemb),\n","            dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            #weighted_dist_sim(nyt_sim, lema, lemb),\n","            #weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    \n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    '''test_data=[]\n","    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n","        y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    \n","    f= open('trial1/STS.gs.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)'''\n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIrGBG4Y-AOs","colab_type":"code","colab":{}},"source":["with open('jc_lin.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","\n","model_pred=np.array(post_process('jc_lin.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9bJvGKmI-AOx","colab_type":"text"},"source":["## word2vec with weighted_word2vec"]},{"cell_type":"code","metadata":{"id":"B4Ccvhp6-AOy","colab_type":"code","colab":{}},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,0),\n","            wn_sim_match(lema, lemb,1),\n","            wn_sim_match(lema, lemb,2),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            wvec_sim(lema,lemb),\n","            weighted_wvec_sim(lema,lemb),\n","            #dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            #weighted_dist_sim(nyt_sim, lema, lemb),\n","            #weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n","   \n","    \n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)\n","    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10) \n","  \n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiDv1K1Q-AO3","colab_type":"code","colab":{}},"source":["with open('wv_wwv.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","\n","model_pred=np.array(post_process('wv_wwv.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmnlH16n-AO9","colab_type":"text"},"source":["## word2vec_wwv_jc_lin"]},{"cell_type":"code","metadata":{"id":"d2eOioqW-AO-","colab_type":"code","colab":{}},"source":["def calc_features(sa, sb):\n","    olca = get_locase_words(sa)\n","    olcb = get_locase_words(sb)\n","    lca = [w for w in olca if w not in stopwords]\n","    lcb = [w for w in olcb if w not in stopwords]\n","    lema = get_lemmatized_words(sa)\n","    lemb = get_lemmatized_words(sb)\n","   \n","    f = []\n","    f += number_features(sa, sb)\n","    f += case_matches(sa, sb)\n","    f += stocks_matches(sa, sb)\n","   \n","    f += [\n","            ngram_match(lca, lcb, 1),\n","            ngram_match(lca, lcb, 2),\n","            ngram_match(lca, lcb, 3),\n","            ngram_match(lema, lemb, 1),\n","            ngram_match(lema, lemb, 2),\n","            ngram_match(lema, lemb, 3),\n","            wn_sim_match(lema, lemb,1),\n","            wn_sim_match(lema, lemb,2),\n","            weighted_word_match(olca, olcb),\n","            weighted_word_match(lema, lemb),\n","            wvec_sim(lema,lemb),\n","            weighted_wvec_sim(lema,lemb),\n","            #dist_sim(nyt_sim, lema, lemb),\n","            #dist_sim(wiki_sim, lema, lemb),\n","            #weighted_dist_sim(nyt_sim, lema, lemb),\n","            #weighted_dist_sim(wiki_sim, lema, lemb),\n","            relative_len_difference(lca, lcb),\n","            relative_ic_difference(olca, olcb)\n","        ]\n","\n","    return f\n","\n","if __name__ == \"__main__\":\n","    \n","   \n","\n","    '''scores = None\n","    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n"," \n","    data=[]\n","    c=0\n","    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        data+=[(calc_features(*sp))]\n","        \n","    data=np.array(data)   \n","    data= np.append(data, train_cnn, axis=1)\n","    \n","    f= open('train/sts_train_scores.txt')\n","    label= f.readlines()\n","    label=[float(w.strip()) for w in label]\n","    label=np.array(label).reshape(len(label),)\n","    \n","    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)\n","    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n","    \n","    gs.fit(data,label)\n","    m=gs.best_estimator_\n","    model=m.fit(data,label)\n","    \n","    \n","    test_data=[]\n","    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n","        #y = 0. if scores is None else scores[idx]\n","        test_data+=[(calc_features(*sp))]\n","        \n","    test_data=np.array(test_data)\n","    test_data=np.append(test_data,temp,axis=1)\n","    print(test_data.shape\n","    \n","    f= open('test/sts_test_scores.txt')\n","    test_label= f.readlines()\n","    test_label=[float(w.strip()) for w in test_label]\n","    test_label=np.array(test_label).reshape(len(test_label),)\n","    \n"," \n","    \n","    predictions= model.predict(test_data)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HT6_AGv6-APE","colab_type":"code","colab":{}},"source":["with open('full.txt', 'w') as f:\n","        for item in predictions:\n","            f.write(\"%s\\n\" % item)\n","f.close()\n","model_pred=np.array(post_process('full.txt'))\n","corr,_ = pearsonr(test_label, model_pred)\n","print (corr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MquUE3B7-APJ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}