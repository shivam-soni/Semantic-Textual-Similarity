{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn_sts as cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/shivam/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/shivam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shivam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vec_path = '../../../../../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = gensim.models.KeyedVectors.load_word2vec_format(eng_vec_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sim:\n",
    "    def __init__(self, words, vectors):\n",
    "        self.word_to_idx = {a: b for b, a in\n",
    "                            enumerate(w.strip() for w in open(words))}\n",
    "        self.mat = np.loadtxt(vectors)\n",
    "\n",
    "    def bow_vec(self, b):\n",
    "        vec = np.zeros(self.mat.shape[1])\n",
    "        for k, v in b.items():\n",
    "            idx = self.word_to_idx.get(k, -1)\n",
    "            if idx >= 0:\n",
    "                vec += self.mat[idx] / (norm(self.mat[idx]) + 1e-8) * v\n",
    "        return vec\n",
    "\n",
    "    def calc(self, b1, b2):\n",
    "        v1 = self.bow_vec(b1)\n",
    "        v2 = self.bow_vec(b2)\n",
    "        return abs(v1.dot(v2) / (norm(v1) + 1e-8) / (norm(v2) + 1e-8))\n",
    "\n",
    "stopwords = set([\n",
    "\"i\", \"a\", \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n",
    "\"how\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\", \"the\", \"this\", \"to\",\n",
    "\"was\", \"what\", \"when\", \"where\", \"who\", \"will\", \"with\", \"the\", \"'s\", \"did\",\n",
    "\"have\", \"has\", \"had\", \"were\", \"'ll\"\n",
    "])\n",
    "\n",
    "nyt_sim = Sim('nyt_words.txt', 'nyt_word_vectors.txt')\n",
    "wiki_sim = Sim('wikipedia_words.txt', 'wikipedia_word_vectors.txt')\n",
    "\n",
    "def fix_compounds(a, b):\n",
    "    sb = set(x.lower() for x in b)\n",
    "\n",
    "    a_fix = []\n",
    "    la = len(a)\n",
    "    i = 0\n",
    "    while i < la:\n",
    "        if i + 1 < la:\n",
    "            comb = a[i] + a[i + 1]\n",
    "            if comb.lower() in sb:\n",
    "                a_fix.append(a[i] + a[i + 1])\n",
    "                i += 2\n",
    "                continue\n",
    "        a_fix.append(a[i])\n",
    "        i += 1\n",
    "    return a_fix\n",
    "\n",
    "def load_data(path):\n",
    "    sentences_pos = []\n",
    "    r1 = re.compile(r'\\<([^ ]+)\\>')\n",
    "    r2 = re.compile(r'\\$US(\\d)')\n",
    "    for l in open(path):\n",
    "        #l = l.decode('utf-8')\n",
    "        l = l.replace(u'’', \"'\")\n",
    "        l = l.replace(u'``', '\"')\n",
    "        l = l.replace(u\"''\", '\"')\n",
    "        l = l.replace(u\"—\", '--')\n",
    "        l = l.replace(u\"–\", '--')\n",
    "        l = l.replace(u\"´\", \"'\")\n",
    "        l = l.replace(u\"-\", \" \")\n",
    "        l = l.replace(u\"/\", \" \")\n",
    "        l = r1.sub(r'\\1', l)\n",
    "        l = r2.sub(r'$\\1', l)\n",
    "        s = l.strip().split('\\t')\n",
    "        sa, sb = tuple(nltk.word_tokenize(s)\n",
    "                          for s in l.strip().split('\\t'))\n",
    "        '''sa, sb = ([x.encode('utf-8') for x in sa],\n",
    "                  [x.encode('utf-8') for x in sb])'''\n",
    "\n",
    "        for s in (sa, sb):\n",
    "            for i in range(len(s)):\n",
    "                if s[i] == \"n't\":\n",
    "                    s[i] = \"not\"\n",
    "                elif s[i] == \"'m\":\n",
    "                    s[i] = \"am\"\n",
    "        sa, sb = fix_compounds(sa, sb), fix_compounds(sb, sa)\n",
    "        sentences_pos.append((nltk.pos_tag(sa), nltk.pos_tag(sb)))\n",
    "    return sentences_pos\n",
    "\n",
    "def load_wweight_table(path):\n",
    "    lines = open(path).readlines()\n",
    "    wweight = defaultdict(float)\n",
    "    if not len(lines):\n",
    "        return (wweight, 0.)\n",
    "    totfreq = int(lines[0])\n",
    "    for l in lines[1:]:\n",
    "        w, freq = l.split()\n",
    "        freq = float(freq)\n",
    "        if freq < 10:\n",
    "            continue\n",
    "        wweight[w] = math.log(totfreq / freq)\n",
    "\n",
    "    return wweight\n",
    "\n",
    "wweight = load_wweight_table('word-frequencies.txt')\n",
    "minwweight = min(wweight.values())\n",
    "\n",
    "def len_compress(l):\n",
    "    return math.log(1. + l)\n",
    "\n",
    "to_wordnet_tag = {\n",
    "        'NN':wordnet.NOUN,\n",
    "        'JJ':wordnet.ADJ,\n",
    "        'VB':wordnet.VERB,\n",
    "        'RB':wordnet.ADV\n",
    "    }\n",
    "\n",
    "word_matcher = re.compile('[^0-9,.(=)\\[\\]/_`]+$')\n",
    "def is_word(w):\n",
    "    return word_matcher.match(w) is not None\n",
    "\n",
    "def get_locase_words(spos):\n",
    "    return [x[0].lower() for x in spos\n",
    "            if is_word(x[0])]\n",
    "\n",
    "def make_ngrams(l, n):\n",
    "    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n",
    "    rez.append(l[n - 1:])\n",
    "    return list(zip(*rez))\n",
    "\n",
    "def wvec_sim(la,lb):\n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    for i in la:\n",
    "        try:\n",
    "            veca+= word_vec[i]/norm(word_vec[i]+ 1e-8)\n",
    "        except KeyError:\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "    for i in lb:\n",
    "        try:\n",
    "            vecb+= word_vec[i]/norm(word_vec[i] + 1e-8)\n",
    "        except:\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n",
    "        \n",
    "    return abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def weighted_wvec_sim(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    \n",
    "    veca=np.zeros(300)\n",
    "    vecb=np.zeros(300)\n",
    "    m=0\n",
    "    s=1\n",
    "    \n",
    "    for k,v in wa.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             veca+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            veca+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "\n",
    "    for k,v in wb.items():\n",
    "        #print(k,v)\n",
    "        try:\n",
    "             vecb+= word_vec[k]/norm(word_vec[k]+ 1e-8)*v\n",
    "        except KeyError:\n",
    "            print(k)\n",
    "            vecb+= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)*v\n",
    "        \n",
    "    return  abs(veca.dot(vecb) / (norm(veca) + 1e-8 ) / (norm(vecb) + 1e-8))\n",
    "\n",
    "def dist_sim(sim, la, lb):\n",
    "    wa = Counter(la)\n",
    "    wb = Counter(lb)\n",
    "    d1 = {x:1 for x in wa}\n",
    "    d2 = {x:1 for x in wb}\n",
    "    return sim.calc(d1, d2)\n",
    "\n",
    "def weighted_dist_sim(sim, lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wa = {x: wweight[x] * wa[x] for x in wa}\n",
    "    wb = {x: wweight[x] * wb[x] for x in wb}\n",
    "    return sim.calc(wa, wb)\n",
    "\n",
    "def weighted_word_match(lca, lcb):\n",
    "    wa = Counter(lca)\n",
    "    wb = Counter(lcb)\n",
    "    wsuma = sum(wweight[w] * wa[w] for w in wa)\n",
    "    wsumb = sum(wweight[w] * wb[w] for w in wb)\n",
    "    wsum = 0.\n",
    "\n",
    "    for w in wa:\n",
    "        wd = min(wa[w], wb[w])\n",
    "        wsum += wweight[w] * wd\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if wsuma > 0 and wsum > 0:\n",
    "        p = wsum / wsuma\n",
    "    if wsumb > 0 and wsum > 0:\n",
    "        r = wsum / wsumb\n",
    "    f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "wpathsimcache = {}\n",
    "def wpathsim(a, b):\n",
    "    \n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache:\n",
    "        return wpathsimcache[p]\n",
    "    if a == b:\n",
    "        wpathsimcache[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    mx = max([wa.path_similarity(wb)\n",
    "              for wa in sa\n",
    "              for wb in sb if wa.path_similarity(wb) is not None ] + [0.]  )\n",
    "    wpathsimcache[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_jc = {}\n",
    "def wpathsim_jc(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_jc:\n",
    "        return wpathsimcache_jc[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_jc[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.jcn_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_jc[p] = mx\n",
    "    return mx\n",
    "\n",
    "wpathsimcache_lin = {}\n",
    "def wpathsim_lin(a, b):\n",
    "    if a > b:\n",
    "        b, a = a, b\n",
    "    p = (a, b)\n",
    "    if p in wpathsimcache_lin:\n",
    "        return wpathsimcache_lin[p]\n",
    "    if a == b:\n",
    "        wpathsimcache_lin[p] = 1.\n",
    "        return 1.\n",
    "    sa = wordnet.synsets(a)\n",
    "    sb = wordnet.synsets(b)\n",
    "    '''print(a)\n",
    "    print(sa)\n",
    "    print(b)\n",
    "    print(sb)'''\n",
    "    #print(a, b)\n",
    "    mx_list=[]\n",
    "    for wa in sa:\n",
    "        for wb in sb:\n",
    "            if (wa.pos()=='s' or wb.pos()=='s'):\n",
    "                continue\n",
    "            if (wa.pos()=='a' or wb.pos()=='a'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos()=='r' or wb.pos()=='r'):\n",
    "                continue\n",
    "            \n",
    "            if (wa.pos() == wb.pos()):\n",
    "                t=wa.jcn_similarity(wb,semcor_ic)\n",
    "                if(t<0.001):\n",
    "                    t1=0.001\n",
    "                    mx_list.append(t1)\n",
    "                elif (t>1):\n",
    "                    t1=1\n",
    "                    mx_list.append(t1)\n",
    "                else:\n",
    "                    mx_list.append(wa.lin_similarity(wb,semcor_ic))\n",
    "    mx_list=mx_list +[0.0]\n",
    "    mx= max(mx_list)\n",
    "    wpathsimcache_lin[p] = mx\n",
    "    return mx\n",
    "\n",
    "def calc_wn_prec(lema, lemb,val):\n",
    "    rez = 0.\n",
    "    for a in lema:\n",
    "        ms = 0.\n",
    "        for b in lemb:\n",
    "            if(val==0):\n",
    "                ms = max(ms, wpathsim(a, b))\n",
    "            elif(val==1):\n",
    "                ms = max(ms, wpathsim_jc(a, b))\n",
    "            else:\n",
    "                ms = max(ms, wpathsim_lin(a, b))\n",
    "                \n",
    "        rez += ms\n",
    "    return rez / len(lema)\n",
    "\n",
    "def wn_sim_match(lema, lemb,val):\n",
    "    f1 = 1.\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    if len(lema) > 0 and len(lemb) > 0:\n",
    "        p = calc_wn_prec(lema, lemb,val)\n",
    "        r = calc_wn_prec(lemb, lema,val)\n",
    "        f1 = 2. * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def ngram_match(sa, sb, n):\n",
    "    nga = make_ngrams(sa, n)\n",
    "    ngb = make_ngrams(sb, n)\n",
    "    matches = 0\n",
    "    c1 = Counter(nga)\n",
    "    for ng in ngb:\n",
    "        if c1[ng] > 0:\n",
    "            c1[ng] -= 1\n",
    "            matches += 1\n",
    "    p = 0.\n",
    "    r = 0.\n",
    "    f1 = 1.\n",
    "    if len(nga) > 0 and len(ngb) > 0:\n",
    "        p = matches / float(len(nga))\n",
    "        r = matches / float(len(ngb))\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0.\n",
    "    return f1\n",
    "\n",
    "def get_lemmatized_words(sa):\n",
    "    rez = []\n",
    "    for w, wpos in sa:\n",
    "        w = w.lower()\n",
    "        if w in stopwords or not is_word(w):\n",
    "            continue\n",
    "        wtag = to_wordnet_tag.get(wpos[:2])\n",
    "        if wtag is None:\n",
    "            wlem = w\n",
    "        else:\n",
    "            wlem = wordnet.morphy(w, wtag) or w\n",
    "        rez.append(wlem)\n",
    "    return rez\n",
    "\n",
    "def is_stock_tick(w):\n",
    "    return w[0] == '.' and len(w) > 1 and w[1:].isupper()\n",
    "\n",
    "def stocks_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa if is_stock_tick(x[0]))\n",
    "    cb = set(x[0] for x in sb if is_stock_tick(x[0]))\n",
    "    isect = len(ca.intersection(cb))\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "def case_matches(sa, sb):\n",
    "    ca = set(x[0] for x in sa[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    cb = set(x[0] for x in sb[1:] if x[0][0].isupper()\n",
    "            and x[0][-1] != '.')\n",
    "    la = len(ca)\n",
    "    lb = len(cb)\n",
    "    isect = len(ca.intersection(cb))\n",
    "\n",
    "    f = 1.\n",
    "    if la > 0 and lb > 0:\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f)\n",
    "\n",
    "risnum = re.compile(r'^[0-9,./-]+$')\n",
    "rhasdigit = re.compile(r'[0-9]')\n",
    "\n",
    "def match_number(xa, xb):\n",
    "    if xa == xb:\n",
    "        return True\n",
    "    xa = xa.replace(',', '')\n",
    "    xb = xb.replace(',', '')\n",
    "\n",
    "    try:\n",
    "        va = int(float(xa))\n",
    "        vb = int(float(xb))\n",
    "        if (va == 0 or vb == 0) and va != vb:\n",
    "            return False\n",
    "        fxa = float(xa)\n",
    "        fxb = float(xb)\n",
    "        if abs(fxa - fxb) > 1:\n",
    "            return False\n",
    "        diga = xa.find('.')\n",
    "        digb = xb.find('.')\n",
    "        diga = 0 if diga == -1 else len(xa) - diga - 1\n",
    "        digb = 0 if digb == -1 else len(xb) - digb - 1\n",
    "        if diga > 0 and digb > 0 and va != vb:\n",
    "            return False\n",
    "        dmin = min(diga, digb)\n",
    "        if dmin == 0:\n",
    "            if abs(round(fxa, 0) - round(fxb, 0)) < 1e-5:\n",
    "                return True\n",
    "            return va == vb\n",
    "        return abs(round(fxa, dmin) - round(fxb, dmin)) < 1e-5\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "def number_features(sa, sb):\n",
    "    numa = set(x[0] for x in sa if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    numb = set(x[0] for x in sb if risnum.match(x[0]) and\n",
    "            rhasdigit.match(x[0]))\n",
    "    isect = 0\n",
    "    for na in numa:\n",
    "        if na in numb:\n",
    "            isect += 1\n",
    "            continue\n",
    "        for nb in numb:\n",
    "            if match_number(na, nb):\n",
    "                isect += 1\n",
    "                break\n",
    "\n",
    "    la, lb = len(numa), len(numb)\n",
    "\n",
    "    f = 1.\n",
    "    subset = 0.\n",
    "    if la + lb > 0:\n",
    "        if isect == la or isect == lb:\n",
    "            subset = 1.\n",
    "        if isect > 0:\n",
    "            p = float(isect) / la\n",
    "            r = float(isect) / lb\n",
    "            f = 2. * p * r / (p + r)\n",
    "        else:\n",
    "            f = 0.\n",
    "    return (len_compress(la + lb), f, subset)\n",
    "\n",
    "def relative_len_difference(lca, lcb):\n",
    "    la, lb = len(lca), len(lcb)\n",
    "    return abs(la - lb) / float(max(la, lb) + 1e-5)\n",
    "\n",
    "def relative_ic_difference(lca, lcb):\n",
    "    #wa = sum(wweight[x] for x in lca)\n",
    "    #wb = sum(wweight[x] for x in lcb)\n",
    "    wa = sum(max(0., wweight[x] - minwweight) for x in lca)\n",
    "    wb = sum(max(0., wweight[x] - minwweight) for x in lcb)\n",
    "    return abs(wa - wb) / (max(wa, wb) + 1e-5)\n",
    "\n",
    "def my_custom_function(y_true, y_predict):\n",
    "    corr,_ = pearsonr(y_true, y_predict)\n",
    "    return corr\n",
    "\n",
    "    \n",
    " #   gs = GridSearchCV(clf, param_grid={'kernel': [ 'rbf'],'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='roc_auc')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dict()\n",
    "c['num_runs']   = 3\n",
    "c['num_epochs'] = 64\n",
    "c['num_batchs'] = 2\n",
    "c['batch_size'] = 3000\n",
    "c['wordvectdim']  = 300\n",
    "c['sentencepad']  = 60\n",
    "c['num_classes']  = 6\n",
    "c['cnnfilters']     = {1: 1800}\n",
    "c['cnninitial']     = 'he_uniform'\n",
    "c['cnnactivate']    = 'relu'\n",
    "c['densedimension'] = list([1800])\n",
    "c['denseinitial']   = 'he_uniform'\n",
    "c['denseactivate']  = 'tanh'\n",
    "c['optimizer']  = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk= cn.STSTask(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside convo: \n",
      "(?, 60, 300)\n",
      "(?, 60, 300)\n",
      "WARNING:tensorflow:From /apps/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "(?, 60, 1800)\n",
      "(?, 60, 1800)\n",
      "inside max\n",
      "(?, 1, 1800)\n",
      "(?, 1, 1800)\n",
      "inside flat\n",
      "(?, 1800)\n",
      "(?, 1800)\n",
      "(?, 1800)\n",
      "(?, 3600)\n",
      "inside dense\n",
      "(?, 1800)\n",
      "kd\n",
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "tsk.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe...(This might take one or two minutes.)\n"
     ]
    }
   ],
   "source": [
    "tsk.load_resc('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsk.load_data('sts-train.csv', 'sts-dev.csv', 'sts-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter 1\n",
      "sl slice(0, 1379, None)\n",
      "(1379,)\n",
      "[Test]=[0.7762]\n"
     ]
    }
   ],
   "source": [
    "tsk.model.load_weights('weightfile0')\n",
    "l=tsk.eval_model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= l[1].reshape(1379,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter 1\n",
      "sl slice(0, 5749, None)\n",
      "[Test]=[0.9398]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5749,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsk.eval_model2()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter 1\n",
      "sl slice(0, 5749, None)\n",
      "[Test]=[0.9398]\n"
     ]
    }
   ],
   "source": [
    "train_cnn=tsk.eval_model2()[1].reshape(5749,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(path):\n",
    "    import sys\n",
    "\n",
    "\n",
    "    orig = []\n",
    "    for l in open('test/sts_test.txt'):\n",
    "        orig.append([x.strip() for x in l.lower().split(\"\\t\")])\n",
    "\n",
    "    scores = list(map(float, open(path).readlines()))\n",
    "\n",
    "    if len(orig) != len(scores):\n",
    "        print (sys.stderr, \"Error: inputs should have the same number of lines\")\n",
    "        exit(1)\n",
    "    processed=[]\n",
    "    f = open('processed_'+path, 'w')\n",
    "    for i, s in enumerate(scores):\n",
    "        if orig[0] == orig[1]:\n",
    "            s = 5.\n",
    "        if s > 5:\n",
    "            s = 5.\n",
    "        if s < 0:\n",
    "            s = 0.\n",
    "        processed.append(s)\n",
    "        #print (s)\n",
    "        f.write(\"%s\\n\" % s)\n",
    "    f.close()\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## takelab_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)\n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    \n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    \n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf',), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1379, 22)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 22)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7870603748644066\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_output_MSRpar.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_output_MSRpar.txt'))\n",
    "\n",
    "\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using JC Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1379, 22)\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape)\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7872532529776621\n"
     ]
    }
   ],
   "source": [
    "with open('takelab_jc.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('takelab_jc.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lin Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-b69c90f6ff28>, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-b69c90f6ff28>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    f= open('test/sts_test_scores.txt')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    \n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('takelab_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('takelab_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec with weighted_LSA_vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb)\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    \n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wv_wlsa.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('wv_wlsa.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JC AND LIN Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            #wvec_sim(lema,lemb),\n",
    "            #weighted_wvec_sim(lema,lemb),\n",
    "            dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    '''gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)'''\n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    \n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    '''test_data=[]\n",
    "    for idx, sp in enumerate(load_data('trial1/STS.input.txt')):\n",
    "        y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    \n",
    "    f= open('trial1/STS.gs.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)'''\n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jc_lin.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('jc_lin.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec with weighted_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,0),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    my_scorer = make_scorer(my_custom_function, greater_is_better=True)\n",
    "   \n",
    "    \n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)\n",
    "    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring='r2',cv=10) \n",
    "  \n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wv_wwv.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "\n",
    "model_pred=np.array(post_process('wv_wwv.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec_wwv_jc_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(sa, sb):\n",
    "    olca = get_locase_words(sa)\n",
    "    olcb = get_locase_words(sb)\n",
    "    lca = [w for w in olca if w not in stopwords]\n",
    "    lcb = [w for w in olcb if w not in stopwords]\n",
    "    lema = get_lemmatized_words(sa)\n",
    "    lemb = get_lemmatized_words(sb)\n",
    "   \n",
    "    f = []\n",
    "    f += number_features(sa, sb)\n",
    "    f += case_matches(sa, sb)\n",
    "    f += stocks_matches(sa, sb)\n",
    "   \n",
    "    f += [\n",
    "            ngram_match(lca, lcb, 1),\n",
    "            ngram_match(lca, lcb, 2),\n",
    "            ngram_match(lca, lcb, 3),\n",
    "            ngram_match(lema, lemb, 1),\n",
    "            ngram_match(lema, lemb, 2),\n",
    "            ngram_match(lema, lemb, 3),\n",
    "            wn_sim_match(lema, lemb,1),\n",
    "            wn_sim_match(lema, lemb,2),\n",
    "            weighted_word_match(olca, olcb),\n",
    "            weighted_word_match(lema, lemb),\n",
    "            wvec_sim(lema,lemb),\n",
    "            weighted_wvec_sim(lema,lemb),\n",
    "            #dist_sim(nyt_sim, lema, lemb),\n",
    "            #dist_sim(wiki_sim, lema, lemb),\n",
    "            #weighted_dist_sim(nyt_sim, lema, lemb),\n",
    "            #weighted_dist_sim(wiki_sim, lema, lemb),\n",
    "            relative_len_difference(lca, lcb),\n",
    "            relative_ic_difference(olca, olcb)\n",
    "        ]\n",
    "\n",
    "    return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    '''scores = None\n",
    "    scores = [float(x) for x in open('../../train/STS.gs.MSRpar.txt','r')]'''\n",
    " \n",
    "    data=[]\n",
    "    c=0\n",
    "    for idx, sp in enumerate(load_data('train/sts_train.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        data+=[(calc_features(*sp))]\n",
    "        \n",
    "    data=np.array(data)   \n",
    "    data= np.append(data, train_cnn, axis=1)\n",
    "    \n",
    "    f= open('train/sts_train_scores.txt')\n",
    "    label= f.readlines()\n",
    "    label=[float(w.strip()) for w in label]\n",
    "    label=np.array(label).reshape(len(label),)\n",
    "    \n",
    "    #gs = GridSearchCV(SVR(kernel='rbf'), param_grid={'C': [1,2,5,10,20, 50,100,200,500,1000],'gamma':[2,1,.5,.2,.1,.05,.02,.01,.005,.002]},scoring=my_scorer,cv=10,n_jobs=-1)\n",
    "    gs = GridSearchCV(MLPRegressor(), param_grid={'solver': ['lbfgs','sgd','adam'],'alpha': 10.0 ** -np.arange(1, 7),'learning_rate':['constant'], 'learning_rate_init': [0.001],'hidden_layer_sizes': [25,50,100],'activation': ['identity','relu','logistic', 'tanh']},scoring=my_scorer,n_jobs=-1)\n",
    "    \n",
    "    gs.fit(data,label)\n",
    "    m=gs.best_estimator_\n",
    "    model=m.fit(data,label)\n",
    "    \n",
    "    \n",
    "    test_data=[]\n",
    "    for idx, sp in enumerate(load_data('test/sts_test.txt')):\n",
    "        #y = 0. if scores is None else scores[idx]\n",
    "        test_data+=[(calc_features(*sp))]\n",
    "        \n",
    "    test_data=np.array(test_data)\n",
    "    test_data=np.append(test_data,temp,axis=1)\n",
    "    print(test_data.shape\n",
    "    \n",
    "    f= open('test/sts_test_scores.txt')\n",
    "    test_label= f.readlines()\n",
    "    test_label=[float(w.strip()) for w in test_label]\n",
    "    test_label=np.array(test_label).reshape(len(test_label),)\n",
    "    \n",
    " \n",
    "    \n",
    "    predictions= model.predict(test_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full.txt', 'w') as f:\n",
    "        for item in predictions:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "model_pred=np.array(post_process('full.txt'))\n",
    "corr,_ = pearsonr(test_label, model_pred)\n",
    "print (corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
