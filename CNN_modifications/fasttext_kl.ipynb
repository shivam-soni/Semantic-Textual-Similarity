{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"fasttext_kl.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SH2wpCTYZdMh","colab_type":"code","colab":{}},"source":["import numpy as np\n","import multiprocessing as mp\n","import random,copy,string\n","from nltk.tokenize import word_tokenize\n","from scipy.stats import pearsonr\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Input, Convolution1D, MaxPooling1D, Flatten\n","from tensorflow.python.keras.layers import Lambda, multiply, concatenate, Dense\n","from tensorflow.python.keras.regularizers import l2\n","from tensorflow.python.keras.callbacks import Callback\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q_ABwOwnYl1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e93f45d2-fc36-49f6-c5c7-af6c8c2595a8"},"source":["import os, string, random, time, math\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import torch\n","'''import mlflow\n","import mlflow.pytorch'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import mlflow\\nimport mlflow.pytorch'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"JQcOsPfMJL1-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"f8f4745e-6b6b-4070-a28c-1372c4656c33"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"uLz42OGuar31","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"3de694fc-0c5d-467f-8598-6f8da2562ed1"},"source":["'''from google.colab import drive\n","drive.mount('/content/drive')'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ae7JCVPfnj-p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a3a2bd8f-f16d-430d-f193-610193c0e0c6"},"source":["#cd 'drive/My Drive'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dAPg3_LAoKHs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":322},"outputId":"5ea477c1-8e88-4b11-d197-ab3af90e2db7"},"source":["#ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34m2017_takelab+cnn+mlp\u001b[0m/     glove_kl_model.pt\n"," cc.en.300.bin             \u001b[01;34mmlflow_server\u001b[0m/\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/         \u001b[01;34mmlruns\u001b[0m/\n"," data.zip                  model_cifar.pt\n"," deep_fasttext_epoch.pt    msr_vid_SVR.ipynb\n"," fasttext_cross0           name2lang.txt\n"," fasttext_cross1           pytorch_cnn_code.ipynb\n"," fasttext_cross2           sts-dev.csv\n"," fasttext_cross_epoch.pt   STS_survey.odt\n"," fasttext_cross_model.pt   sts-test.csv\n"," fasttext_cross.pt         sts-train.csv\n"," glove.840B.300d.txt       substitute_vec.txt\n"," glove_ce_epoch.pt         \u001b[01;34mtargets_subvecs\u001b[0m/\n"," glove_ce_model.pt         try.csv\n"," glove_cross_epoch.pt      try_test.csv\n"," glove_cross_model.pt      try_validation.csv\n"," glove_kl_epoch.pt         Understanding_cnn_code.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aN1UtOJrZdM5","colab_type":"code","colab":{}},"source":["c = dict()\n","c['num_runs']   = 3\n","c['num_epochs'] = 2\n","c['num_batchs'] = 4\n","c['batch_size'] = 3\n","c['wordvectdim']  = 300\n","c['sentencepad']  = 60\n","c['num_classes']  = 6\n","c['cnnfilters']     = {1: 1800}\n","c['cnninitial']     = 'he_uniform'\n","c['cnnactivate']    = 'relu'\n","c['densedimension'] = list([1800])\n","c['denseinitial']   = 'he_uniform'\n","c['denseactivate']  = 'tanh'\n","c['optimizer']  = 'adam'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGZC-I84ZdNN","colab_type":"code","colab":{}},"source":["wordvectdim=300\n","wordtoindex= dict()\n","indextovector= []\n","indextovector.append(np.zeros(wordvectdim))    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WI95iihPfWFV","colab_type":"text"},"source":["# Loading word vectors"]},{"cell_type":"code","metadata":{"id":"5sOYuK24UniQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":270},"outputId":"d4a6feee-95ba-4a3e-c4a7-d740f44f70f9"},"source":["!pip3 install fasttext"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting fasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n","\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.4)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3020488 sha256=3033d4de5958613509b67cb428f0d3a0d24b2c15000adcf22d654f022b42eb05\n","  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e_m1YW_vUTF0","colab_type":"code","colab":{}},"source":["import fasttext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xygLK_HgUtAM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"da4d1f9d-0dd8-412f-fd75-7b343cc0c6f4"},"source":["ft = fasttext.load_model('../cc.en.300.bin')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vr-1hkKAlgRi","colab_type":"text"},"source":["# Params"]},{"cell_type":"code","metadata":{"id":"F1YZLUtAldcF","colab_type":"code","colab":{}},"source":["class Params(object):\n","  def __init__(self, batch_size, epochs, seed, log_interval):\n","    self.batch_size = batch_size\n","    self.epochs = epochs\n","    self.seed = seed\n","    self.log_interval = log_interval\n","\n","args= Params(16,8,0,8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1oW_df2ZdRk","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"v87IpbJR9gDG","colab_type":"code","colab":{}},"source":["def matrixize(sentencelist, sentencepad):\n","  padding= np.zeros(300)\n","  matrix= np.zeros((len(sentencelist),sentencepad,300 ))\n","  m=0\n","  s=1\n","\n","  for i in range(len(sentencelist)):\n","    for j in range(len(sentencelist[i])):\n","      try:\n","        matrix[i][j]= ft.get_word_vector(sentencelist[i][j])\n","      except:\n","        print(sentencelist[i][j])\n","        matrix[i][j]= np.random.normal(m,s,300)/norm(np.random.normal(m,s,300) + 1e-8)\n","\n","  return matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdRruMEiZdRt","colab_type":"code","colab":{}},"source":["def _load_data(filename):\n","        s0,s1,labels = [],[],[]\n","        lines=open(filename,'r').read().splitlines()\n","        for line in lines:\n","            _,_,_,_, label, s0x, s1x = line.rstrip().split('\\t')[:7]\n","            labels.append(float(label))\n","            s0.append([word.lower() for word in word_tokenize(s0x) if word not in string.punctuation])\n","            s1.append([word.lower() for word in word_tokenize(s1x) if word not in string.punctuation])\n","\n","        m0 = matrixize(s0, c['sentencepad'])\n","        m1 = matrixize(s1, c['sentencepad'])\n","        classes = np.zeros((len(labels),c['num_classes']))\n","        for i, label in enumerate(labels): # making probability distribution of classes\n","            if np.floor(label) + 1 < c['num_classes']:\n","                classes[i, int(np.floor(label)) + 1] = label - np.floor(label)\n","            classes[i, int(np.floor(label))] = np.floor(label) - label + 1\n","            \n","        return {'labels': labels, 's0': s0, 's1': s1, 'classes': classes, 'm0': m0, 'm1': m1}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMYnoNCrdNYu","colab_type":"text"},"source":["# Sampling Batch"]},{"cell_type":"code","metadata":{"id":"grGwK-j0-ctQ","colab_type":"code","colab":{}},"source":["def _sample_pairs(data,batch_size):\n","  datacopy={}\n","  for i in data.keys():\n","    datacopy[i]= []\n","  for i in range(batch_size):\n","    index = np.random.randint(len(data['labels']))\n","    #print(index)\n","    for key,value in data.items():\n","      datacopy[key].append(value[index])\n","  datacopy['classes']= torch.tensor(datacopy['classes'])\n","  datacopy['m0']= torch.tensor(datacopy['m0'])\n","  datacopy['m1']= torch.tensor(datacopy['m1'])\n","  return datacopy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BT33frFDlodS","colab_type":"text"},"source":["# Defining CNN Model"]},{"cell_type":"code","metadata":{"id":"GnxSiUWpl-qn","colab_type":"code","colab":{}},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OSip9cpbZdTt","colab_type":"code","colab":{}},"source":["class CNN_Model(nn.Module):\n","  def __init__(self,nh):\n","    super(CNN_Model, self).__init__()\n","    self.cnn= nn.Sequential(nn.Conv1d(300,1800,1),              #(bs,300,60) -> (bs, 1800, 60)\n","                            nn.LeakyReLU(0.01),\n","                            nn.MaxPool1d(kernel_size=60))       #(bs,1800,60)-> (bs, 1800, 1)\n","\n","    self.Linear= nn.Sequential(nn.Linear(3600,nh),            #(bs, 3600)  -> (bs, 1800 )\n","                               nn.LeakyReLU(0.01),\n","                               nn.Linear(nh,6),               #(bs,1800)   -> (bs,6)\n","                               nn.LogSoftmax(dim=1))\n","                            \n","    \n","\n","  def forward(self,input1, input2):\n","    input1= self.cnn(input1)\n","    input2= self.cnn(input2)\n","    #print(\"input1=\",input1.shape)\n","    #print(\"input2=\",input2.shape)\n","    input1= torch.flatten(input1,1)\n","    input2= torch.flatten(input2,1)\n","    #print(\"input1=\",input1.shape)\n","    #print(\"input2=\",input2.shape)\n","    absdiff= abs(input1 - input2)\n","    mulDifference= input1 * input2\n","    concatenate = torch.cat((absdiff,mulDifference),1)\n","    #print(concatenate.shape)\n","    output= self.Linear(concatenate)\n","    #print(output.shape)\n","    return (output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSeeJEb5MXWa","colab_type":"code","colab":{}},"source":["def weights_init(m):\n","    if isinstance(m, nn.Conv1d) or isinstance(m, torch.nn.Linear):\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRUv6bDcmYpE","colab_type":"text"},"source":["# Defining Loss"]},{"cell_type":"code","metadata":{"id":"JgSXUhJLCuQn","colab_type":"code","colab":{}},"source":["criterion= nn.KLDivLoss(reduction= 'batchmean')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6g9dYuH9x1Dx","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"QxQ4r7AzyWA-","colab_type":"text"},"source":["## Train setup"]},{"cell_type":"code","metadata":{"id":"QCWkAB75yili","colab_type":"code","colab":{}},"source":["def train_batch(net,trainload, opt,batch_size, device= 'cpu'):\n","  \n","  net.train().to(device)\n","  opt.zero_grad()\n","  \n","  \n","\n","  data= _sample_pairs(trainload,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype=torch.float)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype=torch.float)\n","  true_output= (data['classes']).to(device, dtype=torch.float)\n","  pred_output= net(input1,input2).to(torch.float)\n","  #loss= _lossfunction(true_output,pred_output)\n","\n","  loss= criterion(pred_output,true_output)\n","  loss.backward()\n","  opt.step()\n","  #print(\"lossgf\",loss.item())\n","  return loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUuqHGuH45FT","colab_type":"text"},"source":["## Validation Setup"]},{"cell_type":"code","metadata":{"id":"M6VfkAYo3RN5","colab_type":"code","colab":{}},"source":["def valid_batch(net,validation, opt,batch_size, device= 'cpu'):\n","  net.eval().to(device)\n","  \n","  data = _sample_pairs(validation,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype=torch.float)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype=torch.float)\n","  true_output= (data['classes']).to(device, dtype=torch.float)\n","  pred_output= net(input1,input2).to(torch.float)\n","  #loss= _lossfunction(true_output,pred_output)\n","  loss= criterion(pred_output,true_output)\n","  return loss.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWJNMId9pJIH","colab_type":"text"},"source":["# Full Training Setup"]},{"cell_type":"code","metadata":{"id":"Y5WzVLt2pPkA","colab_type":"code","colab":{}},"source":["def train_setup_Adam(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.Adam(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('fasttext_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'fasttext_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M350LAyaqbk","colab_type":"code","colab":{}},"source":["def train_setup_Adamax(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.Adamax(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('fasttext_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'fasttext_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3lxuhFAopUC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"43031128-550b-4419-934e-7ab43a070645"},"source":["'''net= CNN_Model()\n","train_setup(net,device='cuda:0')'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"net= CNN_Model()\\ntrain_setup(net,device='cuda:0')\""]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"-dkld6lIWW8b","colab_type":"text"},"source":["# Evaluate"]},{"cell_type":"markdown","metadata":{"id":"2GQzXlEi-XHH","colab_type":"text"},"source":["## Evaluation Setup"]},{"cell_type":"code","metadata":{"id":"SkXPty0k-h9O","colab_type":"code","colab":{}},"source":["def eval_setup(net,batch_size = 1379, device = 'cpu'):\n","  \n","  net.load_state_dict(torch.load('fasttext_kl_epoch.pt'))\n","  net.eval().to(device)\n","  testload= _load_data('sts-test.csv')\n","  test_n_batch= int(len(testload['labels'])/batch_size)\n","  #print(len(testload['labels']))\n","  test_loss_arr = np.zeros(test_n_batch+ 1)\n","\n","\n","  data= _sample_pairs(testload,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype=torch.float32)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype=torch.float32)\n","  pred_output= (net(input1,input2)).to('cpu')\n","  pred= pred_output.detach().numpy()\n","  prediction = np.dot(np.array(pred),np.arange(c['num_classes']))\n","  print('pred_shape',prediction.shape)\n","  goldlabels = data['labels']\n","  result=round((pearsonr(prediction, goldlabels)[0]),4)\n","  print(\"result\",result)\n","\n","  return result\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDkaR6L8Sc6s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4c4d9082-2ff0-4ff2-95b1-4543c1e78521"},"source":["'''net=CNN_Model(1800)\n","eval_setup(net,device = 'cuda:0')'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"net=CNN_Model(1800)\\neval_setup(net,device = 'cuda:0')\""]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"uACzDtuiU-_k","colab_type":"text"},"source":["# Hyper-parameter Tuning"]},{"cell_type":"code","metadata":{"id":"ZDX421f0VWRP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"964f1b57-d568-4c57-d9e8-006b56886082"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.0002,0.0005,0.0006]:\n","    \n","        for hidden_nodes in [1500,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.4f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adam(model,lr=learning_rate,device='cuda:0')\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'fasttext_kl_adam_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.0002, Hidden nodes = 1500\n","\n","Epoch: 0 \tTraining Loss: 1.068886 \tValidation Loss: 1.103054\n","Validation loss decreased (inf --> 1.103054).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.852450 \tValidation Loss: 0.987310\n","Validation loss decreased (1.103054 --> 0.987310).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.675030 \tValidation Loss: 1.003044\n","Epoch: 3 \tTraining Loss: 0.679298 \tValidation Loss: 1.163934\n","Epoch: 4 \tTraining Loss: 0.672606 \tValidation Loss: 0.946332\n","Validation loss decreased (0.987310 --> 0.946332).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.541607 \tValidation Loss: 0.982673\n","Epoch: 6 \tTraining Loss: 0.522097 \tValidation Loss: 1.156564\n","Epoch: 7 \tTraining Loss: 0.560939 \tValidation Loss: 1.022425\n","pred_shape (1379,)\n","result 0.5673\n","best_model_score increased (-inf --> 0.567300).  Saving model ...\n","\n","LR = 0.0002, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: 1.063822 \tValidation Loss: 1.077518\n","Validation loss decreased (inf --> 1.077518).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.846831 \tValidation Loss: 1.108177\n","Epoch: 2 \tTraining Loss: 0.844500 \tValidation Loss: 1.133321\n","Epoch: 3 \tTraining Loss: 0.843660 \tValidation Loss: 0.994460\n","Validation loss decreased (1.077518 --> 0.994460).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.686935 \tValidation Loss: 0.957809\n","Validation loss decreased (0.994460 --> 0.957809).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.541695 \tValidation Loss: 1.023747\n","Epoch: 6 \tTraining Loss: 0.544340 \tValidation Loss: 1.139479\n","Epoch: 7 \tTraining Loss: 0.548036 \tValidation Loss: 1.037232\n","pred_shape (1379,)\n","result 0.5675\n","best_model_score increased (0.567300 --> 0.567500).  Saving model ...\n","\n","LR = 0.0002, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.072454 \tValidation Loss: 1.100948\n","Validation loss decreased (inf --> 1.100948).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.850362 \tValidation Loss: 0.988393\n","Validation loss decreased (1.100948 --> 0.988393).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.679354 \tValidation Loss: 0.951849\n","Validation loss decreased (0.988393 --> 0.951849).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.537902 \tValidation Loss: 1.053657\n","Epoch: 4 \tTraining Loss: 0.528323 \tValidation Loss: 1.019165\n","Epoch: 5 \tTraining Loss: 0.534980 \tValidation Loss: 0.994280\n","Epoch: 6 \tTraining Loss: 0.526257 \tValidation Loss: 1.040759\n","Epoch: 7 \tTraining Loss: 0.524107 \tValidation Loss: 0.957102\n","pred_shape (1379,)\n","result 0.5821\n","best_model_score increased (0.567500 --> 0.582100).  Saving model ...\n","\n","LR = 0.0005, Hidden nodes = 1500\n","\n","Epoch: 0 \tTraining Loss: 1.012510 \tValidation Loss: 1.008982\n","Validation loss decreased (inf --> 1.008982).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.728757 \tValidation Loss: 1.034207\n","Epoch: 2 \tTraining Loss: 0.731681 \tValidation Loss: 1.015324\n","Epoch: 3 \tTraining Loss: 0.736594 \tValidation Loss: 0.970470\n","Validation loss decreased (1.008982 --> 0.970470).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.546564 \tValidation Loss: 1.003392\n","Epoch: 5 \tTraining Loss: 0.552704 \tValidation Loss: 1.234616\n","Epoch: 6 \tTraining Loss: 0.544828 \tValidation Loss: 1.010271\n","Epoch: 7 \tTraining Loss: 0.541609 \tValidation Loss: 1.028461\n","pred_shape (1379,)\n","result 0.5932\n","best_model_score increased (0.582100 --> 0.593200).  Saving model ...\n","\n","LR = 0.0005, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: 1.003147 \tValidation Loss: 1.042949\n","Validation loss decreased (inf --> 1.042949).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.755512 \tValidation Loss: 0.970373\n","Validation loss decreased (1.042949 --> 0.970373).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.540051 \tValidation Loss: 1.034493\n","Epoch: 3 \tTraining Loss: 0.546590 \tValidation Loss: 1.044238\n","Epoch: 4 \tTraining Loss: 0.552997 \tValidation Loss: 1.121696\n","Epoch: 5 \tTraining Loss: 0.542949 \tValidation Loss: 1.024127\n","Epoch: 6 \tTraining Loss: 0.551875 \tValidation Loss: 1.058987\n","Epoch: 7 \tTraining Loss: 0.539391 \tValidation Loss: 1.078355\n","pred_shape (1379,)\n","result 0.6041\n","best_model_score increased (0.593200 --> 0.604100).  Saving model ...\n","\n","LR = 0.0005, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.001036 \tValidation Loss: 0.961703\n","Validation loss decreased (inf --> 0.961703).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.737970 \tValidation Loss: 0.974893\n","Epoch: 2 \tTraining Loss: 0.717251 \tValidation Loss: 1.060676\n","Epoch: 3 \tTraining Loss: 0.726747 \tValidation Loss: 0.987510\n","Epoch: 4 \tTraining Loss: 0.740233 \tValidation Loss: 0.952525\n","Validation loss decreased (0.961703 --> 0.952525).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.556314 \tValidation Loss: 1.078675\n","Epoch: 6 \tTraining Loss: 0.560443 \tValidation Loss: 1.128017\n","Epoch: 7 \tTraining Loss: 0.530210 \tValidation Loss: 1.108490\n","pred_shape (1379,)\n","result 0.6103\n","best_model_score increased (0.604100 --> 0.610300).  Saving model ...\n","\n","LR = 0.0006, Hidden nodes = 1500\n","\n","Epoch: 0 \tTraining Loss: 1.011143 \tValidation Loss: 1.065810\n","Validation loss decreased (inf --> 1.065810).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.721660 \tValidation Loss: 1.018525\n","Validation loss decreased (1.065810 --> 1.018525).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.525904 \tValidation Loss: 0.993557\n","Validation loss decreased (1.018525 --> 0.993557).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.381364 \tValidation Loss: 1.101505\n","Epoch: 4 \tTraining Loss: 0.385991 \tValidation Loss: 1.032719\n","Epoch: 5 \tTraining Loss: 0.386222 \tValidation Loss: 1.146579\n","Epoch: 6 \tTraining Loss: 0.377319 \tValidation Loss: 1.059326\n","Epoch: 7 \tTraining Loss: 0.375302 \tValidation Loss: 1.135715\n","pred_shape (1379,)\n","result 0.6268\n","best_model_score increased (0.610300 --> 0.626800).  Saving model ...\n","\n","LR = 0.0006, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: 0.985816 \tValidation Loss: 1.061312\n","Validation loss decreased (inf --> 1.061312).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.713450 \tValidation Loss: 0.970842\n","Validation loss decreased (1.061312 --> 0.970842).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.530718 \tValidation Loss: 0.976090\n","Epoch: 3 \tTraining Loss: 0.522500 \tValidation Loss: 1.001525\n","Epoch: 4 \tTraining Loss: 0.513229 \tValidation Loss: 1.241072\n","Epoch: 5 \tTraining Loss: 0.520569 \tValidation Loss: 1.047718\n","Epoch: 6 \tTraining Loss: 0.518030 \tValidation Loss: 1.008815\n","Epoch: 7 \tTraining Loss: 0.515264 \tValidation Loss: 1.071582\n","pred_shape (1379,)\n","result 0.5942\n","\n","LR = 0.0006, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.010974 \tValidation Loss: 1.000784\n","Validation loss decreased (inf --> 1.000784).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.710806 \tValidation Loss: 1.038184\n","Epoch: 2 \tTraining Loss: 0.706810 \tValidation Loss: 1.074521\n","Epoch: 3 \tTraining Loss: 0.716459 \tValidation Loss: 1.076679\n","Epoch: 4 \tTraining Loss: 0.713586 \tValidation Loss: 1.013502\n","Epoch: 5 \tTraining Loss: 0.709534 \tValidation Loss: 1.149012\n","Epoch: 6 \tTraining Loss: 0.710662 \tValidation Loss: 1.008523\n","Epoch: 7 \tTraining Loss: 0.725398 \tValidation Loss: 0.960667\n","Validation loss decreased (1.000784 --> 0.960667).  Saving model ...\n","pred_shape (1379,)\n","result 0.596\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0UAagsWKazLp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"afaddacc-592e-475e-cec6-1559eb26b7ef"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.0001,0.00008,0.0005]:\n","    \n","        for hidden_nodes in [1700,1800,1900]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.4f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adamax(model,lr=learning_rate,device='cuda:0')\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'fasttext_kl_adamax_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.0001, Hidden nodes = 1700\n","\n","Epoch: 0 \tTraining Loss: 1.180238 \tValidation Loss: 1.196380\n","Validation loss decreased (inf --> 1.196380).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.069227 \tValidation Loss: 1.115158\n","Validation loss decreased (1.196380 --> 1.115158).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.989156 \tValidation Loss: 1.125852\n","Epoch: 3 \tTraining Loss: 1.000520 \tValidation Loss: 1.128462\n","Epoch: 4 \tTraining Loss: 0.990412 \tValidation Loss: 1.097261\n","Validation loss decreased (1.115158 --> 1.097261).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.943522 \tValidation Loss: 1.123934\n","Epoch: 6 \tTraining Loss: 0.944244 \tValidation Loss: 1.084867\n","Validation loss decreased (1.097261 --> 1.084867).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.891903 \tValidation Loss: 1.029650\n","Validation loss decreased (1.084867 --> 1.029650).  Saving model ...\n","pred_shape (1379,)\n","result 0.5593\n","best_model_score increased (-inf --> 0.559300).  Saving model ...\n","\n","LR = 0.0001, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.179256 \tValidation Loss: 1.181358\n","Validation loss decreased (inf --> 1.181358).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.061320 \tValidation Loss: 1.134532\n","Validation loss decreased (1.181358 --> 1.134532).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.990741 \tValidation Loss: 1.119307\n","Validation loss decreased (1.134532 --> 1.119307).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.944402 \tValidation Loss: 1.068181\n","Validation loss decreased (1.119307 --> 1.068181).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.894765 \tValidation Loss: 1.075360\n","Epoch: 5 \tTraining Loss: 0.891538 \tValidation Loss: 1.059596\n","Validation loss decreased (1.068181 --> 1.059596).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.847613 \tValidation Loss: 1.034011\n","Validation loss decreased (1.059596 --> 1.034011).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.827817 \tValidation Loss: 1.041772\n","pred_shape (1379,)\n","result 0.5835\n","best_model_score increased (0.559300 --> 0.583500).  Saving model ...\n","\n","LR = 0.0001, Hidden nodes = 1900\n","\n","Epoch: 0 \tTraining Loss: 1.172675 \tValidation Loss: 1.210683\n","Validation loss decreased (inf --> 1.210683).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.053284 \tValidation Loss: 1.107752\n","Validation loss decreased (1.210683 --> 1.107752).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.993507 \tValidation Loss: 1.104183\n","Validation loss decreased (1.107752 --> 1.104183).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.948744 \tValidation Loss: 1.067648\n","Validation loss decreased (1.104183 --> 1.067648).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.903406 \tValidation Loss: 1.052276\n","Validation loss decreased (1.067648 --> 1.052276).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.857375 \tValidation Loss: 1.065579\n","Epoch: 6 \tTraining Loss: 0.859213 \tValidation Loss: 1.066363\n","Epoch: 7 \tTraining Loss: 0.855571 \tValidation Loss: 1.055979\n","pred_shape (1379,)\n","result 0.5389\n","\n","LR = 0.0001, Hidden nodes = 1700\n","\n","Epoch: 0 \tTraining Loss: 1.210309 \tValidation Loss: 1.172806\n","Validation loss decreased (inf --> 1.172806).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.083211 \tValidation Loss: 1.150283\n","Validation loss decreased (1.172806 --> 1.150283).  Saving model ...\n","Epoch: 2 \tTraining Loss: 1.024767 \tValidation Loss: 1.138960\n","Validation loss decreased (1.150283 --> 1.138960).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.972071 \tValidation Loss: 1.104336\n","Validation loss decreased (1.138960 --> 1.104336).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.932697 \tValidation Loss: 1.135576\n","Epoch: 5 \tTraining Loss: 0.919136 \tValidation Loss: 1.063283\n","Validation loss decreased (1.104336 --> 1.063283).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.881727 \tValidation Loss: 1.072885\n","Epoch: 7 \tTraining Loss: 0.887120 \tValidation Loss: 1.080548\n","pred_shape (1379,)\n","result 0.5425\n","\n","LR = 0.0001, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.182991 \tValidation Loss: 1.209079\n","Validation loss decreased (inf --> 1.209079).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.087109 \tValidation Loss: 1.138172\n","Validation loss decreased (1.209079 --> 1.138172).  Saving model ...\n","Epoch: 2 \tTraining Loss: 1.021130 \tValidation Loss: 1.124168\n","Validation loss decreased (1.138172 --> 1.124168).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.978298 \tValidation Loss: 1.114379\n","Validation loss decreased (1.124168 --> 1.114379).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.935129 \tValidation Loss: 1.116936\n","Epoch: 5 \tTraining Loss: 0.938535 \tValidation Loss: 1.101473\n","Validation loss decreased (1.114379 --> 1.101473).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.891886 \tValidation Loss: 1.065909\n","Validation loss decreased (1.101473 --> 1.065909).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.864501 \tValidation Loss: 1.079546\n","pred_shape (1379,)\n","result 0.5337\n","\n","LR = 0.0001, Hidden nodes = 1900\n","\n","Epoch: 0 \tTraining Loss: 1.189285 \tValidation Loss: 1.224601\n","Validation loss decreased (inf --> 1.224601).  Saving model ...\n","Epoch: 1 \tTraining Loss: 1.081958 \tValidation Loss: 1.120387\n","Validation loss decreased (1.224601 --> 1.120387).  Saving model ...\n","Epoch: 2 \tTraining Loss: 1.011433 \tValidation Loss: 1.169102\n","Epoch: 3 \tTraining Loss: 1.024406 \tValidation Loss: 1.091108\n","Validation loss decreased (1.120387 --> 1.091108).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.979129 \tValidation Loss: 1.094134\n","Epoch: 5 \tTraining Loss: 0.972073 \tValidation Loss: 1.106390\n","Epoch: 6 \tTraining Loss: 0.970922 \tValidation Loss: 1.073091\n","Validation loss decreased (1.091108 --> 1.073091).  Saving model ...\n","Epoch: 7 \tTraining Loss: 0.927129 \tValidation Loss: 1.058234\n","Validation loss decreased (1.073091 --> 1.058234).  Saving model ...\n","pred_shape (1379,)\n","result 0.5501\n","\n","LR = 0.0005, Hidden nodes = 1700\n","\n","Epoch: 0 \tTraining Loss: 1.079471 \tValidation Loss: 1.114678\n","Validation loss decreased (inf --> 1.114678).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.908382 \tValidation Loss: 1.132027\n","Epoch: 2 \tTraining Loss: 0.880150 \tValidation Loss: 1.021930\n","Validation loss decreased (1.114678 --> 1.021930).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.768009 \tValidation Loss: 1.034709\n","Epoch: 4 \tTraining Loss: 0.755284 \tValidation Loss: 1.080812\n","Epoch: 5 \tTraining Loss: 0.770041 \tValidation Loss: 0.988127\n","Validation loss decreased (1.021930 --> 0.988127).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.639650 \tValidation Loss: 1.014091\n","Epoch: 7 \tTraining Loss: 0.646791 \tValidation Loss: 1.044969\n","pred_shape (1379,)\n","result 0.538\n","\n","LR = 0.0005, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: 1.077617 \tValidation Loss: 1.134228\n","Validation loss decreased (inf --> 1.134228).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.897036 \tValidation Loss: 1.027068\n","Validation loss decreased (1.134228 --> 1.027068).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.755311 \tValidation Loss: 1.005707\n","Validation loss decreased (1.027068 --> 1.005707).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.634395 \tValidation Loss: 1.074324\n","Epoch: 4 \tTraining Loss: 0.623574 \tValidation Loss: 1.022109\n","Epoch: 5 \tTraining Loss: 0.634580 \tValidation Loss: 1.038614\n","Epoch: 6 \tTraining Loss: 0.645738 \tValidation Loss: 1.007830\n","Epoch: 7 \tTraining Loss: 0.633936 \tValidation Loss: 0.973543\n","Validation loss decreased (1.005707 --> 0.973543).  Saving model ...\n","pred_shape (1379,)\n","result 0.5755\n","\n","LR = 0.0005, Hidden nodes = 1900\n","\n","Epoch: 0 \tTraining Loss: 1.066275 \tValidation Loss: 1.066972\n","Validation loss decreased (inf --> 1.066972).  Saving model ...\n","Epoch: 1 \tTraining Loss: 0.877448 \tValidation Loss: 1.038991\n","Validation loss decreased (1.066972 --> 1.038991).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.747896 \tValidation Loss: 1.031852\n","Validation loss decreased (1.038991 --> 1.031852).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.619009 \tValidation Loss: 1.023089\n","Validation loss decreased (1.031852 --> 1.023089).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.518208 \tValidation Loss: 0.985054\n","Validation loss decreased (1.023089 --> 0.985054).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.417648 \tValidation Loss: 1.072375\n","Epoch: 6 \tTraining Loss: 0.416225 \tValidation Loss: 1.036848\n","Epoch: 7 \tTraining Loss: 0.429856 \tValidation Loss: 1.093767\n","pred_shape (1379,)\n","result 0.5625\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Js120I3bTMaB","colab_type":"text"},"source":["# Practice"]},{"cell_type":"code","metadata":{"id":"99F0SRW5dyY9","colab_type":"code","colab":{}},"source":["trainload= _load_data('sts-train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNi4lsSTdkGL","colab_type":"code","colab":{}},"source":["data=_sample_pairs(trainload,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhA8win0gR0V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"15ee7d0c-693b-4bd4-f857-d14e5358ec66"},"source":["data_len= []\n","for i in data['s0']:\n","  data_len.append(len(i))\n","max_len_sent=max(data_len)\n","\n","rep= torch.zeros(max_len_sent, 2 ,300)\n","rep.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([11, 2, 300])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"u2UgwP2Zk23J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"83d2fa18-b2ce-4912-8f4d-0953eedf9be6"},"source":["(data['s0'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['brown',\n","  'and',\n","  'white',\n","  'cow',\n","  'standing',\n","  'in',\n","  'grass',\n","  'at',\n","  'side',\n","  'of',\n","  'road'],\n"," ['boy', 'scouts', 'delay', 'decision', 'on', 'admitting', 'gays']]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"1aknSKgzjzY1","colab_type":"code","colab":{}},"source":["temp=[]\n","rep[0][0]=torch.tensor(indextovector[wordtoindex['brown']])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63gsjzqaoYLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ec8900d0-ef3d-46a2-e169-ac80a390c947"},"source":["rep[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3741, -0.0763,  0.1093,  0.1866,  0.0299,  0.1827, -0.6320,  0.1331,\n","         -0.1290,  0.6034, -0.6804, -0.1422, -0.1336, -0.6594,  0.0524,  0.1674,\n","          0.6392,  1.7680,  0.3462, -0.6248, -0.1287, -0.1970, -0.3745,  0.3306,\n","          0.0468, -0.6535, -0.5614,  0.2274,  0.2292, -0.4158, -0.1677,  0.3354,\n","          0.0972, -0.4670, -0.0269, -0.0677, -0.1921, -0.1337,  0.0163, -0.2083,\n","          0.6486, -0.1102, -0.0510,  0.0722,  0.1877,  0.2711, -0.3142,  0.1832,\n","          0.3915, -0.2255, -0.3819,  0.3306, -0.0899, -0.3763,  0.0480, -0.2050,\n","         -0.5489,  0.3044, -0.1887, -0.3034, -0.1157, -0.3487,  0.2800,  0.0501,\n","         -0.2814, -0.2335, -0.3683, -0.1204, -0.2833,  0.1857,  0.1036,  0.2531,\n","         -0.0340,  0.1051,  0.1214, -0.1630, -0.3318,  0.1731,  0.1076, -0.9936,\n","         -0.1171,  0.4223,  0.1512,  0.3106,  0.2674, -0.4927,  1.8049,  1.0738,\n","          0.3442,  0.1128, -0.1049,  0.3694,  0.4082, -0.4037,  0.3233, -0.0939,\n","         -0.0290,  0.3825, -0.3389, -0.6132,  0.8434,  0.1593, -0.1174,  0.0806,\n","         -0.2899, -0.4439, -0.1183,  0.1658,  0.1525,  0.2386, -0.3295, -0.3199,\n","         -0.3118, -0.1919,  0.2847,  0.2568,  0.4379, -0.0233,  0.1891, -0.0850,\n","         -0.1164, -0.1102, -0.0595,  0.1155,  0.2070,  0.5587,  0.8177, -0.2563,\n","         -0.0181, -0.0410, -0.2660, -0.4568, -0.0374,  0.3325,  0.1414, -0.0630,\n","         -0.0866, -0.3327, -0.0092, -0.0355, -2.8871,  0.2453,  0.2974,  0.6599,\n","         -0.0134, -0.1089,  0.1155,  0.0526, -0.0430,  0.2285,  0.4021, -0.4891,\n","          0.0441, -0.1368, -0.7181,  0.2526, -0.5788, -0.4807, -0.2409,  0.0427,\n","         -0.0946, -0.3034, -0.3197,  0.3554,  0.0629, -0.2043, -0.2978, -0.1545,\n","          0.2443,  0.0489, -0.0773,  0.3643, -0.1513, -0.4539, -0.3431,  0.1069,\n","          0.4292, -0.0260,  0.4825,  0.3361, -0.5032,  0.2241, -0.2737, -0.4904,\n","         -0.1173,  1.0537, -0.2023,  0.0490, -0.1223,  0.1100,  0.4155, -0.1183,\n","          0.1148, -0.2663,  0.2908,  0.2541,  0.3535,  0.3172, -0.1517, -0.5050,\n","         -0.2202,  0.1163, -0.2513,  0.2280,  0.2628,  0.2107,  0.0236,  0.0769,\n","         -0.1486,  0.0720,  0.3767,  0.3536, -0.3933, -0.1338,  0.5593,  0.0337,\n","         -0.4850, -0.3276,  0.2587,  0.4876, -0.2649,  0.0329, -0.0838, -0.0638,\n","          0.1076, -0.1855, -0.0523,  0.0767, -0.2148,  0.9646, -0.2479, -0.1211,\n","          0.0394,  0.4471, -0.1380, -0.0278, -0.4937, -0.5163,  0.3386,  0.5921,\n","         -0.2013, -0.0832, -0.3758, -0.2127, -0.3856,  0.2259, -0.3722, -0.1872,\n","         -0.6052, -0.1279,  0.2344, -0.4222, -0.2354,  0.2968,  0.0678,  0.0780,\n","          0.3195, -0.0348,  0.2981,  0.4400,  0.1174,  0.0550,  0.2368,  0.8982,\n","         -0.4097,  0.0752, -0.1103, -0.4099, -0.9572,  0.5275, -0.0427,  0.2662,\n","          0.3053, -0.5190, -0.4604, -0.0938,  0.1301,  0.0193,  0.0102,  0.0076,\n","          0.2955,  0.2316, -0.0349, -0.1169, -0.3273,  0.2049,  0.4750,  0.5131,\n","         -0.1458, -0.1851, -0.0154,  0.3929, -0.0348, -0.7203, -0.3653,  0.7405,\n","          0.1084, -0.3658, -0.2882,  0.1146],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000]])"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"A2XUoGKXm6bu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":249},"outputId":"492d4acd-34f3-4ba7-e431-75db36c221ee"},"source":["data['m0'][0][11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"VB2STaT2SzyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b337bbc4-9adc-4234-c976-8006afce7033"},"source":["m = nn.MaxPool1d(1)\n","input = torch.randn(6, 5)\n","input.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 5])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"xTHe3bx83kR6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"035245a9-3273-4e41-ead4-fde0c6edb090"},"source":["input"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.7378,  1.7368,  0.0198,  0.8431,  0.9046],\n","        [-1.5884, -1.1110,  0.8694,  0.3053,  0.3550],\n","        [ 0.1539, -0.4941, -0.0704,  1.3168,  0.9116],\n","        [ 1.2474,  0.6942,  0.1928, -0.5697, -0.8765],\n","        [-0.5326, -0.9174,  0.2422, -0.1825,  0.2426],\n","        [-1.5224, -1.7681, -1.8564, -1.0241, -1.3065]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"YytbQVTy2LTs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"d23b8943-a917-4490-8178-78e3b4bfa8f8"},"source":["input.to(torch.float32)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.7378,  1.7368,  0.0198,  0.8431,  0.9046],\n","        [-1.5884, -1.1110,  0.8694,  0.3053,  0.3550],\n","        [ 0.1539, -0.4941, -0.0704,  1.3168,  0.9116],\n","        [ 1.2474,  0.6942,  0.1928, -0.5697, -0.8765],\n","        [-0.5326, -0.9174,  0.2422, -0.1825,  0.2426],\n","        [-1.5224, -1.7681, -1.8564, -1.0241, -1.3065]])"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"c4ifcUQ-rE-z","colab_type":"code","colab":{}},"source":["output = m(input)\n","output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtNIYGWArTMW","colab_type":"code","colab":{}},"source":["out= torch.flatten(output, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KNDFi6QhK3V","colab_type":"code","colab":{}},"source":["out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oa8AVUN1TFka","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo28Dj7EhNUl","colab_type":"code","colab":{}},"source":["a=torch.tensor(np.array([1,2,3]))\n","b=torch.tensor(np.array([4,5,6]))\n","a*b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ec595DaW2i6W","colab_type":"code","colab":{}},"source":["c=abs(a-b)\n","\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcJ9pxdi3tIc","colab_type":"code","colab":{}},"source":["import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-r14qRP595v","colab_type":"code","colab":{}},"source":["torch.cat((a,b),1).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-F3HFrZ_ASV5","colab_type":"code","colab":{}},"source":["c= torch.randn(2, 5)\n","d= torch.randn(2,5)\n","e= torch.cat((c,d),1)\n","e.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sk0E5F22DpCX","colab_type":"code","colab":{}},"source":["(c-d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwGrQJ4TENvS","colab_type":"code","colab":{}},"source":["-1.5705-1.4329"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9NMh84DEcqd","colab_type":"code","colab":{}},"source":["(c*d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-V2_tVDEsj4","colab_type":"code","colab":{}},"source":["-1.5705*1.4329"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSrDjaRbEw58","colab_type":"code","colab":{}},"source":["-0.6561*-0.2841"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3FunLaWZdOb","colab_type":"code","colab":{}},"source":["'''blocksize=2\n","k= ((li[block:block+blocksize], block) for block in range(0,len(li),blocksize))\n","for i,j in k:\n","    print(i,j)'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKmW1OrSE5Fi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIw11E00ZdOp","colab_type":"code","colab":{}},"source":["'''def worker(args):\n","        print(args[0])\n","        print(args[1])'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFRV8fpxq6_H","colab_type":"code","colab":{}},"source":["p= np.random.randn(1,5)\n","p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIxJzwg4rFr7","colab_type":"code","colab":{}},"source":["plt.plot(p,'-*')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA4LFKOErSjU","colab_type":"code","colab":{}},"source":["p[1:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2pIVqMUrnEk","colab_type":"code","colab":{}},"source":["d={'a':[1],'b':[2],'c':[3]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utzQBcocDKgw","colab_type":"code","colab":{}},"source":["d1={}\n","d1= d.keys\n","d1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StnIcTfHDNOr","colab_type":"code","colab":{}},"source":["d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ILOBRLaDUoR","colab_type":"code","colab":{}},"source":["d2={}\n","d3=d2.fromkeys(d.keys(),[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYqLiw7OFti_","colab_type":"code","colab":{}},"source":["d3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDD4QynwF1mT","colab_type":"code","colab":{}},"source":["d3['b'].append(909)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gW5xs9TOF2gQ","colab_type":"code","colab":{}},"source":["d4 = copy.deepcopy(d3)\n","d4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzfN1pbYF3t4","colab_type":"code","colab":{}},"source":["d4['a'].append(78)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8gYRILJJVMk","colab_type":"code","colab":{}},"source":["d4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATDR9bO1JZYi","colab_type":"code","colab":{}},"source":["d['a'].append(909)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6_Vlp8KJmtO","colab_type":"code","colab":{}},"source":["d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2ZuH1ZyJt7W","colab_type":"code","colab":{}},"source":["d5={}\n","for i in d.keys():\n","  d5[i]=[]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ownN6bJmJ2-e","colab_type":"code","colab":{}},"source":["d5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUD2KxkvJ8Ap","colab_type":"code","colab":{}},"source":["\n","d5['a'].append(78)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vCw3STpKA01","colab_type":"code","colab":{}},"source":["d5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdtczaL6KBuo","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}