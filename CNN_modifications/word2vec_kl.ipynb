{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"word2vec_kl.ipynb","provenance":[{"file_id":"1sZlZWL7aYcsla5wvMM91-ZCkfR_19vG0","timestamp":1589965515847},{"file_id":"1NBK6cBXMzB9eoIWVaYMUZoyMn2zwIHt6","timestamp":1589914460202},{"file_id":"1uQ45aKC1kvpKNW_91iy6tqsoixHScEm8","timestamp":1589912225093},{"file_id":"1yqdy1wAa1K44GBjjCnjhVdfHdzv0FhOd","timestamp":1588956399944}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SH2wpCTYZdMh","colab_type":"code","colab":{}},"source":["import numpy as np\n","import multiprocessing as mp\n","import random,copy,string\n","from nltk.tokenize import word_tokenize\n","from scipy.stats import pearsonr\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Input, Convolution1D, MaxPooling1D, Flatten\n","from tensorflow.python.keras.layers import Lambda, multiply, concatenate, Dense\n","from tensorflow.python.keras.regularizers import l2\n","from tensorflow.python.keras.callbacks import Callback\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q_ABwOwnYl1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1590553875523,"user_tz":-330,"elapsed":2317,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"d1fc37e0-dea4-486f-8e20-354f85d92920"},"source":["import os, string, random, time, math\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import torch\n","'''import mlflow\n","import mlflow.pytorch'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import mlflow\\nimport mlflow.pytorch'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"cuP_JYvpMMrF","colab_type":"code","colab":{}},"source":["from numpy import linalg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQcOsPfMJL1-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"executionInfo":{"status":"ok","timestamp":1590553877015,"user_tz":-330,"elapsed":3763,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"69493f70-8eca-4998-8e5a-94d01b1e384e"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"uLz42OGuar31","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1590553913158,"user_tz":-330,"elapsed":39867,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"bcf06990-5e11-447b-af01-35a6e6804441"},"source":["'''from google.colab import drive\n","drive.mount('/content/drive')'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ae7JCVPfnj-p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1590553913162,"user_tz":-330,"elapsed":39843,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"30988e0e-96d3-4884-cdc3-fc65bdaa48cb"},"source":["#cd 'drive/My Drive'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dAPg3_LAoKHs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"status":"ok","timestamp":1590553916019,"user_tz":-330,"elapsed":42675,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"828ea25a-a3d4-4da7-c9c7-75f811e3a2b4"},"source":["#ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34m2017_takelab+cnn\u001b[0m/                    temp_word2vec_kl.ipynb\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/                    \u001b[01;34mthesis\u001b[0m/\n"," Copy_word2vec_kl_cnn_code.ipynb      word2vec_cross_cnn_code.ipynb\n","'Getting started.pdf'                 word2vec_cross_cnn.ipynb\n"," GoogleNews-vectors-negative300.bin   word2vec_cross_epoch.pt\n"," model_cifar.pt                       word2vec_cross_model.pt\n"," sts-dev.csv                          word2vec_kl_cnn_code.ipynb\n"," sts-test.csv                         word2vec_kl_epoch.pt\n"," sts-train.csv                        word2vec_kl_model.pt\n"," temp.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aN1UtOJrZdM5","colab_type":"code","colab":{}},"source":["c = dict()\n","c['num_runs']   = 3\n","c['num_epochs'] = 2\n","c['num_batchs'] = 4\n","c['batch_size'] = 3\n","c['wordvectdim']  = 300\n","c['sentencepad']  = 60\n","c['num_classes']  = 6\n","c['cnnfilters']     = {1: 1800}\n","c['cnninitial']     = 'he_uniform'\n","c['cnnactivate']    = 'relu'\n","c['densedimension'] = list([1800])\n","c['denseinitial']   = 'he_uniform'\n","c['denseactivate']  = 'tanh'\n","c['optimizer']  = 'adam'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WI95iihPfWFV","colab_type":"text"},"source":["# Loading word vectors"]},{"cell_type":"code","metadata":{"id":"IG9kzwY1TD_7","colab_type":"code","colab":{}},"source":["import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kkgQV8OJUrBG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1590553978206,"user_tz":-330,"elapsed":104814,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"c73f84be-73e0-4826-b901-4c26ca34f54c"},"source":["word_vector = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vr-1hkKAlgRi","colab_type":"text"},"source":["# Params"]},{"cell_type":"code","metadata":{"id":"F1YZLUtAldcF","colab_type":"code","colab":{}},"source":["class Params(object):\n","  def __init__(self, batch_size, epochs, seed, log_interval):\n","    self.batch_size = batch_size\n","    self.epochs = epochs\n","    self.seed = seed\n","    self.log_interval = log_interval\n","\n","args= Params(16,8,0,8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1oW_df2ZdRk","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"v87IpbJR9gDG","colab_type":"code","colab":{}},"source":["def matrixize(sentencelist, sentencepad):\n","  padding= np.zeros(300)\n","  matrix= np.zeros((len(sentencelist),sentencepad,300 ))\n","  m=0\n","  s=1\n","\n","  for i in range(len(sentencelist)):\n","    for j in range(len(sentencelist[i])):\n","      try:\n","        matrix[i][j]= word_vector[sentencelist[i][j]]\n","      except:\n","        #print(sentencelist[i][j])\n","        matrix[i][j]= np.random.normal(m,s,300)/linalg.norm(np.random.normal(m,s,300) + 1e-8)\n","\n","  return matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdRruMEiZdRt","colab_type":"code","colab":{}},"source":["def _load_data(filename):\n","        s0,s1,labels = [],[],[]\n","        lines=open(filename,'r').read().splitlines()\n","        for line in lines:\n","            _,_,_,_, label, s0x, s1x = line.rstrip().split('\\t')[:7]\n","            labels.append(float(label))\n","            s0.append([word.lower() for word in word_tokenize(s0x) if word not in string.punctuation])\n","            s1.append([word.lower() for word in word_tokenize(s1x) if word not in string.punctuation])\n","\n","        m0 = matrixize(s0, c['sentencepad'])\n","        m1 = matrixize(s1, c['sentencepad'])\n","        classes = np.zeros((len(labels),c['num_classes']))\n","        for i, label in enumerate(labels): # making probability distribution of classes\n","            if np.floor(label) + 1 < c['num_classes']:\n","                classes[i, int(np.floor(label)) + 1] = label - np.floor(label)\n","            classes[i, int(np.floor(label))] = np.floor(label) - label + 1\n","            \n","        return {'labels': labels, 's0': s0, 's1': s1, 'classes': classes, 'm0': m0, 'm1': m1}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMYnoNCrdNYu","colab_type":"text"},"source":["# Sampling Batch"]},{"cell_type":"code","metadata":{"id":"grGwK-j0-ctQ","colab_type":"code","colab":{}},"source":["def _sample_pairs(data,batch_size):\n","  datacopy={}\n","  for i in data.keys():\n","    datacopy[i]= []\n","  for i in range(batch_size):\n","    index = np.random.randint(len(data['labels']))\n","    #print(index)\n","    for key,value in data.items():\n","      datacopy[key].append(value[index])\n","  datacopy['classes']= torch.tensor(datacopy['classes'])\n","  datacopy['m0']= torch.tensor(datacopy['m0'])\n","  datacopy['m1']= torch.tensor(datacopy['m1'])\n","  return datacopy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BT33frFDlodS","colab_type":"text"},"source":["# Defining CNN Model"]},{"cell_type":"code","metadata":{"id":"GnxSiUWpl-qn","colab_type":"code","colab":{}},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OSip9cpbZdTt","colab_type":"code","colab":{}},"source":["class CNN_Model(nn.Module):\n","  def __init__(self,nh=1800):\n","    super(CNN_Model, self).__init__()\n","    self.cnn= nn.Sequential(nn.Conv1d(300,1800,1),              #(bs,300,60) -> (bs, 1800, 60)\n","                            nn.LeakyReLU(0.01),\n","                            nn.MaxPool1d(kernel_size=60))       #(bs,1800,60)-> (bs, 1800, 1)\n","\n","    self.Linear= nn.Sequential(nn.Linear(3600,nh),            #(bs, 3600)  -> (bs, 1800 )\n","                               nn.LeakyReLU(0.01),\n","                               nn.Linear(nh,6),               #(bs,1800)   -> (bs,6)\n","                               nn.Softmax(dim=1))\n","                            \n","    \n","\n","  def forward(self,input1, input2):\n","    input1= self.cnn(input1)\n","    input2= self.cnn(input2)\n","    #print(\"input1=\",input1.shape)\n","    #print(\"input2=\",input2.shape)\n","    input1= torch.flatten(input1,1)\n","    input2= torch.flatten(input2,1)\n","    #print(\"input1=\",input1.shape)\n","    #print(\"input2=\",input2.shape)\n","    absdiff= abs(input1 - input2)\n","    mulDifference= input1 * input2\n","    concatenate = torch.cat((absdiff,mulDifference),1)\n","    #print(concatenate.shape)\n","    output= self.Linear(concatenate)\n","    #print(output.shape)\n","    return (output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qk5XamSwZBra","colab_type":"code","colab":{}},"source":["def weights_init(m):\n","    if isinstance(m, nn.Conv1d) or isinstance(m, torch.nn.Linear):\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.zeros_(m.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRUv6bDcmYpE","colab_type":"text"},"source":["# Defining Loss"]},{"cell_type":"code","metadata":{"id":"yx9uWC_TVYND","colab_type":"code","colab":{}},"source":["criterion= nn.KLDivLoss(reduction='batchmean')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6g9dYuH9x1Dx","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"QxQ4r7AzyWA-","colab_type":"text"},"source":["## Train setup"]},{"cell_type":"code","metadata":{"id":"QCWkAB75yili","colab_type":"code","colab":{}},"source":["def train_batch(net,trainload, opt,batch_size, device= 'cpu'):\n","  \n","  net.train().to(device)\n","  opt.zero_grad()\n","  \n","  \n","\n","  data= _sample_pairs(trainload,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype= torch.float32)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype= torch.float32)\n","  true_output= (data['classes']).to(device,dtype= torch.float32)\n","  pred_output= net(input1,input2).to(torch.float32)\n","  #loss= _lossfunction(true_output,pred_output)\n","\n","  loss= criterion(pred_output,true_output)\n","  loss.backward()\n","  opt.step()\n","  #print(\"lossgf\",loss.item())\n","  return loss.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUuqHGuH45FT","colab_type":"text"},"source":["## Validation Setup"]},{"cell_type":"code","metadata":{"id":"M6VfkAYo3RN5","colab_type":"code","colab":{}},"source":["def valid_batch(net,validation, opt,batch_size, device= 'cpu'):\n","  net.eval().to(device)\n","  \n","  data = _sample_pairs(validation,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype= torch.float32)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype= torch.float32)\n","  true_output= (data['classes']).to(device,dtype= torch.float32)\n","  pred_output= net(input1,input2).to(torch.float32)\n","  #loss= _lossfunction(true_output,pred_output)\n","  loss= criterion(pred_output,true_output)\n","  return loss.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWJNMId9pJIH","colab_type":"text"},"source":["# Full Training Setup"]},{"cell_type":"markdown","metadata":{"id":"J0cOkFXSb7mP","colab_type":"text"},"source":["## Adam"]},{"cell_type":"code","metadata":{"id":"Y5WzVLt2pPkA","colab_type":"code","colab":{}},"source":["def train_setup(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.Adam(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'word2vec_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSXGs0aNc7lB","colab_type":"text"},"source":["## Adadelta"]},{"cell_type":"code","metadata":{"id":"M3_bepxWdCGp","colab_type":"code","colab":{}},"source":["def train_setup_Adadleta(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.Adadelta(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'word2vec_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFK1-o0IcCVj","colab_type":"text"},"source":["## Adamw"]},{"cell_type":"code","metadata":{"id":"bNba1PZYcSs7","colab_type":"code","colab":{}},"source":["def train_setup_AdamW(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.AdamW(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'word2vec_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fx0YqyHtcIIE","colab_type":"text"},"source":["## Adamax"]},{"cell_type":"code","metadata":{"id":"wWi94FnrdN4k","colab_type":"code","colab":{}},"source":["def train_setup_Adamax(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.Adamax(net.parameters(), lr=lr)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'word2vec_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bbhRIxNcLiY","colab_type":"text"},"source":["## RMS prop"]},{"cell_type":"code","metadata":{"id":"TdQzIAL_daxS","colab_type":"code","colab":{}},"source":["def train_setup_Adamw(net,lr = 0.01,  batch_size = args.batch_size, momentum = 0.9, device = 'cpu',epoch= args.epochs):\n","  torch.autograd.set_detect_anomaly(True)\n","  valid_loss_min = np.Inf\n","  net.apply(weights_init)\n","  opt = optim.RMSprop(net.parameters(), lr=lr, momentum= momentum)\n","  trainload= _load_data('sts-train.csv')\n","  validation= _load_data('sts-dev.csv')\n","  train_n_batch= int(len(trainload['labels'])/batch_size)\n","  train_loss_arr = np.zeros(train_n_batch+ 1)\n","  ###################\n","  # train the model #\n","  ###################\n","  \n","  for epoch in range(epoch):\n","    if epoch != 0:\n","      net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","    for i in range(train_n_batch):\n","      train_loss_arr[i+1] = (train_loss_arr[i]*i + train_batch(net,trainload, opt, batch_size, device))/(i + 1)\n","      #print(\"train_loss_MSE\",train_loss_arr[i+1])\n","\n","     \n","   \n","    \n","      #if i%display_freq == display_freq-1:\n","      '''clear_output(wait=True)\n","      print('Iteration', i, 'Loss', loss_arr[i])\n","      plt.figure()\n","      plt.plot(train_loss_arr,'-*')\n","      plt.xlabel('Iteration')\n","      plt.ylabel('Loss')\n","      plt.show()\n","      print('\\n\\n')'''\n","\n","      ######################    \n","      # validate the model #\n","      ######################\n","    valid_n_batch = int(len(validation['labels'])/batch_size)\n","    valid_loss_arr = np.zeros(valid_n_batch + 1)\n","    for i in range(valid_n_batch):\n","      valid_loss_arr[i+1] = (valid_loss_arr[i]*i + valid_batch(net,validation, opt, batch_size, device))/(i + 1)\n","    \n","    '''mlflow.log_metric('train_loss_MSE', train_loss_arr[i+1])\n","    mlflow.log_metric('valid_loss_MSE', valid_loss_arr[i+1])'''\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss_arr[train_n_batch ], valid_loss_arr[valid_n_batch ]))\n","  \n","    if valid_loss_arr[valid_n_batch ] <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min, valid_loss_arr[valid_n_batch ]))\n","          torch.save(net.state_dict(), 'word2vec_kl_epoch.pt')\n","          valid_loss_min = valid_loss_arr[valid_n_batch ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3lxuhFAopUC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1590553978226,"user_tz":-330,"elapsed":104697,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"0e2f640b-dc70-4ef3-e8d8-24861ad4c4b0"},"source":["'''net= CNN_Model()\n","train_setup(net,device='cuda:0')'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"net= CNN_Model()\\ntrain_setup(net,device='cuda:0')\""]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"-dkld6lIWW8b","colab_type":"text"},"source":["# Evaluate"]},{"cell_type":"markdown","metadata":{"id":"2GQzXlEi-XHH","colab_type":"text"},"source":["## Evaluation Setup"]},{"cell_type":"code","metadata":{"id":"SkXPty0k-h9O","colab_type":"code","colab":{}},"source":["def eval_setup(net,batch_size = 1379, device = 'cpu'):\n","  \n","  net.load_state_dict(torch.load('word2vec_kl_epoch.pt'))\n","  net.eval().to(device)\n","  testload= _load_data('sts-test.csv')\n","  test_n_batch= int(len(testload['labels'])/batch_size)\n","  #print(len(testload['labels']))\n","  test_loss_arr = np.zeros(test_n_batch+ 1)\n","\n","\n","  data= _sample_pairs(testload,batch_size)\n","  input1= (torch.transpose(data['m0'],1,2)).to(device,dtype= torch.float32)\n","  input2= (torch.transpose(data['m1'],1,2)).to(device,dtype= torch.float32)\n","  pred_output= (net(input1,input2)).to('cpu')\n","  pred= pred_output.detach().numpy()\n","  prediction = np.dot(np.array(pred),np.arange(c['num_classes']))\n","  print('pred_shape',prediction.shape)\n","  goldlabels = data['labels']\n","  result=round((pearsonr(prediction, goldlabels)[0]),4)\n","  print(\"result\",result)\n","\n","  return result\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDkaR6L8Sc6s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1590553978228,"user_tz":-330,"elapsed":104665,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"0ba00346-8120-4afe-f5b2-5e98f994b92d"},"source":["'''net=CNN_Model(1800)\n","eval_setup(net,device = 'cuda:0')'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"net=CNN_Model(1800)\\neval_setup(net,device = 'cuda:0')\""]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"uACzDtuiU-_k","colab_type":"text"},"source":["# Hyper-parameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"p9X9B22WZX9Q","colab_type":"text"},"source":["## optim.sgd"]},{"cell_type":"code","metadata":{"id":"ZDX421f0VWRP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"ok","timestamp":1589971012708,"user_tz":-330,"elapsed":356074,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"1c9903c5-495a-41ec-a0c7-5ba705fa434b"},"source":["\n","\n","'''for learning_rate in [0.01]:                    #with 0.001(try big momentum)\n","    for momemtum in [0.09,0.087]:\n","        for hidden_nodes in [1800]:\n","        \n","          expt_id = '%d_%d_%d' % (int(learning_rate*100), int(momemtum*100), hidden_nodes)\n","          print('\\nLR = %.4f, Momentum = %.4f, Hidden nodes = %d\\n' % (learning_rate, momemtum, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup(model,lr=learning_rate,momentum= momemtum ,device='cuda:0',epoch= args.epochs)\n","          eval_setup(model,device = 'cuda:0')'''\n","\n","            \n","\n","          \n","\n","          \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.0100, Momentum = 0.0900, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.583344 \tValidation Loss: -0.595736\n","Validation loss decreased (inf --> -0.595736).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.588067 \tValidation Loss: -0.592433\n","Epoch: 2 \tTraining Loss: -0.587731 \tValidation Loss: -0.584254\n","Epoch: 3 \tTraining Loss: -0.585534 \tValidation Loss: -0.600466\n","Validation loss decreased (-0.595736 --> -0.600466).  Saving model ...\n","pred_shape (1379,)\n","result 0.4951\n","\n","LR = 0.0100, Momentum = 0.0870, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.584323 \tValidation Loss: -0.600755\n","Validation loss decreased (inf --> -0.600755).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.585079 \tValidation Loss: -0.593198\n","Epoch: 2 \tTraining Loss: -0.590510 \tValidation Loss: -0.593639\n","Epoch: 3 \tTraining Loss: -0.588037 \tValidation Loss: -0.597081\n","pred_shape (1379,)\n","result 0.6184\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xgdyqp0tZbt9","colab_type":"text"},"source":["##optim.adam"]},{"cell_type":"code","metadata":{"id":"YLNYDRpUZfR4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590557596246,"user_tz":-330,"elapsed":3152416,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"26fd9c12-8444-4000-cbcb-a24b07197fbc"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.0001,0.0005,0.0006]:\n","    \n","        for hidden_nodes in [1024,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.8f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup(model,lr=learning_rate,device='cuda:0',epoch= args.epochs)\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'word2vec_kl_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.00010000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.733784 \tValidation Loss: -0.757467\n","Validation loss decreased (inf --> -0.757467).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.800647 \tValidation Loss: -0.747774\n","Epoch: 2 \tTraining Loss: -0.794570 \tValidation Loss: -0.793757\n","Validation loss decreased (-0.757467 --> -0.793757).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.832864 \tValidation Loss: -0.765571\n","Epoch: 4 \tTraining Loss: -0.834651 \tValidation Loss: -0.733003\n","Epoch: 5 \tTraining Loss: -0.831087 \tValidation Loss: -0.764624\n","Epoch: 6 \tTraining Loss: -0.842959 \tValidation Loss: -0.758328\n","Epoch: 7 \tTraining Loss: -0.839821 \tValidation Loss: -0.778610\n","pred_shape (1379,)\n","result 0.6881\n","best_model_score increased (-inf --> 0.688100).  Saving model ...\n","\n","LR = 0.00010000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.709481 \tValidation Loss: -0.718445\n","Validation loss decreased (inf --> -0.718445).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.786051 \tValidation Loss: -0.732148\n","Validation loss decreased (-0.718445 --> -0.732148).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.818912 \tValidation Loss: -0.729085\n","Epoch: 3 \tTraining Loss: -0.809796 \tValidation Loss: -0.748188\n","Validation loss decreased (-0.732148 --> -0.748188).  Saving model ...\n","Epoch: 4 \tTraining Loss: -0.850697 \tValidation Loss: -0.735520\n","Epoch: 5 \tTraining Loss: -0.863303 \tValidation Loss: -0.755425\n","Validation loss decreased (-0.748188 --> -0.755425).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.880545 \tValidation Loss: -0.750767\n","Epoch: 7 \tTraining Loss: -0.881923 \tValidation Loss: -0.763042\n","Validation loss decreased (-0.755425 --> -0.763042).  Saving model ...\n","pred_shape (1379,)\n","result 0.6665\n","\n","LR = 0.00010000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.729241 \tValidation Loss: -0.707867\n","Validation loss decreased (inf --> -0.707867).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.801769 \tValidation Loss: -0.777696\n","Validation loss decreased (-0.707867 --> -0.777696).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.846812 \tValidation Loss: -0.769088\n","Epoch: 3 \tTraining Loss: -0.859762 \tValidation Loss: -0.788439\n","Validation loss decreased (-0.777696 --> -0.788439).  Saving model ...\n","Epoch: 4 \tTraining Loss: -0.882315 \tValidation Loss: -0.774484\n","Epoch: 5 \tTraining Loss: -0.886847 \tValidation Loss: -0.739725\n","Epoch: 6 \tTraining Loss: -0.890314 \tValidation Loss: -0.780746\n","Epoch: 7 \tTraining Loss: -0.897437 \tValidation Loss: -0.758681\n","pred_shape (1379,)\n","result 0.6651\n","\n","LR = 0.00050000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.753112 \tValidation Loss: -0.760959\n","Validation loss decreased (inf --> -0.760959).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.830043 \tValidation Loss: -0.789293\n","Validation loss decreased (-0.760959 --> -0.789293).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.872002 \tValidation Loss: -0.825609\n","Validation loss decreased (-0.789293 --> -0.825609).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.893855 \tValidation Loss: -0.812300\n","Epoch: 4 \tTraining Loss: -0.877944 \tValidation Loss: -0.788019\n","Epoch: 5 \tTraining Loss: -0.897470 \tValidation Loss: -0.789313\n","Epoch: 6 \tTraining Loss: -0.904224 \tValidation Loss: -0.809073\n","Epoch: 7 \tTraining Loss: -0.910393 \tValidation Loss: -0.807506\n","pred_shape (1379,)\n","result 0.691\n","best_model_score increased (0.688100 --> 0.691000).  Saving model ...\n","\n","LR = 0.00050000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.729010 \tValidation Loss: -0.748623\n","Validation loss decreased (inf --> -0.748623).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.722554 \tValidation Loss: -0.761777\n","Validation loss decreased (-0.748623 --> -0.761777).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.727423 \tValidation Loss: -0.683346\n","Epoch: 3 \tTraining Loss: -0.728379 \tValidation Loss: -0.747344\n","Epoch: 4 \tTraining Loss: -0.752893 \tValidation Loss: -0.719887\n","Epoch: 5 \tTraining Loss: -0.733784 \tValidation Loss: -0.732926\n","Epoch: 6 \tTraining Loss: -0.728635 \tValidation Loss: -0.747452\n","Epoch: 7 \tTraining Loss: -0.725271 \tValidation Loss: -0.754477\n","pred_shape (1379,)\n","result 0.6061\n","\n","LR = 0.00050000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.662377 \tValidation Loss: -0.605384\n","Validation loss decreased (inf --> -0.605384).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.659927 \tValidation Loss: -0.605556\n","Validation loss decreased (-0.605384 --> -0.605556).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.663573 \tValidation Loss: -0.625460\n","Validation loss decreased (-0.605556 --> -0.625460).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.653440 \tValidation Loss: -0.597596\n","Epoch: 4 \tTraining Loss: -0.650263 \tValidation Loss: -0.595460\n","Epoch: 5 \tTraining Loss: -0.659936 \tValidation Loss: -0.595391\n","Epoch: 6 \tTraining Loss: -0.658196 \tValidation Loss: -0.592268\n","Epoch: 7 \tTraining Loss: -0.659994 \tValidation Loss: -0.594017\n","pred_shape (1379,)\n","result -0.095\n","\n","LR = 0.00060000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.689412 \tValidation Loss: -0.667965\n","Validation loss decreased (inf --> -0.667965).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.721785 \tValidation Loss: -0.671741\n","Validation loss decreased (-0.667965 --> -0.671741).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.722342 \tValidation Loss: -0.695412\n","Validation loss decreased (-0.671741 --> -0.695412).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.736121 \tValidation Loss: -0.685383\n","Epoch: 4 \tTraining Loss: -0.725273 \tValidation Loss: -0.682217\n","Epoch: 5 \tTraining Loss: -0.742169 \tValidation Loss: -0.691907\n","Epoch: 6 \tTraining Loss: -0.732404 \tValidation Loss: -0.654777\n","Epoch: 7 \tTraining Loss: -0.724326 \tValidation Loss: -0.666959\n","pred_shape (1379,)\n","result 0.5776\n","\n","LR = 0.00060000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.686689 \tValidation Loss: -0.678649\n","Validation loss decreased (inf --> -0.678649).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.690490 \tValidation Loss: -0.695247\n","Validation loss decreased (-0.678649 --> -0.695247).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.736077 \tValidation Loss: -0.685467\n","Epoch: 3 \tTraining Loss: -0.731560 \tValidation Loss: -0.680165\n","Epoch: 4 \tTraining Loss: -0.723313 \tValidation Loss: -0.635321\n","Epoch: 5 \tTraining Loss: -0.743530 \tValidation Loss: -0.715236\n","Validation loss decreased (-0.695247 --> -0.715236).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.750407 \tValidation Loss: -0.702872\n","Epoch: 7 \tTraining Loss: -0.765652 \tValidation Loss: -0.709311\n","pred_shape (1379,)\n","result 0.6574\n","\n","LR = 0.00060000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.653449 \tValidation Loss: -0.617038\n","Validation loss decreased (inf --> -0.617038).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.654265 \tValidation Loss: -0.599836\n","Epoch: 2 \tTraining Loss: -0.658374 \tValidation Loss: -0.614515\n","Epoch: 3 \tTraining Loss: -0.661492 \tValidation Loss: -0.613236\n","Epoch: 4 \tTraining Loss: -0.661056 \tValidation Loss: -0.602928\n","Epoch: 5 \tTraining Loss: -0.655154 \tValidation Loss: -0.605807\n","Epoch: 6 \tTraining Loss: -0.663076 \tValidation Loss: -0.599056\n","Epoch: 7 \tTraining Loss: -0.654403 \tValidation Loss: -0.637832\n","Validation loss decreased (-0.617038 --> -0.637832).  Saving model ...\n","pred_shape (1379,)\n","result 0.0781\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"twLVEUYYc0Fv","colab_type":"text"},"source":["## Adadelta"]},{"cell_type":"code","metadata":{"id":"z-ONcMX2dtbG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590477848200,"user_tz":-330,"elapsed":1518915,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"80ffac3b-3ede-459a-bd8a-25b646cee5cc"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.01,0.02,0.06]:\n","    \n","        for hidden_nodes in [1024,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.8f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adadleta(model,lr=learning_rate,device='cuda:0')\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'word2vec_kl_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.01000000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.616614 \tValidation Loss: -0.605659\n","Validation loss decreased (inf --> -0.605659).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.646019 \tValidation Loss: -0.636993\n","Validation loss decreased (-0.605659 --> -0.636993).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.670678 \tValidation Loss: -0.646871\n","Validation loss decreased (-0.636993 --> -0.646871).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.679009 \tValidation Loss: -0.693373\n","Validation loss decreased (-0.646871 --> -0.693373).  Saving model ...\n","pred_shape (1379,)\n","result 0.6496\n","best_model_score increased (-inf --> 0.649600).  Saving model ...\n","\n","LR = 0.01000000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.616065 \tValidation Loss: -0.619386\n","Validation loss decreased (inf --> -0.619386).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.654137 \tValidation Loss: -0.631456\n","Validation loss decreased (-0.619386 --> -0.631456).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.681396 \tValidation Loss: -0.675652\n","Validation loss decreased (-0.631456 --> -0.675652).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.704259 \tValidation Loss: -0.692876\n","Validation loss decreased (-0.675652 --> -0.692876).  Saving model ...\n","pred_shape (1379,)\n","result 0.679\n","best_model_score increased (0.649600 --> 0.679000).  Saving model ...\n","\n","LR = 0.01000000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.618496 \tValidation Loss: -0.613536\n","Validation loss decreased (inf --> -0.613536).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.646546 \tValidation Loss: -0.624460\n","Validation loss decreased (-0.613536 --> -0.624460).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.677604 \tValidation Loss: -0.687145\n","Validation loss decreased (-0.624460 --> -0.687145).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.709801 \tValidation Loss: -0.675127\n","pred_shape (1379,)\n","result 0.6411\n","\n","LR = 0.02000000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.625433 \tValidation Loss: -0.635773\n","Validation loss decreased (inf --> -0.635773).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.675283 \tValidation Loss: -0.656197\n","Validation loss decreased (-0.635773 --> -0.656197).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.721343 \tValidation Loss: -0.711637\n","Validation loss decreased (-0.656197 --> -0.711637).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.726142 \tValidation Loss: -0.700264\n","pred_shape (1379,)\n","result 0.6917\n","best_model_score increased (0.679000 --> 0.691700).  Saving model ...\n","\n","LR = 0.02000000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.636372 \tValidation Loss: -0.605771\n","Validation loss decreased (inf --> -0.605771).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.657156 \tValidation Loss: -0.631207\n","Validation loss decreased (-0.605771 --> -0.631207).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.673387 \tValidation Loss: -0.624008\n","Epoch: 3 \tTraining Loss: -0.670527 \tValidation Loss: -0.648152\n","Validation loss decreased (-0.631207 --> -0.648152).  Saving model ...\n","pred_shape (1379,)\n","result 0.5559\n","\n","LR = 0.02000000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.632924 \tValidation Loss: -0.614975\n","Validation loss decreased (inf --> -0.614975).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.682789 \tValidation Loss: -0.694148\n","Validation loss decreased (-0.614975 --> -0.694148).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.716190 \tValidation Loss: -0.693293\n","Epoch: 3 \tTraining Loss: -0.702387 \tValidation Loss: -0.699625\n","Validation loss decreased (-0.694148 --> -0.699625).  Saving model ...\n","pred_shape (1379,)\n","result 0.6623\n","\n","LR = 0.06000000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.659395 \tValidation Loss: -0.692817\n","Validation loss decreased (inf --> -0.692817).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.738071 \tValidation Loss: -0.736953\n","Validation loss decreased (-0.692817 --> -0.736953).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.750691 \tValidation Loss: -0.745373\n","Validation loss decreased (-0.736953 --> -0.745373).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.771820 \tValidation Loss: -0.760145\n","Validation loss decreased (-0.745373 --> -0.760145).  Saving model ...\n","pred_shape (1379,)\n","result 0.6912\n","\n","LR = 0.06000000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.666261 \tValidation Loss: -0.702228\n","Validation loss decreased (inf --> -0.702228).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.736486 \tValidation Loss: -0.749717\n","Validation loss decreased (-0.702228 --> -0.749717).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.762628 \tValidation Loss: -0.759762\n","Validation loss decreased (-0.749717 --> -0.759762).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.768097 \tValidation Loss: -0.748356\n","pred_shape (1379,)\n","result 0.6648\n","\n","LR = 0.06000000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.673524 \tValidation Loss: -0.709949\n","Validation loss decreased (inf --> -0.709949).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.722847 \tValidation Loss: -0.743906\n","Validation loss decreased (-0.709949 --> -0.743906).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.756174 \tValidation Loss: -0.758456\n","Validation loss decreased (-0.743906 --> -0.758456).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.765816 \tValidation Loss: -0.759737\n","Validation loss decreased (-0.758456 --> -0.759737).  Saving model ...\n","pred_shape (1379,)\n","result 0.6764\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AxYIrf5TaKHb","colab_type":"text"},"source":["## optim.AdamW"]},{"cell_type":"code","metadata":{"id":"q4pNDahtd6-G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590479396608,"user_tz":-330,"elapsed":3062021,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"d39a0b6c-04f4-405a-df3d-7c4ab6651e79"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.0001,0.0002,0.0006]:\n","    \n","        for hidden_nodes in [1024,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.8f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adamw(model,lr=learning_rate,device='cuda:0')\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'word2vec_kl_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.00010000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.655386 \tValidation Loss: -0.647266\n","Validation loss decreased (inf --> -0.647266).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.651338 \tValidation Loss: -0.659980\n","Validation loss decreased (-0.647266 --> -0.659980).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.654971 \tValidation Loss: -0.634847\n","Epoch: 3 \tTraining Loss: -0.652093 \tValidation Loss: -0.648497\n","pred_shape (1379,)\n","result nan\n","\n","LR = 0.00010000, Hidden nodes = 1600\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n","  warnings.warn(PearsonRConstantInputWarning())\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0 \tTraining Loss: -0.725891 \tValidation Loss: -0.716092\n","Validation loss decreased (inf --> -0.716092).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.724866 \tValidation Loss: -0.676003\n","Epoch: 2 \tTraining Loss: -0.725892 \tValidation Loss: -0.637315\n","Epoch: 3 \tTraining Loss: -0.727580 \tValidation Loss: -0.603493\n","pred_shape (1379,)\n","result 0.5896\n","best_model_score increased (-inf --> 0.589600).  Saving model ...\n","\n","LR = 0.00010000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.645589 \tValidation Loss: -0.656145\n","Validation loss decreased (inf --> -0.656145).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.642847 \tValidation Loss: -0.638018\n","Epoch: 2 \tTraining Loss: -0.642057 \tValidation Loss: -0.642251\n","Epoch: 3 \tTraining Loss: -0.637805 \tValidation Loss: -0.655323\n","pred_shape (1379,)\n","result 0.0526\n","\n","LR = 0.00020000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.641877 \tValidation Loss: -0.651110\n","Validation loss decreased (inf --> -0.651110).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.652203 \tValidation Loss: -0.641935\n","Epoch: 2 \tTraining Loss: -0.645473 \tValidation Loss: -0.636386\n","Epoch: 3 \tTraining Loss: -0.644257 \tValidation Loss: -0.653559\n","Validation loss decreased (-0.651110 --> -0.653559).  Saving model ...\n","pred_shape (1379,)\n","result 0.0562\n","\n","LR = 0.00020000, Hidden nodes = 1600\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3538: PearsonRNearConstantInputWarning: An input array is nearly constant; the computed correlation coefficent may be inaccurate.\n","  warnings.warn(PearsonRNearConstantInputWarning())\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0 \tTraining Loss: -0.659922 \tValidation Loss: -0.622755\n","Validation loss decreased (inf --> -0.622755).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.654538 \tValidation Loss: -0.584550\n","Epoch: 2 \tTraining Loss: -0.662981 \tValidation Loss: -0.594376\n","Epoch: 3 \tTraining Loss: -0.660212 \tValidation Loss: -0.611915\n","pred_shape (1379,)\n","result 0.0591\n","\n","LR = 0.00020000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.565855 \tValidation Loss: -0.608054\n","Validation loss decreased (inf --> -0.608054).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.553918 \tValidation Loss: -0.575417\n","Epoch: 2 \tTraining Loss: -0.556033 \tValidation Loss: -0.598827\n","Epoch: 3 \tTraining Loss: -0.549968 \tValidation Loss: -0.583208\n","pred_shape (1379,)\n","result nan\n","\n","LR = 0.00060000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.639985 \tValidation Loss: -0.661437\n","Validation loss decreased (inf --> -0.661437).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.647877 \tValidation Loss: -0.657286\n","Epoch: 2 \tTraining Loss: -0.643230 \tValidation Loss: -0.627971\n","Epoch: 3 \tTraining Loss: -0.648053 \tValidation Loss: -0.630295\n","pred_shape (1379,)\n","result nan\n","\n","LR = 0.00060000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.665508 \tValidation Loss: -0.602949\n","Validation loss decreased (inf --> -0.602949).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.665277 \tValidation Loss: -0.592390\n","Epoch: 2 \tTraining Loss: -0.661945 \tValidation Loss: -0.608941\n","Validation loss decreased (-0.602949 --> -0.608941).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.655938 \tValidation Loss: -0.592260\n","pred_shape (1379,)\n","result nan\n","\n","LR = 0.00060000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.644949 \tValidation Loss: -0.652527\n","Validation loss decreased (inf --> -0.652527).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.646791 \tValidation Loss: -0.642250\n","Epoch: 2 \tTraining Loss: -0.653936 \tValidation Loss: -0.654676\n","Validation loss decreased (-0.652527 --> -0.654676).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.644216 \tValidation Loss: -0.634975\n","pred_shape (1379,)\n","result nan\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rifGk_VhbRJ-","colab_type":"text"},"source":["## opim.Adamax"]},{"cell_type":"code","metadata":{"id":"-gJ-9k8kd_50","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590561502201,"user_tz":-330,"elapsed":65246,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"dc183e6f-af42-4199-aa55-eadb6f1b6dc8"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.0001,0.0002,0.0006]:\n","    \n","        for hidden_nodes in [1024,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.8f, Hidden nodes = %d\\n' % (learning_rate, hidden_nodes))\n","\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adamax(model,lr=learning_rate,device='cuda:0',epoch= args.epochs)\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'word2vec_kl_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.00010000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.698171 \tValidation Loss: -0.684966\n","Validation loss decreased (inf --> -0.684966).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.729320 \tValidation Loss: -0.712570\n","Validation loss decreased (-0.684966 --> -0.712570).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.733361 \tValidation Loss: -0.696998\n","Epoch: 3 \tTraining Loss: -0.753042 \tValidation Loss: -0.700151\n","Epoch: 4 \tTraining Loss: -0.739874 \tValidation Loss: -0.711209\n","Epoch: 5 \tTraining Loss: -0.737811 \tValidation Loss: -0.708916\n","Epoch: 6 \tTraining Loss: -0.745635 \tValidation Loss: -0.688213\n","Epoch: 7 \tTraining Loss: -0.737028 \tValidation Loss: -0.717606\n","Validation loss decreased (-0.712570 --> -0.717606).  Saving model ...\n","pred_shape (1379,)\n","result 0.7096\n","best_model_score increased (-inf --> 0.709600).  Saving model ...\n","\n","LR = 0.00010000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.696010 \tValidation Loss: -0.706939\n","Validation loss decreased (inf --> -0.706939).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.739626 \tValidation Loss: -0.722351\n","Validation loss decreased (-0.706939 --> -0.722351).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.764488 \tValidation Loss: -0.715983\n","Epoch: 3 \tTraining Loss: -0.764948 \tValidation Loss: -0.716457\n","Epoch: 4 \tTraining Loss: -0.768625 \tValidation Loss: -0.739294\n","Validation loss decreased (-0.722351 --> -0.739294).  Saving model ...\n","Epoch: 5 \tTraining Loss: -0.786555 \tValidation Loss: -0.732015\n","Epoch: 6 \tTraining Loss: -0.781104 \tValidation Loss: -0.743002\n","Validation loss decreased (-0.739294 --> -0.743002).  Saving model ...\n","Epoch: 7 \tTraining Loss: -0.796585 \tValidation Loss: -0.740465\n","pred_shape (1379,)\n","result 0.7074\n","\n","LR = 0.00010000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.711555 \tValidation Loss: -0.738700\n","Validation loss decreased (inf --> -0.738700).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.762377 \tValidation Loss: -0.744886\n","Validation loss decreased (-0.738700 --> -0.744886).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.793068 \tValidation Loss: -0.759348\n","Validation loss decreased (-0.744886 --> -0.759348).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.814741 \tValidation Loss: -0.766881\n","Validation loss decreased (-0.759348 --> -0.766881).  Saving model ...\n","Epoch: 4 \tTraining Loss: -0.844453 \tValidation Loss: -0.763372\n","Epoch: 5 \tTraining Loss: -0.847746 \tValidation Loss: -0.770388\n","Validation loss decreased (-0.766881 --> -0.770388).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.862940 \tValidation Loss: -0.766295\n","Epoch: 7 \tTraining Loss: -0.856499 \tValidation Loss: -0.772503\n","Validation loss decreased (-0.770388 --> -0.772503).  Saving model ...\n","pred_shape (1379,)\n","result 0.6732\n","\n","LR = 0.00020000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.710024 \tValidation Loss: -0.742497\n","Validation loss decreased (inf --> -0.742497).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.788976 \tValidation Loss: -0.774644\n","Validation loss decreased (-0.742497 --> -0.774644).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.817324 \tValidation Loss: -0.762665\n","Epoch: 3 \tTraining Loss: -0.815212 \tValidation Loss: -0.762676\n","Epoch: 4 \tTraining Loss: -0.824763 \tValidation Loss: -0.780656\n","Validation loss decreased (-0.774644 --> -0.780656).  Saving model ...\n","Epoch: 5 \tTraining Loss: -0.846619 \tValidation Loss: -0.763511\n","Epoch: 6 \tTraining Loss: -0.836812 \tValidation Loss: -0.775394\n","Epoch: 7 \tTraining Loss: -0.842917 \tValidation Loss: -0.756784\n","pred_shape (1379,)\n","result 0.7051\n","\n","LR = 0.00020000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.680948 \tValidation Loss: -0.659764\n","Validation loss decreased (inf --> -0.659764).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.712693 \tValidation Loss: -0.677113\n","Validation loss decreased (-0.659764 --> -0.677113).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.724826 \tValidation Loss: -0.693180\n","Validation loss decreased (-0.677113 --> -0.693180).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.750996 \tValidation Loss: -0.647576\n","Epoch: 4 \tTraining Loss: -0.756631 \tValidation Loss: -0.655573\n","Epoch: 5 \tTraining Loss: -0.744210 \tValidation Loss: -0.648149\n","Epoch: 6 \tTraining Loss: -0.739486 \tValidation Loss: -0.672938\n","Epoch: 7 \tTraining Loss: -0.750845 \tValidation Loss: -0.680928\n","pred_shape (1379,)\n","result 0.576\n","\n","LR = 0.00020000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.736981 \tValidation Loss: -0.742004\n","Validation loss decreased (inf --> -0.742004).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.791908 \tValidation Loss: -0.761880\n","Validation loss decreased (-0.742004 --> -0.761880).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.824640 \tValidation Loss: -0.759853\n","Epoch: 3 \tTraining Loss: -0.805158 \tValidation Loss: -0.759777\n","Epoch: 4 \tTraining Loss: -0.825849 \tValidation Loss: -0.762848\n","Validation loss decreased (-0.761880 --> -0.762848).  Saving model ...\n","Epoch: 5 \tTraining Loss: -0.847141 \tValidation Loss: -0.781354\n","Validation loss decreased (-0.762848 --> -0.781354).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.877023 \tValidation Loss: -0.778971\n","Epoch: 7 \tTraining Loss: -0.877928 \tValidation Loss: -0.766098\n","pred_shape (1379,)\n","result 0.6811\n","\n","LR = 0.00060000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.691933 \tValidation Loss: -0.681348\n","Validation loss decreased (inf --> -0.681348).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.724925 \tValidation Loss: -0.665436\n","Epoch: 2 \tTraining Loss: -0.716696 \tValidation Loss: -0.665245\n","Epoch: 3 \tTraining Loss: -0.728373 \tValidation Loss: -0.684313\n","Validation loss decreased (-0.681348 --> -0.684313).  Saving model ...\n","Epoch: 4 \tTraining Loss: -0.745260 \tValidation Loss: -0.670140\n","Epoch: 5 \tTraining Loss: -0.747740 \tValidation Loss: -0.668820\n","Epoch: 6 \tTraining Loss: -0.740828 \tValidation Loss: -0.672850\n","Epoch: 7 \tTraining Loss: -0.731072 \tValidation Loss: -0.679301\n","pred_shape (1379,)\n","result 0.5481\n","\n","LR = 0.00060000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.657211 \tValidation Loss: -0.602909\n","Validation loss decreased (inf --> -0.602909).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.658784 \tValidation Loss: -0.597419\n","Epoch: 2 \tTraining Loss: -0.669632 \tValidation Loss: -0.582675\n","Epoch: 3 \tTraining Loss: -0.653113 \tValidation Loss: -0.581195\n","Epoch: 4 \tTraining Loss: -0.660419 \tValidation Loss: -0.597731\n","Epoch: 5 \tTraining Loss: -0.637101 \tValidation Loss: -0.653045\n","Validation loss decreased (-0.602909 --> -0.653045).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.680734 \tValidation Loss: -0.698652\n","Validation loss decreased (-0.653045 --> -0.698652).  Saving model ...\n","Epoch: 7 \tTraining Loss: -0.728833 \tValidation Loss: -0.685907\n","pred_shape (1379,)\n","result 0.5116\n","\n","LR = 0.00060000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.739283 \tValidation Loss: -0.725261\n","Validation loss decreased (inf --> -0.725261).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.754271 \tValidation Loss: -0.752701\n","Validation loss decreased (-0.725261 --> -0.752701).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.809659 \tValidation Loss: -0.778412\n","Validation loss decreased (-0.752701 --> -0.778412).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.870474 \tValidation Loss: -0.785571\n","Validation loss decreased (-0.778412 --> -0.785571).  Saving model ...\n","Epoch: 4 \tTraining Loss: -0.909148 \tValidation Loss: -0.787179\n","Validation loss decreased (-0.785571 --> -0.787179).  Saving model ...\n","Epoch: 5 \tTraining Loss: -0.940877 \tValidation Loss: -0.794779\n","Validation loss decreased (-0.787179 --> -0.794779).  Saving model ...\n","Epoch: 6 \tTraining Loss: -0.956416 \tValidation Loss: -0.783883\n","Epoch: 7 \tTraining Loss: -0.958293 \tValidation Loss: -0.782578\n","pred_shape (1379,)\n","result 0.685\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YS8GKXNzbwsL","colab_type":"text"},"source":["## optim.RMSprop"]},{"cell_type":"code","metadata":{"id":"HjjWJzmYeHW8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590482527748,"user_tz":-330,"elapsed":3131102,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"5f3f0d6c-9b52-4afa-e065-aac0c361467b"},"source":["best_model_score = -(np.Inf)\n","for learning_rate in [0.01,0.02,0.001]:\n","    for momemtum in [0.09,0.087]:\n","        for hidden_nodes in [1024,1600,1800]:\n","        \n","          expt_id = '%d_%d' % (int(learning_rate*100),  hidden_nodes)\n","          print('\\nLR = %.8f, Momentum = %.5f, Hidden nodes = %d\\n' % (learning_rate, momemtum, hidden_nodes))\n","          model = CNN_Model(hidden_nodes)\n","          train_setup_Adadleta(model,lr=learning_rate,momentum= momemtum ,device='cuda:0')\n","\n","          eval_score=eval_setup(model,device = 'cuda:0')\n","\n","          if eval_score >= best_model_score :\n","            print('best_model_score increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            best_model_score, eval_score))\n","            torch.save(model.state_dict(), 'word2vec_kl_model.pt')\n","            best_model_score= eval_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","LR = 0.01000000, Momentum = 0.09000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.600980 \tValidation Loss: -0.605686\n","Validation loss decreased (inf --> -0.605686).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.646431 \tValidation Loss: -0.639014\n","Validation loss decreased (-0.605686 --> -0.639014).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.655960 \tValidation Loss: -0.624927\n","Epoch: 3 \tTraining Loss: -0.658725 \tValidation Loss: -0.624825\n","pred_shape (1379,)\n","result 0.324\n","best_model_score increased (-inf --> 0.324000).  Saving model ...\n","\n","LR = 0.01000000, Momentum = 0.09000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.611263 \tValidation Loss: -0.622550\n","Validation loss decreased (inf --> -0.622550).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.656972 \tValidation Loss: -0.624875\n","Validation loss decreased (-0.622550 --> -0.624875).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.666565 \tValidation Loss: -0.613075\n","Epoch: 3 \tTraining Loss: -0.663706 \tValidation Loss: -0.638375\n","Validation loss decreased (-0.624875 --> -0.638375).  Saving model ...\n","pred_shape (1379,)\n","result 0.5523\n","best_model_score increased (0.324000 --> 0.552300).  Saving model ...\n","\n","LR = 0.01000000, Momentum = 0.09000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.607415 \tValidation Loss: -0.617653\n","Validation loss decreased (inf --> -0.617653).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.647162 \tValidation Loss: -0.631916\n","Validation loss decreased (-0.617653 --> -0.631916).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.651971 \tValidation Loss: -0.637518\n","Validation loss decreased (-0.631916 --> -0.637518).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.679719 \tValidation Loss: -0.665755\n","Validation loss decreased (-0.637518 --> -0.665755).  Saving model ...\n","pred_shape (1379,)\n","result 0.6178\n","best_model_score increased (0.552300 --> 0.617800).  Saving model ...\n","\n","LR = 0.01000000, Momentum = 0.08700, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.614772 \tValidation Loss: -0.647425\n","Validation loss decreased (inf --> -0.647425).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.651159 \tValidation Loss: -0.600722\n","Epoch: 2 \tTraining Loss: -0.649418 \tValidation Loss: -0.640925\n","Epoch: 3 \tTraining Loss: -0.654844 \tValidation Loss: -0.618496\n","pred_shape (1379,)\n","result 0.3966\n","\n","LR = 0.01000000, Momentum = 0.08700, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.611672 \tValidation Loss: -0.630623\n","Validation loss decreased (inf --> -0.630623).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.654429 \tValidation Loss: -0.624425\n","Epoch: 2 \tTraining Loss: -0.653030 \tValidation Loss: -0.637961\n","Validation loss decreased (-0.630623 --> -0.637961).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.655367 \tValidation Loss: -0.651592\n","Validation loss decreased (-0.637961 --> -0.651592).  Saving model ...\n","pred_shape (1379,)\n","result 0.5427\n","\n","LR = 0.01000000, Momentum = 0.08700, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.606984 \tValidation Loss: -0.631100\n","Validation loss decreased (inf --> -0.631100).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.642544 \tValidation Loss: -0.616884\n","Epoch: 2 \tTraining Loss: -0.650394 \tValidation Loss: -0.621809\n","Epoch: 3 \tTraining Loss: -0.654026 \tValidation Loss: -0.622901\n","pred_shape (1379,)\n","result 0.3403\n","\n","LR = 0.02000000, Momentum = 0.09000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.628713 \tValidation Loss: -0.633487\n","Validation loss decreased (inf --> -0.633487).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.656802 \tValidation Loss: -0.626431\n","Epoch: 2 \tTraining Loss: -0.666395 \tValidation Loss: -0.618105\n","Epoch: 3 \tTraining Loss: -0.660991 \tValidation Loss: -0.634018\n","Validation loss decreased (-0.633487 --> -0.634018).  Saving model ...\n","pred_shape (1379,)\n","result 0.5082\n","\n","LR = 0.02000000, Momentum = 0.09000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.621208 \tValidation Loss: -0.634387\n","Validation loss decreased (inf --> -0.634387).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.668351 \tValidation Loss: -0.651112\n","Validation loss decreased (-0.634387 --> -0.651112).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.691746 \tValidation Loss: -0.700877\n","Validation loss decreased (-0.651112 --> -0.700877).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.726272 \tValidation Loss: -0.716951\n","Validation loss decreased (-0.700877 --> -0.716951).  Saving model ...\n","pred_shape (1379,)\n","result 0.671\n","best_model_score increased (0.617800 --> 0.671000).  Saving model ...\n","\n","LR = 0.02000000, Momentum = 0.09000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.632773 \tValidation Loss: -0.627619\n","Validation loss decreased (inf --> -0.627619).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.664379 \tValidation Loss: -0.687821\n","Validation loss decreased (-0.627619 --> -0.687821).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.715564 \tValidation Loss: -0.699890\n","Validation loss decreased (-0.687821 --> -0.699890).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.715966 \tValidation Loss: -0.717848\n","Validation loss decreased (-0.699890 --> -0.717848).  Saving model ...\n","pred_shape (1379,)\n","result 0.6587\n","\n","LR = 0.02000000, Momentum = 0.08700, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.624876 \tValidation Loss: -0.611606\n","Validation loss decreased (inf --> -0.611606).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.676443 \tValidation Loss: -0.655647\n","Validation loss decreased (-0.611606 --> -0.655647).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.686899 \tValidation Loss: -0.678474\n","Validation loss decreased (-0.655647 --> -0.678474).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.719443 \tValidation Loss: -0.679832\n","Validation loss decreased (-0.678474 --> -0.679832).  Saving model ...\n","pred_shape (1379,)\n","result 0.6593\n","\n","LR = 0.02000000, Momentum = 0.08700, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.637971 \tValidation Loss: -0.618593\n","Validation loss decreased (inf --> -0.618593).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.678056 \tValidation Loss: -0.692937\n","Validation loss decreased (-0.618593 --> -0.692937).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.713200 \tValidation Loss: -0.693931\n","Validation loss decreased (-0.692937 --> -0.693931).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.715949 \tValidation Loss: -0.708572\n","Validation loss decreased (-0.693931 --> -0.708572).  Saving model ...\n","pred_shape (1379,)\n","result 0.6575\n","\n","LR = 0.02000000, Momentum = 0.08700, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.630595 \tValidation Loss: -0.611557\n","Validation loss decreased (inf --> -0.611557).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.683781 \tValidation Loss: -0.690757\n","Validation loss decreased (-0.611557 --> -0.690757).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.712841 \tValidation Loss: -0.713249\n","Validation loss decreased (-0.690757 --> -0.713249).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.729493 \tValidation Loss: -0.715622\n","Validation loss decreased (-0.713249 --> -0.715622).  Saving model ...\n","pred_shape (1379,)\n","result 0.6517\n","\n","LR = 0.00100000, Momentum = 0.09000, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.577070 \tValidation Loss: -0.584615\n","Validation loss decreased (inf --> -0.584615).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.592403 \tValidation Loss: -0.587111\n","Validation loss decreased (-0.584615 --> -0.587111).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.594938 \tValidation Loss: -0.604350\n","Validation loss decreased (-0.587111 --> -0.604350).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.607521 \tValidation Loss: -0.605891\n","Validation loss decreased (-0.604350 --> -0.605891).  Saving model ...\n","pred_shape (1379,)\n","result 0.3905\n","\n","LR = 0.00100000, Momentum = 0.09000, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.586436 \tValidation Loss: -0.587577\n","Validation loss decreased (inf --> -0.587577).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.593379 \tValidation Loss: -0.588123\n","Validation loss decreased (-0.587577 --> -0.588123).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.603718 \tValidation Loss: -0.618259\n","Validation loss decreased (-0.588123 --> -0.618259).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.609509 \tValidation Loss: -0.603572\n","pred_shape (1379,)\n","result 0.3292\n","\n","LR = 0.00100000, Momentum = 0.09000, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.586962 \tValidation Loss: -0.601214\n","Validation loss decreased (inf --> -0.601214).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.592995 \tValidation Loss: -0.602483\n","Validation loss decreased (-0.601214 --> -0.602483).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.608155 \tValidation Loss: -0.622946\n","Validation loss decreased (-0.602483 --> -0.622946).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.610707 \tValidation Loss: -0.621573\n","pred_shape (1379,)\n","result 0.3441\n","\n","LR = 0.00100000, Momentum = 0.08700, Hidden nodes = 1024\n","\n","Epoch: 0 \tTraining Loss: -0.590369 \tValidation Loss: -0.597177\n","Validation loss decreased (inf --> -0.597177).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.595422 \tValidation Loss: -0.595591\n","Epoch: 2 \tTraining Loss: -0.593215 \tValidation Loss: -0.604065\n","Validation loss decreased (-0.597177 --> -0.604065).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.606864 \tValidation Loss: -0.603309\n","pred_shape (1379,)\n","result 0.2697\n","\n","LR = 0.00100000, Momentum = 0.08700, Hidden nodes = 1600\n","\n","Epoch: 0 \tTraining Loss: -0.574864 \tValidation Loss: -0.589800\n","Validation loss decreased (inf --> -0.589800).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.585125 \tValidation Loss: -0.590569\n","Validation loss decreased (-0.589800 --> -0.590569).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.587899 \tValidation Loss: -0.610646\n","Validation loss decreased (-0.590569 --> -0.610646).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.594506 \tValidation Loss: -0.605070\n","pred_shape (1379,)\n","result 0.1523\n","\n","LR = 0.00100000, Momentum = 0.08700, Hidden nodes = 1800\n","\n","Epoch: 0 \tTraining Loss: -0.579692 \tValidation Loss: -0.587399\n","Validation loss decreased (inf --> -0.587399).  Saving model ...\n","Epoch: 1 \tTraining Loss: -0.594112 \tValidation Loss: -0.594661\n","Validation loss decreased (-0.587399 --> -0.594661).  Saving model ...\n","Epoch: 2 \tTraining Loss: -0.600105 \tValidation Loss: -0.602231\n","Validation loss decreased (-0.594661 --> -0.602231).  Saving model ...\n","Epoch: 3 \tTraining Loss: -0.602057 \tValidation Loss: -0.612289\n","Validation loss decreased (-0.602231 --> -0.612289).  Saving model ...\n","pred_shape (1379,)\n","result 0.1103\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pISYepuieC_P","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Js120I3bTMaB","colab_type":"text"},"source":["# Practice"]},{"cell_type":"code","metadata":{"id":"99F0SRW5dyY9","colab_type":"code","colab":{}},"source":["trainload= _load_data('sts-train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNi4lsSTdkGL","colab_type":"code","colab":{}},"source":["data=_sample_pairs(trainload,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhA8win0gR0V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1589382413225,"user_tz":-330,"elapsed":1485,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"15ee7d0c-693b-4bd4-f857-d14e5358ec66"},"source":["data_len= []\n","for i in data['s0']:\n","  data_len.append(len(i))\n","max_len_sent=max(data_len)\n","\n","rep= torch.zeros(max_len_sent, 2 ,300)\n","rep.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([11, 2, 300])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"u2UgwP2Zk23J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"ok","timestamp":1589383031128,"user_tz":-330,"elapsed":1647,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"83d2fa18-b2ce-4912-8f4d-0953eedf9be6"},"source":["(data['s0'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['brown',\n","  'and',\n","  'white',\n","  'cow',\n","  'standing',\n","  'in',\n","  'grass',\n","  'at',\n","  'side',\n","  'of',\n","  'road'],\n"," ['boy', 'scouts', 'delay', 'decision', 'on', 'admitting', 'gays']]"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"1aknSKgzjzY1","colab_type":"code","colab":{}},"source":["temp=[]\n","rep[0][0]=torch.tensor(indextovector[wordtoindex['brown']])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63gsjzqaoYLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589383373257,"user_tz":-330,"elapsed":1130,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"ec8900d0-ef3d-46a2-e169-ac80a390c947"},"source":["rep[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3741, -0.0763,  0.1093,  0.1866,  0.0299,  0.1827, -0.6320,  0.1331,\n","         -0.1290,  0.6034, -0.6804, -0.1422, -0.1336, -0.6594,  0.0524,  0.1674,\n","          0.6392,  1.7680,  0.3462, -0.6248, -0.1287, -0.1970, -0.3745,  0.3306,\n","          0.0468, -0.6535, -0.5614,  0.2274,  0.2292, -0.4158, -0.1677,  0.3354,\n","          0.0972, -0.4670, -0.0269, -0.0677, -0.1921, -0.1337,  0.0163, -0.2083,\n","          0.6486, -0.1102, -0.0510,  0.0722,  0.1877,  0.2711, -0.3142,  0.1832,\n","          0.3915, -0.2255, -0.3819,  0.3306, -0.0899, -0.3763,  0.0480, -0.2050,\n","         -0.5489,  0.3044, -0.1887, -0.3034, -0.1157, -0.3487,  0.2800,  0.0501,\n","         -0.2814, -0.2335, -0.3683, -0.1204, -0.2833,  0.1857,  0.1036,  0.2531,\n","         -0.0340,  0.1051,  0.1214, -0.1630, -0.3318,  0.1731,  0.1076, -0.9936,\n","         -0.1171,  0.4223,  0.1512,  0.3106,  0.2674, -0.4927,  1.8049,  1.0738,\n","          0.3442,  0.1128, -0.1049,  0.3694,  0.4082, -0.4037,  0.3233, -0.0939,\n","         -0.0290,  0.3825, -0.3389, -0.6132,  0.8434,  0.1593, -0.1174,  0.0806,\n","         -0.2899, -0.4439, -0.1183,  0.1658,  0.1525,  0.2386, -0.3295, -0.3199,\n","         -0.3118, -0.1919,  0.2847,  0.2568,  0.4379, -0.0233,  0.1891, -0.0850,\n","         -0.1164, -0.1102, -0.0595,  0.1155,  0.2070,  0.5587,  0.8177, -0.2563,\n","         -0.0181, -0.0410, -0.2660, -0.4568, -0.0374,  0.3325,  0.1414, -0.0630,\n","         -0.0866, -0.3327, -0.0092, -0.0355, -2.8871,  0.2453,  0.2974,  0.6599,\n","         -0.0134, -0.1089,  0.1155,  0.0526, -0.0430,  0.2285,  0.4021, -0.4891,\n","          0.0441, -0.1368, -0.7181,  0.2526, -0.5788, -0.4807, -0.2409,  0.0427,\n","         -0.0946, -0.3034, -0.3197,  0.3554,  0.0629, -0.2043, -0.2978, -0.1545,\n","          0.2443,  0.0489, -0.0773,  0.3643, -0.1513, -0.4539, -0.3431,  0.1069,\n","          0.4292, -0.0260,  0.4825,  0.3361, -0.5032,  0.2241, -0.2737, -0.4904,\n","         -0.1173,  1.0537, -0.2023,  0.0490, -0.1223,  0.1100,  0.4155, -0.1183,\n","          0.1148, -0.2663,  0.2908,  0.2541,  0.3535,  0.3172, -0.1517, -0.5050,\n","         -0.2202,  0.1163, -0.2513,  0.2280,  0.2628,  0.2107,  0.0236,  0.0769,\n","         -0.1486,  0.0720,  0.3767,  0.3536, -0.3933, -0.1338,  0.5593,  0.0337,\n","         -0.4850, -0.3276,  0.2587,  0.4876, -0.2649,  0.0329, -0.0838, -0.0638,\n","          0.1076, -0.1855, -0.0523,  0.0767, -0.2148,  0.9646, -0.2479, -0.1211,\n","          0.0394,  0.4471, -0.1380, -0.0278, -0.4937, -0.5163,  0.3386,  0.5921,\n","         -0.2013, -0.0832, -0.3758, -0.2127, -0.3856,  0.2259, -0.3722, -0.1872,\n","         -0.6052, -0.1279,  0.2344, -0.4222, -0.2354,  0.2968,  0.0678,  0.0780,\n","          0.3195, -0.0348,  0.2981,  0.4400,  0.1174,  0.0550,  0.2368,  0.8982,\n","         -0.4097,  0.0752, -0.1103, -0.4099, -0.9572,  0.5275, -0.0427,  0.2662,\n","          0.3053, -0.5190, -0.4604, -0.0938,  0.1301,  0.0193,  0.0102,  0.0076,\n","          0.2955,  0.2316, -0.0349, -0.1169, -0.3273,  0.2049,  0.4750,  0.5131,\n","         -0.1458, -0.1851, -0.0154,  0.3929, -0.0348, -0.7203, -0.3653,  0.7405,\n","          0.1084, -0.3658, -0.2882,  0.1146],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","          0.0000,  0.0000,  0.0000,  0.0000]])"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"A2XUoGKXm6bu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"ok","timestamp":1589383146976,"user_tz":-330,"elapsed":1264,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"492d4acd-34f3-4ba7-e431-75db36c221ee"},"source":["data['m0'][0][11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"VB2STaT2SzyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1589370183806,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"b337bbc4-9adc-4234-c976-8006afce7033"},"source":["m = nn.MaxPool1d(1)\n","input = torch.randn(6, 5)\n","input.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 5])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"xTHe3bx83kR6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1589370748582,"user_tz":-330,"elapsed":1446,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"035245a9-3273-4e41-ead4-fde0c6edb090"},"source":["input"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.7378,  1.7368,  0.0198,  0.8431,  0.9046],\n","        [-1.5884, -1.1110,  0.8694,  0.3053,  0.3550],\n","        [ 0.1539, -0.4941, -0.0704,  1.3168,  0.9116],\n","        [ 1.2474,  0.6942,  0.1928, -0.5697, -0.8765],\n","        [-0.5326, -0.9174,  0.2422, -0.1825,  0.2426],\n","        [-1.5224, -1.7681, -1.8564, -1.0241, -1.3065]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"YytbQVTy2LTs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1589371055905,"user_tz":-330,"elapsed":1936,"user":{"displayName":"Nyg line","photoUrl":"","userId":"16627810253525004666"}},"outputId":"d23b8943-a917-4490-8178-78e3b4bfa8f8"},"source":["input.to(torch.float32)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.7378,  1.7368,  0.0198,  0.8431,  0.9046],\n","        [-1.5884, -1.1110,  0.8694,  0.3053,  0.3550],\n","        [ 0.1539, -0.4941, -0.0704,  1.3168,  0.9116],\n","        [ 1.2474,  0.6942,  0.1928, -0.5697, -0.8765],\n","        [-0.5326, -0.9174,  0.2422, -0.1825,  0.2426],\n","        [-1.5224, -1.7681, -1.8564, -1.0241, -1.3065]])"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"c4ifcUQ-rE-z","colab_type":"code","colab":{}},"source":["output = m(input)\n","output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtNIYGWArTMW","colab_type":"code","colab":{}},"source":["out= torch.flatten(output, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KNDFi6QhK3V","colab_type":"code","colab":{}},"source":["out.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oa8AVUN1TFka","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo28Dj7EhNUl","colab_type":"code","colab":{}},"source":["a=torch.tensor(np.array([1,2,3]))\n","b=torch.tensor(np.array([4,5,6]))\n","a*b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ec595DaW2i6W","colab_type":"code","colab":{}},"source":["c=abs(a-b)\n","\n","c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcJ9pxdi3tIc","colab_type":"code","colab":{}},"source":["import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-r14qRP595v","colab_type":"code","colab":{}},"source":["torch.cat((a,b),1).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-F3HFrZ_ASV5","colab_type":"code","colab":{}},"source":["c= torch.randn(2, 5)\n","d= torch.randn(2,5)\n","e= torch.cat((c,d),1)\n","e.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sk0E5F22DpCX","colab_type":"code","colab":{}},"source":["(c-d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwGrQJ4TENvS","colab_type":"code","colab":{}},"source":["-1.5705-1.4329"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9NMh84DEcqd","colab_type":"code","colab":{}},"source":["(c*d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-V2_tVDEsj4","colab_type":"code","colab":{}},"source":["-1.5705*1.4329"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSrDjaRbEw58","colab_type":"code","colab":{}},"source":["-0.6561*-0.2841"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3FunLaWZdOb","colab_type":"code","colab":{}},"source":["'''blocksize=2\n","k= ((li[block:block+blocksize], block) for block in range(0,len(li),blocksize))\n","for i,j in k:\n","    print(i,j)'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKmW1OrSE5Fi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIw11E00ZdOp","colab_type":"code","colab":{}},"source":["'''def worker(args):\n","        print(args[0])\n","        print(args[1])'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFRV8fpxq6_H","colab_type":"code","colab":{}},"source":["p= np.random.randn(1,5)\n","p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIxJzwg4rFr7","colab_type":"code","colab":{}},"source":["plt.plot(p,'-*')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA4LFKOErSjU","colab_type":"code","colab":{}},"source":["p[1:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2pIVqMUrnEk","colab_type":"code","colab":{}},"source":["d={'a':[1],'b':[2],'c':[3]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utzQBcocDKgw","colab_type":"code","colab":{}},"source":["d1={}\n","d1= d.keys\n","d1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StnIcTfHDNOr","colab_type":"code","colab":{}},"source":["d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ILOBRLaDUoR","colab_type":"code","colab":{}},"source":["d2={}\n","d3=d2.fromkeys(d.keys(),[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYqLiw7OFti_","colab_type":"code","colab":{}},"source":["d3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDD4QynwF1mT","colab_type":"code","colab":{}},"source":["d3['b'].append(909)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gW5xs9TOF2gQ","colab_type":"code","colab":{}},"source":["d4 = copy.deepcopy(d3)\n","d4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzfN1pbYF3t4","colab_type":"code","colab":{}},"source":["d4['a'].append(78)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8gYRILJJVMk","colab_type":"code","colab":{}},"source":["d4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATDR9bO1JZYi","colab_type":"code","colab":{}},"source":["d['a'].append(909)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6_Vlp8KJmtO","colab_type":"code","colab":{}},"source":["d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2ZuH1ZyJt7W","colab_type":"code","colab":{}},"source":["d5={}\n","for i in d.keys():\n","  d5[i]=[]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ownN6bJmJ2-e","colab_type":"code","colab":{}},"source":["d5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUD2KxkvJ8Ap","colab_type":"code","colab":{}},"source":["\n","d5['a'].append(78)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vCw3STpKA01","colab_type":"code","colab":{}},"source":["d5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-IFwXmcLL4A","colab_type":"code","colab":{}},"source":["from numpy import linalg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdtczaL6KBuo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"error","timestamp":1589962938659,"user_tz":-330,"elapsed":1187,"user":{"displayName":"Rachna Soni","photoUrl":"","userId":"11374143874237782535"}},"outputId":"ab6da03f-0bb3-417e-db02-fa44cfc5e31f"},"source":[""],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-867cd236419a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'linalg'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"YkK96xS3LKDu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}